{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sage Monorepo","text":"<p>Build robust biomedical research apps faster</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Developers building components in silos can lead to a number of problems, including duplicated components, increased maintenance, and reduced visibility.</p> <p>Sage Monorepo addresses these problems by providing a centralized repository for standardized, reusable components. This solution also improves the developer experience (DX), which can lead to increased morale, productivity, retention, and knowledge transfer.</p>"},{"location":"#why-sage-monorepo","title":"Why Sage Monorepo?","text":"<p>Build Products, Not Build Systems - Focus on what matters: solving biomedical research challenges, not wrestling with development infrastructure.</p> <ul> <li>Shared Components: Reusable libraries and components across all projects</li> <li>Modern Tooling: Nx workspace with TypeScript, Angular, Java, Python, and R support</li> <li>Streamlined CI/CD: Automated testing, building, and deployment workflows</li> <li>Developer Experience: Enhanced productivity with intelligent caching and task orchestration</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to start building? Choose your path:</p> \ud83d\ude80 Quick Start <p>Jump right in with our local development setup</p> Get Started \u2192 \ud83d\udcda Learn the Basics <p>Understand the architecture and concepts</p> Learn More \u2192"},{"location":"#our-products","title":"Our Products","text":"<p>Explore the biomedical research platforms built with Sage Monorepo:</p> Product Description Links Agora High-dimensional human transcriptomic, proteomic, and metabolomic evidence for genes associated with Alzheimer's disease Docs \u2022 API BixArena Platform for biomedical data analysis competitions and challenges API Model-AD Resource for Alzheimer's disease model development and validation API OpenChallenges Cloud-native platform for biomedical challenges and citizen science data benchmarking Docs \u2022 API Synapse Collaborative platform for data sharing and analysis in biomedical research API"},{"location":"#platform-statistics","title":"Platform Statistics","text":"<p>Our monorepo powers a comprehensive ecosystem:</p> <ul> <li>145 Active Projects across multiple domains</li> <li>6 Major APIs with comprehensive documentation</li> <li>4 Programming Languages (TypeScript, Java, Python, R)</li> <li>Multiple Frameworks (Angular, Spring Boot, Flask, Shiny)</li> </ul>"},{"location":"#whats-new","title":"What's New","text":"<p>Check out our latest updates and improvements:</p> <ul> <li>Latest Updates - Recent releases and announcements</li> <li>Blog - Technical insights and developer stories</li> <li>Service Catalog - Complete project inventory</li> </ul>"},{"location":"#community","title":"Community","text":"<p>Join our community and contribute to advancing biomedical research:</p> <ul> <li>Contribute: Check our Contributing Guide</li> <li>Issues: Report bugs and request features on GitHub</li> <li>Support: Get help through our FAQ or community channels</li> </ul> <p>Ready to build the future of biomedical research? Get started today! |</p>"},{"location":"storybooks/","title":"Component Storybooks","text":"<p>Browse interactive component documentation for each domain:</p> <ul> <li>Agora Storybook - AD research platform components</li> <li>Explorers Storybook - Data exploration components</li> <li>All Storybooks - Unified navigation (composition host)</li> </ul>"},{"location":"storybooks/#about-storybook","title":"About Storybook","text":"<p>Storybook is an interactive component explorer that lets you browse, test, and document UI components in isolation. Each storybook contains:</p> <ul> <li>Component stories - Examples of components in different states</li> <li>Interactive controls - Modify component props in real-time</li> <li>Documentation - Usage guidelines and API reference</li> </ul>"},{"location":"submission-workflow/","title":"Documentation Submission Workflow","text":"<p>This guide describes the standard two-PR workflow for submitting technical documentation (RFCs, Architecture plans, and ADRs) to this repository.</p>"},{"location":"submission-workflow/#overview","title":"Overview","text":"<p>All technical documentation follows a two-PR workflow to ensure proposals are easily readable on the docs site while centralizing feedback:</p> <ol> <li>PR #1: Publish to Docs Site - Quick merge to make the document readable with rendered diagrams</li> <li>PR #2: Collect Feedback - Centralized location for team review and discussion</li> </ol> <p>Why Two PRs?</p> <ul> <li>PR #1 gets the document published quickly so it's readable on the docs site with rendered Mermaid diagrams</li> <li>PR #2 centralizes all feedback in one place and allows easy iteration</li> <li>Clear separation between publishing (compliance check) and review (technical feedback)</li> </ul>"},{"location":"submission-workflow/#step-1-publish-document-to-docs-site-pr-1","title":"Step 1: Publish Document to Docs Site (PR #1)","text":"<p>This PR adds your document to the documentation site for easy reading with rendered diagrams.</p>"},{"location":"submission-workflow/#11-choose-document-number","title":"1.1 Choose Document Number","text":"<p>Check the appropriate page to find the next available number:</p> <ul> <li>RFCs: Check the RFCs table on the RFCs page for sequential numbering (e.g., <code>0002-bixarena-leaderboard-automation.md</code>)</li> <li>Architecture: Architecture plans don't use numbers (e.g., <code>bixarena-leaderboard-automation-plan.md</code>)</li> <li>ADRs: Check the ADRs table on the ADRs page for sequential numbering (e.g., <code>0001-use-docker-for-lambda.md</code>)</li> </ul>"},{"location":"submission-workflow/#12-create-branch","title":"1.2 Create Branch","text":"<p>Create a branch from <code>main</code> using a descriptive name (not the document number to avoid conflicts):</p> <pre><code># For RFCs\ngit switch -c docs/rfc-brief-description\n\n# For Architecture plans\ngit switch -c docs/arch-brief-description\n\n# For ADRs\ngit switch -c docs/adr-brief-description\n</code></pre>"},{"location":"submission-workflow/#13-create-document-file","title":"1.3 Create Document File","text":"<p>Copy the appropriate template and fill it out:</p> Document Type Template Filename Format Example RFC docs/rfcs/template.md <code>NNNN-product-brief-title.md</code> <code>docs/rfcs/0002-bixarena-leaderboard-automation.md</code> Architecture docs/architecture/template.md <code>product-brief-title-plan.md</code> <code>docs/architecture/bixarena-leaderboard-automation-plan.md</code> ADR docs/adr/template.md <code>NNNN-brief-title.md</code> <code>docs/adr/0001-use-docker-for-lambda.md</code>"},{"location":"submission-workflow/#14-add-to-readme","title":"1.4 Add to README","text":"<p>Add a row to the appropriate table in the directory's README.md with the initial status:</p> Document Type File to Update Initial Status RFC <code>docs/rfcs/README.md</code> <code>Open for Review</code> Architecture <code>docs/architecture/README.md</code> <code>Active</code> ADR <code>docs/adr/README.md</code> <code>Open for Review</code> <p>Note</p> <p>These are the initial status values when first publishing. See Status Values in Quick Reference for the full status progression.</p>"},{"location":"submission-workflow/#15-commit-and-push","title":"1.5 Commit and Push","text":"<p>Add both the document file and updated README, then commit and push:</p> <pre><code># Example for RFC\ngit add docs/rfcs/NNNN-product-brief-title.md docs/rfcs/README.md\ngit commit -m \"docs: add RFC-NNNN for [brief description]\"\ngit push origin docs/rfc-brief-description\n\n# Adapt the paths and commit message for Architecture plans or ADRs\n</code></pre>"},{"location":"submission-workflow/#16-open-pr-1","title":"1.6 Open PR #1","text":"<p>Open a pull request using the appropriate template below (copy-paste into PR description):</p>"},{"location":"submission-workflow/#rfc-publication-template","title":"RFC Publication Template","text":"<pre><code>## RFC Publication\n\nThis PR publishes RFC-NNNN: [Full RFC title] to the documentation site.\n\n## Checklist\n\n- [ ] RFC follows template structure\n- [ ] RFC number is sequential and unique\n- [ ] Added to the RFCs table in README\n- [ ] All sections completed\n- [ ] No sensitive information included\n\n## Next Steps\n\nAfter this PR merges, I will open a second PR to collect feedback and discussion.\n\n---\n\n\ud83d\udcc4 [View rendered RFC on docs site after merge](https://sage-bionetworks.github.io/sage-monorepo/rfcs/NNNN-product-brief-title/)\n</code></pre>"},{"location":"submission-workflow/#architecture-plan-publication-template","title":"Architecture Plan Publication Template","text":"<pre><code>## Architecture Plan Publication\n\nThis PR publishes [Full plan title] to the documentation site.\n\n**Related RFC**: [Link to RFC if applicable]\n\n## Checklist\n\n- [ ] Plan follows template structure\n- [ ] Added to Architecture Documents table\n- [ ] All sections completed\n- [ ] Diagrams added to `architecture/diagrams/` if applicable\n- [ ] No sensitive information included\n\n## Next Steps\n\nAfter this PR merges, I will open a second PR to collect feedback and discussion.\n\n---\n\n\ud83d\udcc4 [View rendered plan on docs site after merge](https://sage-bionetworks.github.io/sage-monorepo/architecture/product-brief-title-plan/)\n</code></pre>"},{"location":"submission-workflow/#adr-publication-template","title":"ADR Publication Template","text":"<pre><code>## ADR Publication\n\nThis PR publishes ADR-NNNN: [Full ADR title] to the documentation site.\n\n## Checklist\n\n- [ ] ADR follows template structure\n- [ ] ADR number is sequential and unique\n- [ ] Added to ADRs table\n- [ ] All sections completed (Status, Context, Decision, Consequences)\n- [ ] No sensitive information included\n\n## Next Steps\n\nAfter this PR merges, I will open a second PR to collect feedback and discussion.\n\n---\n\n\ud83d\udcc4 [View rendered ADR on docs site after merge](https://sage-bionetworks.github.io/sage-monorepo/adr/NNNN-brief-title/)\n</code></pre>"},{"location":"submission-workflow/#17-review-and-merge","title":"1.7 Review and Merge","text":"<p>A maintainer verifies compliance (template followed, no sensitive info) and merges the PR.</p>"},{"location":"submission-workflow/#step-2-collect-feedback-pr-2","title":"Step 2: Collect Feedback (PR #2)","text":"<p>This PR allows reviewers to provide feedback in a centralized location.</p>"},{"location":"submission-workflow/#21-create-feedback-branch","title":"2.1 Create Feedback Branch","text":"<p>After PR #1 merges, create a new branch from <code>main</code>:</p> <pre><code># For RFCs\ngit switch -c docs/rfc-NNNN-feedback\n\n# For Architecture plans\ngit switch -c docs/arch-[brief-name]-feedback\n\n# For ADRs\ngit switch -c docs/adr-NNNN-feedback\n</code></pre>"},{"location":"submission-workflow/#22-make-initial-commit","title":"2.2 Make Initial Commit","text":"<p>To open a PR, you need at least one commit. Create a placeholder commit that you'll update later:</p> <pre><code># Create empty commit to enable PR creation\ngit commit --allow-empty -m \"docs: open feedback PR for [RFC-NNNN|architecture|ADR-NNNN]\"\ngit push origin [branch-name]\n</code></pre>"},{"location":"submission-workflow/#23-open-pr-2","title":"2.3 Open PR #2","text":"<p>Open a pull request from your feedback branch to <code>main</code> and copy the PR URL. You'll need it in the next step.</p>"},{"location":"submission-workflow/#24-add-discussion-link","title":"2.4 Add Discussion Link","text":"<p>Update the document's frontmatter to include the PR #2 URL:</p> <pre><code>---\ndiscussion: https://github.com/Sage-Bionetworks/sage-monorepo/pull/XXXX # Add PR #2 URL\n---\n</code></pre> <p>Commit and push this change:</p> <pre><code>git add docs/[rfcs|architecture|adr]/[filename].md\ngit commit -m \"docs: add discussion link for feedback tracking\"\ngit push origin [branch-name]\n</code></pre> <p>Note</p> <p>Status remains unchanged (already set to \"Open for Review\" or \"Active\" in PR #1).</p>"},{"location":"submission-workflow/#25-add-pr-description","title":"2.5 Add PR Description","text":"<p>Update PR #2's description using the appropriate template below (copy-paste and fill in):</p>"},{"location":"submission-workflow/#rfc-feedback-template","title":"RFC Feedback Template","text":"<pre><code>## RFC Feedback: [RFC Title]\n\nThis PR collects feedback and discussion on RFC-NNNN. Please review the [rendered RFC on the docs site](https://sage-bionetworks.github.io/sage-monorepo/rfcs/NNNN-product-brief-title/) and provide your feedback through PR comments.\n\n## Review Focus Areas\n\n- [ ] **Motivation**: Is the problem clearly articulated?\n- [ ] **Proposed Solution**: Does the approach make sense?\n- [ ] **Alternatives**: Are other options adequately considered?\n- [ ] **Success Criteria**: Are metrics measurable and appropriate?\n- [ ] **Timeline**: Is the schedule realistic?\n- [ ] **Open Questions**: Can you help answer unresolved questions?\n\n## Reviewers\n\n@[tag relevant team members]\n\n## Discussion\n\nPlease add your comments, questions, and suggestions below or as inline comments on this PR.\n</code></pre>"},{"location":"submission-workflow/#architecture-plan-feedback-template","title":"Architecture Plan Feedback Template","text":"<pre><code>## Architecture Plan Feedback: [Plan Title]\n\n**Related RFC**: [Link to RFC if applicable]\n\nThis PR collects feedback and discussion on the architecture plan. Please review the [rendered plan on the docs site](https://sage-bionetworks.github.io/sage-monorepo/architecture/product-brief-title-plan/) and provide your feedback through PR comments.\n\n## Review Focus Areas\n\n- [ ] **Technical Approach**: Is the architecture sound?\n- [ ] **Implementation Details**: Are the implementation steps clear?\n- [ ] **Dependencies**: Are all dependencies identified?\n- [ ] **Security**: Are security considerations adequate?\n- [ ] **Performance**: Are performance implications addressed?\n- [ ] **Testing**: Is the testing strategy comprehensive?\n\n## Reviewers\n\n@[tag relevant team members]\n\n## Discussion\n\nPlease add your comments, questions, and suggestions below or as inline comments on this PR.\n</code></pre>"},{"location":"submission-workflow/#adr-feedback-template","title":"ADR Feedback Template","text":"<pre><code>## ADR Feedback: [ADR Title]\n\nThis PR collects feedback and discussion on ADR-NNNN. Please review the [rendered ADR on the docs site](https://sage-bionetworks.github.io/sage-monorepo/adr/NNNN-brief-title/) and provide your feedback through PR comments.\n\n## Review Focus Areas\n\n- [ ] **Context**: Is the context clear and complete?\n- [ ] **Decision**: Is the decision well-justified?\n- [ ] **Alternatives**: Were alternatives adequately considered?\n- [ ] **Consequences**: Are consequences (positive and negative) identified?\n- [ ] **Impact**: Is the impact on existing systems understood?\n\n## Reviewers\n\n@[tag relevant team members]\n\n## Discussion\n\nPlease add your comments, questions, and suggestions below or as inline comments on this PR.\n</code></pre>"},{"location":"submission-workflow/#26-incorporate-feedback","title":"2.6 Incorporate Feedback","text":"<p>Address comments by updating the document file in this branch. Push changes as needed:</p> <pre><code>git add docs/[rfcs|architecture|adr]/[filename].md\ngit commit -m \"docs: address feedback - [brief description]\"\ngit push origin [branch-name]\n</code></pre>"},{"location":"submission-workflow/#27-final-decision","title":"2.7 Final Decision","text":"<p>Once consensus is reached, update the document status and close PR #2:</p>"},{"location":"submission-workflow/#for-rfcs","title":"For RFCs:","text":"<ul> <li>Approved: Update status to \"Approved\" in the README. After merging, create a detailed architecture plan as a NEW document in <code>docs/architecture/</code></li> <li>Rejected: Update status to \"Rejected\" with documented rationale in the README</li> <li>Needs Revision: Continue iterating in PR #2 until resolved</li> </ul>"},{"location":"submission-workflow/#for-architecture-plans","title":"For Architecture Plans:","text":"<ul> <li>Ready for Implementation: Status remains \"Active\", begin implementation. Keep the document updated as implementation evolves</li> <li>Needs Changes: Continue iterating in PR #2 until resolved</li> <li>Superseded: Update status to \"Superseded\" only if the architectural approach fundamentally changes and requires a completely new document. Link to the new plan</li> </ul>"},{"location":"submission-workflow/#for-adrs","title":"For ADRs:","text":"<ul> <li>Accepted: Update status to \"Accepted\"</li> <li>Rejected: Update status to \"Rejected\" with documented rationale</li> <li>Superseded: Update status to \"Superseded\" and link to new ADR</li> </ul>"},{"location":"submission-workflow/#quick-reference","title":"Quick Reference","text":""},{"location":"submission-workflow/#branch-naming","title":"Branch Naming","text":"Document Type Branch Name Pattern RFC (PR #1) <code>docs/rfc-brief-description</code> RFC (PR #2) <code>docs/rfc-NNNN-feedback</code> Architecture (PR #1) <code>docs/arch-brief-description</code> Architecture (PR #2) <code>docs/arch-[brief-name]-feedback</code> ADR (PR #1) <code>docs/adr-brief-description</code> ADR (PR #2) <code>docs/adr-NNNN-feedback</code>"},{"location":"submission-workflow/#commit-message-patterns","title":"Commit Message Patterns","text":"Action Pattern Add RFC <code>docs: add RFC-NNNN for [brief description]</code> Add Architecture <code>docs: add architecture plan for [brief description]</code> Add ADR <code>docs: add ADR-NNNN for [brief description]</code> Add discussion link <code>docs: add discussion link for feedback tracking</code> Address feedback <code>docs: address feedback - [brief description]</code> Open feedback PR <code>docs: open feedback PR for [RFC-NNNN\\|architecture\\|...]</code>"},{"location":"submission-workflow/#status-values","title":"Status Values","text":"Document Type Status Values RFC Open for Review \u2192 Approved/Rejected Architecture Active \u2192 Implemented/Superseded ADR Open for Review \u2192 Accepted/Rejected/Superseded"},{"location":"submission-workflow/#tips","title":"Tips","text":"<ul> <li>Use descriptive branch names for PR #1 (not document numbers for RFCs/ADRs) to avoid conflicts when multiple people work simultaneously. PR #2 uses the document number since it's already assigned</li> <li>Check RFCs and ADRs tables on their respective pages before choosing a number to ensure uniqueness</li> <li>Keep PR #1 focused on publishing only; save technical discussions for PR #2</li> <li>Update status in document tables when document reaches final state (Approved, Accepted, Rejected, etc.)</li> <li>Link related documents (e.g., link architecture plan back to source RFC)</li> </ul>"},{"location":"technical-documentation-overview/","title":"Documentation","text":"<p>This directory contains technical documentation for the Sage monorepo.</p>"},{"location":"technical-documentation-overview/#documentation-structure","title":"Documentation Structure","text":""},{"location":"technical-documentation-overview/#rfcs-request-for-comments","title":"RFCs (Request for Comments)","text":"<p>Proposals for significant system changes that require review and feedback.</p> <ul> <li>Purpose: Gather feedback on proposed designs before implementation</li> <li>Audience: All stakeholders (engineering, product, architecture)</li> <li>Lifecycle: Open for Review \u2192 Approved/Rejected (stays in <code>docs/rfcs/</code> as historical record)</li> </ul>"},{"location":"technical-documentation-overview/#architecture","title":"Architecture","text":"<p>Architecture plans, design documents, and system diagrams for implementation.</p> <ul> <li>Purpose: Document detailed designs and system architecture</li> <li>Audience: Engineers implementing features, new team members</li> <li>Lifecycle: Active \u2192 Implemented/Superseded</li> <li>Diagrams: Stored in <code>docs/architecture/diagrams/</code> (supports <code>.gif</code>, <code>.png</code>, <code>.svg</code>, <code>.html</code>)</li> </ul>"},{"location":"technical-documentation-overview/#adrs-architecture-decision-records","title":"ADRs (Architecture Decision Records)","text":"<p>Lightweight records of specific architectural decisions.</p> <ul> <li>Purpose: Capture context and rationale for technical decisions</li> <li>Audience: Current and future engineers</li> <li>Lifecycle: Immutable once accepted (superseded by new ADRs if needed)</li> </ul>"},{"location":"technical-documentation-overview/#documentation-workflow","title":"Documentation Workflow","text":"<pre><code>graph TD\n    A[New Idea] --&gt; B[Create RFC&lt;br/&gt;docs/rfcs/]\n    B --&gt;|Approved| C[Create Architecture Plan&lt;br/&gt;docs/architecture/]\n    B --&gt;|Rejected| D[Mark as&lt;br/&gt;Rejected]\n    C --&gt; E[Implement &amp;&lt;br/&gt;Update]\n    E --&gt; F[Create ADRs&lt;br/&gt;docs/adr/&lt;br/&gt;as needed]</code></pre> <p>Note on workflow</p> <p>While the diagram shows a linear progression, in practice you may draft architecture plans while developing your RFC to clarify your thinking. However, only submit the RFC for initial review. The detailed architecture plan should be created as a NEW document in <code>docs/architecture/</code> only after the RFC is approved (the RFC remains in <code>docs/rfcs/</code> as historical record). This keeps the review process focused and avoids investing heavily in implementation details before gaining consensus on the approach.</p>"},{"location":"technical-documentation-overview/#quick-links","title":"Quick Links","text":"<ul> <li>RFCs</li> <li>Architecture Plans</li> <li>ADRs</li> </ul>"},{"location":"technical-documentation-overview/#submission-process","title":"Submission Process","text":"<p>All technical documents follow a two-PR workflow:</p> <ol> <li>PR #1: Publish document to docs site (quick merge after compliance check)</li> <li>PR #2: Collect feedback from team (detailed technical review)</li> </ol> <p>See the Documentation Submission Workflow for complete step-by-step instructions.</p>"},{"location":"technical-documentation-overview/#contributing","title":"Contributing","text":""},{"location":"technical-documentation-overview/#creating-documentation","title":"Creating Documentation","text":"<ol> <li>Proposing Changes: Create an RFC for significant features or system changes</li> <li>Architecture Plans: Document approved designs with detailed implementation guidance</li> <li>Recording Decisions: Create an ADR for specific technical decisions</li> </ol>"},{"location":"technical-documentation-overview/#submission-process_1","title":"Submission Process","text":"<p>Follow the two-PR workflow for all document submissions:</p> <ul> <li>First PR publishes to docs site</li> <li>Second PR collects team feedback</li> </ul>"},{"location":"technical-documentation-overview/#maintaining-documentation","title":"Maintaining Documentation","text":"<ul> <li>Keep architecture documents current as implementation evolves</li> <li>Use clear, concise language</li> <li>Include diagrams where helpful (store in <code>docs/architecture/diagrams/</code>)</li> <li>Link to related documents and issues</li> <li>Follow the templates provided in each directory</li> </ul>"},{"location":"adr/","title":"Architecture Decision Records (ADRs)","text":"<p>This directory contains lightweight records of architectural decisions made during development.</p>"},{"location":"adr/#what-is-an-adr","title":"What is an ADR?","text":"<p>An Architecture Decision Record (ADR) captures a single architectural decision and its context. ADRs are:</p> <ul> <li>Immutable: Once accepted, they are not modified (superseded by new ADRs instead)</li> <li>Focused: Each ADR covers one decision</li> <li>Concise: Brief format focusing on decision and rationale</li> </ul>"},{"location":"adr/#adr-vs-rfc","title":"ADR vs RFC","text":"<ul> <li>ADR: Records a decision already made or being made (lightweight, focused)</li> <li>RFC: Proposes a comprehensive solution for review (detailed, exploratory)</li> </ul> <p>Use ADRs for specific technical decisions. Use RFCs for major features or system changes.</p>"},{"location":"adr/#format","title":"Format","text":"<p>ADRs use the format:</p> <ul> <li><code>NNNN-title-of-decision.md</code> (e.g., <code>0001-use-docker-for-lambda.md</code>)</li> <li>Numbered sequentially</li> <li>Brief title in kebab-case</li> </ul>"},{"location":"adr/#adrs","title":"ADRs","text":"ADR Title Status Date - - - -"},{"location":"adr/#template","title":"Template","text":"<p>See template.md for the ADR template.</p>"},{"location":"adr/#submitting-an-adr","title":"Submitting an ADR","text":"<p>See the Documentation Submission Workflow guide for detailed step-by-step instructions on the two-PR process.</p>"},{"location":"adr/#related-documentation","title":"Related Documentation","text":"<ul> <li>See RFCs for proposals under review</li> <li>See Architecture Plans for approved designs</li> </ul>"},{"location":"adr/#references","title":"References","text":"<ul> <li>ADR GitHub Organization</li> <li>Documenting Architecture Decisions</li> <li>AWS ADR Examples</li> </ul>"},{"location":"adr/template/","title":"[ADR Number]: [Brief Title]","text":"<ul> <li>Status: open for review | accepted | rejected | deprecated | superseded by [ADR-NNNN]</li> <li>Date: YYYY-MM-DD</li> <li>Decision Makers: [List of people involved]</li> <li>Tags: [e.g., backend, infrastructure, security]</li> </ul>"},{"location":"adr/template/#context","title":"Context","text":"<p>[Describe the context and problem statement. What forces are at play? What are the constraints?]</p>"},{"location":"adr/template/#decision","title":"Decision","text":"<p>[State the decision clearly. Use active voice: \"We will...\"]</p>"},{"location":"adr/template/#rationale","title":"Rationale","text":"<p>[Explain why this decision was made. What are the benefits? What trade-offs were accepted?]</p>"},{"location":"adr/template/#consequences","title":"Consequences","text":""},{"location":"adr/template/#positive","title":"Positive","text":"<ul> <li>[Benefit 1]</li> <li>[Benefit 2]</li> </ul>"},{"location":"adr/template/#negative","title":"Negative","text":"<ul> <li>[Drawback 1]</li> <li>[Drawback 2]</li> </ul>"},{"location":"adr/template/#neutral","title":"Neutral","text":"<ul> <li>[Change 1]</li> <li>[Change 2]</li> </ul>"},{"location":"adr/template/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/template/#option-1-name","title":"Option 1: [Name]","text":"<p>[Brief description]</p> <p>Rejected because: [Reason]</p>"},{"location":"adr/template/#option-2-name","title":"Option 2: [Name]","text":"<p>[Brief description]</p> <p>Rejected because: [Reason]</p>"},{"location":"adr/template/#related-decisions","title":"Related Decisions","text":"<ul> <li>[ADR-NNNN]: [Related decision]</li> <li>[RFC-NNNN]: [Related proposal]</li> </ul>"},{"location":"adr/template/#notes","title":"Notes","text":"<p>[Any additional notes, links, or references]</p>"},{"location":"api/agora/","title":"Agora API","text":"<p>Version: 1.0.0</p>"},{"location":"api/agora/#servers","title":"Servers","text":"<ul> <li>Server: <code>http://localhost/v1</code></li> </ul>"},{"location":"api/agora/#api-endpoints","title":"API Endpoints","text":"<p>This API provides 11 endpoints:</p>"},{"location":"api/agora/#biodomains","title":"BioDomains","text":"<ul> <li>GET <code>/biodomains</code>   List BioDomains</li> </ul> <ul> <li> <p>GET <code>/biodomains/{ensg}</code>   Retrieve bioDomain for a given ENSG</p> <p>Get bioDomain</p> </li> </ul>"},{"location":"api/agora/#genes","title":"Genes","text":"<ul> <li> <p>GET <code>/genes</code>   Retrieve a list of genes or filter by Ensembl gene IDs</p> <p>This endpoint returns all genes or filters genes by Ensembl gene IDs if provided.</p> </li> </ul> <ul> <li>GET <code>/genes/{ensg}</code>   Get gene details by Ensembl Gene ID</li> </ul> <ul> <li>GET <code>/genes/search</code>   Search Genes</li> </ul> <ul> <li>GET <code>/genes/comparison</code>   Get comparison genes based on category and subcategory</li> </ul> <ul> <li> <p>GET <code>/genes/nominated</code>   Get nominated genes</p> <p>Retrieves a list of genes with nominations and relevant information.</p> </li> </ul>"},{"location":"api/agora/#dataversion","title":"Dataversion","text":"<ul> <li>GET <code>/dataversion</code>   Get dataversion</li> </ul>"},{"location":"api/agora/#distribution","title":"Distribution","text":"<ul> <li>GET <code>/distribution</code>   Get distribution data</li> </ul>"},{"location":"api/agora/#teams","title":"Teams","text":"<ul> <li>GET <code>/teams</code>   List Teams</li> </ul> <ul> <li>GET <code>/teamMembers/{name}/image</code>   Get Team Member Image</li> </ul>"},{"location":"api/agora/#interactive-documentation","title":"Interactive Documentation","text":"<p>For detailed API documentation with interactive examples, see:</p> <ul> <li>agora API Docs</li> </ul>"},{"location":"api/agora/#openapi-specification","title":"OpenAPI Specification","text":"<ul> <li>OpenAPI Spec (YAML)</li> </ul> <p>This documentation was automatically generated from the OpenAPI specification. Last updated: 2025-08-23T22:04:00.359Z</p>"},{"location":"api/bixarena/","title":"BixArena API","text":"<p>Version: 1.0.0</p> <p>Advance bioinformatics by evaluating and ranking AI agents.</p>"},{"location":"api/bixarena/#servers","title":"Servers","text":"<ul> <li>Server: <code>http://localhost/v1</code></li> </ul>"},{"location":"api/bixarena/#api-endpoints","title":"API Endpoints","text":"<p>This API provides 4 endpoints:</p>"},{"location":"api/bixarena/#leaderboard","title":"Leaderboard","text":"<ul> <li> <p>GET <code>/leaderboards</code>   List all available leaderboards</p> <p>Get a list of all available leaderboards with their metadata</p> </li> </ul> <ul> <li> <p>GET <code>/leaderboards/{leaderboardId}</code>   Get leaderboard entries</p> <p>Get paginated leaderboard entries for a specific leaderboard</p> </li> </ul> <ul> <li> <p>GET <code>/leaderboards/{leaderboardId}/history/{modelId}</code>   Get model performance history</p> <p>Get historical performance data for a specific model in a leaderboard</p> </li> </ul> <ul> <li> <p>GET <code>/leaderboards/{leaderboardId}/snapshots</code>   Get leaderboard snapshots</p> <p>Get a paginated list of available snapshots for a leaderboard</p> </li> </ul>"},{"location":"api/bixarena/#interactive-documentation","title":"Interactive Documentation","text":"<p>For detailed API documentation with interactive examples, see:</p> <ul> <li>bixarena API Docs</li> </ul>"},{"location":"api/bixarena/#openapi-specification","title":"OpenAPI Specification","text":"<ul> <li>OpenAPI Spec (YAML)</li> </ul> <p>This documentation was automatically generated from the OpenAPI specification. Last updated: 2025-08-23T22:04:00.349Z</p>"},{"location":"api/model-ad/","title":"Model-AD API","text":"<p>Version: 1.0.0</p>"},{"location":"api/model-ad/#servers","title":"Servers","text":"<ul> <li>Server: <code>http://localhost/v1</code></li> </ul>"},{"location":"api/model-ad/#api-endpoints","title":"API Endpoints","text":"<p>This API provides 4 endpoints:</p>"},{"location":"api/model-ad/#dataversion","title":"Dataversion","text":"<ul> <li>GET <code>/dataversion</code>   Get dataversion</li> </ul>"},{"location":"api/model-ad/#models","title":"Models","text":"<ul> <li> <p>GET <code>/models/{name}</code>   Get details for a specific model</p> <p>Retrieve detailed information for a specific model by its name</p> </li> </ul>"},{"location":"api/model-ad/#comparisontoolconfig","title":"ComparisonToolConfig","text":"<ul> <li> <p>GET <code>/comparison-tools/config</code>   Get Comparison Tool configuration</p> <p>Retrieve the Comparison Tool configuration schema for the Model-AD application</p> </li> </ul>"},{"location":"api/model-ad/#modeloverview","title":"ModelOverview","text":"<ul> <li> <p>GET <code>/comparison-tools/model-overview</code>   Get model overview for comparison tools</p> <p>Returns a list of model overview objects for use in comparison tools.</p> </li> </ul>"},{"location":"api/model-ad/#interactive-documentation","title":"Interactive Documentation","text":"<p>For detailed API documentation with interactive examples, see:</p> <ul> <li>model-ad API Docs</li> </ul>"},{"location":"api/model-ad/#openapi-specification","title":"OpenAPI Specification","text":"<ul> <li>OpenAPI Spec (YAML)</li> </ul> <p>This documentation was automatically generated from the OpenAPI specification. Last updated: 2025-08-23T22:04:00.347Z</p>"},{"location":"api/openchallenges/","title":"OpenChallenges API","text":"<p>Version: 1.0.0</p> <p>Discover, explore, and contribute to open biomedical challenges.</p>"},{"location":"api/openchallenges/#servers","title":"Servers","text":"<ul> <li>Server: <code>https://openchallenges.io/api/v1</code></li> </ul>"},{"location":"api/openchallenges/#api-endpoints","title":"API Endpoints","text":"<p>This API provides 17 endpoints:</p>"},{"location":"api/openchallenges/#authentication","title":"Authentication","text":"<ul> <li> <p>POST <code>/auth/login</code>   User login</p> <p>Authenticate user and return JWT token</p> </li> </ul>"},{"location":"api/openchallenges/#api-key","title":"API Key","text":"<ul> <li> <p>GET <code>/auth/api-keys</code>   List API keys</p> <p>Get all API keys for the authenticated user</p> </li> </ul> <ul> <li> <p>POST <code>/auth/api-keys</code>   Create API key</p> <p>Generate a new API key for the authenticated user</p> </li> </ul> <ul> <li> <p>DELETE <code>/auth/api-keys/{keyId}</code>   Delete API key</p> <p>Revoke an API key</p> </li> </ul>"},{"location":"api/openchallenges/#challenge","title":"Challenge","text":"<ul> <li>GET <code>/challenges</code>   List challenges</li> </ul> <ul> <li> <p>POST <code>/challenges</code>   Create a challenge</p> <p>Create a challenge with the specified details</p> </li> </ul> <ul> <li> <p>GET <code>/challenges/{challengeId}</code>   Get a challenge</p> <p>Returns the challenge specified</p> </li> </ul> <ul> <li> <p>PUT <code>/challenges/{challengeId}</code>   Update an existing challenge</p> <p>Updates an existing challenge.</p> </li> </ul> <ul> <li> <p>DELETE <code>/challenges/{challengeId}</code>   Delete a challenge</p> <p>Deletes a challenge by its unique ID.</p> </li> </ul> <ul> <li> <p>GET <code>/challenges/{challengeId}/json-ld</code>   Get a challenge in JSON-LD format</p> <p>Returns the challenge specified in JSON-LD format</p> </li> </ul>"},{"location":"api/openchallenges/#challenge-contribution","title":"Challenge Contribution","text":"<ul> <li>GET <code>/challenges/{challengeId}/contributions</code>   List challenge contributions</li> </ul> <ul> <li> <p>POST <code>/challenges/{challengeId}/contributions</code>   Create a new contribution for a challenge</p> <p>Creates a new contribution record associated with a challenge ID.</p> </li> </ul> <ul> <li> <p>DELETE <code>/challenges/{challengeId}/contributions/{organizationId}/role/{role}</code>   Delete a specific challenge contribution</p> <p>Delete a specific challenge contribution.</p> </li> </ul>"},{"location":"api/openchallenges/#challenge-analytics","title":"Challenge Analytics","text":"<ul> <li> <p>GET <code>/challenge-analytics/challenges-per-year</code>   Get the number of challenges tracked per year</p> <p>Returns the number of challenges tracked per year</p> </li> </ul>"},{"location":"api/openchallenges/#challenge-platform","title":"Challenge Platform","text":"<ul> <li>GET <code>/challenge-platforms</code>   List challenge platforms</li> </ul> <ul> <li> <p>POST <code>/challenge-platforms</code>   Create a challenge platform</p> <p>Create a challenge platform with the specified ID</p> </li> </ul> <ul> <li> <p>GET <code>/challenge-platforms/{challengePlatformId}</code>   Get a challenge platform</p> <p>Returns the challenge platform identified by its unique ID</p> </li> </ul> <ul> <li> <p>PUT <code>/challenge-platforms/{challengePlatformId}</code>   Update an existing challenge platform</p> <p>Updates an existing challenge platform.</p> </li> </ul> <ul> <li> <p>DELETE <code>/challenge-platforms/{challengePlatformId}</code>   Delete a challenge platform</p> <p>Deletes a challenge platform by its unique ID. This action is irreversible.</p> </li> </ul>"},{"location":"api/openchallenges/#edam-concept","title":"Edam Concept","text":"<ul> <li>GET <code>/edam-concepts</code>   List EDAM concepts</li> </ul>"},{"location":"api/openchallenges/#image","title":"Image","text":"<ul> <li> <p>GET <code>/images</code>   Get an image</p> <p>Returns the image specified.</p> </li> </ul>"},{"location":"api/openchallenges/#organization","title":"Organization","text":"<ul> <li>GET <code>/organizations</code>   List organizations</li> </ul> <ul> <li> <p>POST <code>/organizations</code>   Create an organization</p> <p>Create an organization with the specified account name</p> </li> </ul> <ul> <li> <p>GET <code>/organizations/{org}</code>   Get an organization</p> <p>Returns the organization identified by its login or ID.</p> </li> </ul> <ul> <li> <p>PUT <code>/organizations/{org}</code>   Update an existing organization</p> <p>Updates an existing organization.</p> </li> </ul> <ul> <li> <p>DELETE <code>/organizations/{org}</code>   Delete an organization</p> <p>Deletes the organization specified by its login or ID.</p> </li> </ul>"},{"location":"api/openchallenges/#authentication_1","title":"Authentication","text":"<ul> <li>apiBearerAuth: http - API key obtained from /auth/login endpoint</li> </ul>"},{"location":"api/openchallenges/#interactive-documentation","title":"Interactive Documentation","text":"<p>For detailed API documentation with interactive examples, see:</p> <ul> <li>openchallenges API Docs</li> </ul>"},{"location":"api/openchallenges/#openapi-specification","title":"OpenAPI Specification","text":"<ul> <li>OpenAPI Spec (YAML)</li> </ul> <p>This documentation was automatically generated from the OpenAPI specification. Last updated: 2025-08-23T22:04:00.344Z</p>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/synapse/","title":"Synapse REST API","text":"<p>Version: 0.1.0</p>"},{"location":"api/synapse/#servers","title":"Servers","text":"<ul> <li>Server: <code>http://localhost/api/v1</code></li> </ul>"},{"location":"api/synapse/#api-endpoints","title":"API Endpoints","text":"<p>This API provides 1 endpoints:</p>"},{"location":"api/synapse/#challenge","title":"Challenge","text":"<ul> <li>GET <code>/challenge</code>   List the Challenges for which the given participant is registered.   List the Challenges for which the given participant is registered.   To be in the returned list the caller must have READ permission on the   project associated with the Challenge.</li> </ul>"},{"location":"api/synapse/#interactive-documentation","title":"Interactive Documentation","text":"<p>For detailed API documentation with interactive examples, see:</p> <ul> <li>synapse API Docs</li> </ul>"},{"location":"api/synapse/#openapi-specification","title":"OpenAPI Specification","text":"<ul> <li>OpenAPI Spec (YAML)</li> </ul> <p>This documentation was automatically generated from the OpenAPI specification. Last updated: 2025-08-23T22:04:00.336Z</p>"},{"location":"architecture/","title":"Architecture Documentation","text":"<p>This directory contains architecture plans, design documents, and system diagrams for implementation or in active development.</p>"},{"location":"architecture/#process","title":"Process","text":"<p>Architecture documents typically originate from approved RFCs in <code>docs/rfcs/</code>. Once an RFC is approved, a detailed architecture plan is created here as a new document that references the source RFC (the RFC remains in <code>docs/rfcs/</code> as historical record).</p>"},{"location":"architecture/#architecture-documents","title":"Architecture Documents","text":"Document Status Date Description Python FastAPI Microservice with JWT Authentication Implemented 2025-12-01 JWT authentication integration with Python AI service"},{"location":"architecture/#architecture-diagrams","title":"Architecture Diagrams","text":"<p>Diagrams are stored in the docs/architecture/diagrams/ subdirectory.</p> Diagram Description Last Updated BixArena Architecture Current system architecture 2026-01-26 OpenChallenges Architecture Current system architecture 2026-01-26"},{"location":"architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>architecture/\n\u251c\u2500\u2500 README.md                           # This file\n\u251c\u2500\u2500 diagrams/                          # Architecture diagrams (*.gif, *.png, *.svg, *.html)\n\u2502   \u251c\u2500\u2500 bixarena-architecture.gif\n\u2502   \u2514\u2500\u2500 bixarena-architecture-v1.gif\n\u2514\u2500\u2500 *.md                               # Architecture documents\n</code></pre>"},{"location":"architecture/#template","title":"Template","text":"<p>See template.md for the architecture plan template.</p>"},{"location":"architecture/#submitting-an-architecture-plan","title":"Submitting an Architecture Plan","text":"<p>See the Documentation Submission Workflow guide for detailed step-by-step instructions on the two-PR process.</p>"},{"location":"architecture/#guidelines","title":"Guidelines","text":"<ul> <li>Documents in this directory are for implementation (source RFC was approved)</li> <li>Use past or present tense (not future/proposal tense)</li> <li>Keep documents updated as implementation evolves</li> <li>Reference related ADRs and RFCs for context</li> <li>Store diagrams in the <code>docs/architecture/diagrams/</code> subdirectory (supports <code>.gif</code>, <code>.png</code>, <code>.svg</code>, <code>.html</code> with Mermaid)</li> </ul>"},{"location":"architecture/#document-status-definitions","title":"Document Status Definitions","text":"<ul> <li>Active: Architecture plan is available and ready for implementation or in progress</li> <li>Implemented: Completed and deployed</li> <li>Superseded: Replaced by newer design (archive or add note)</li> </ul>"},{"location":"architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>See RFCs for proposals under review</li> <li>See ADRs for specific architectural decisions</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/","title":"BixArena Leaderboard Snapshot Automation Implementation Plan","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#overview","title":"Overview","text":"<p>Implement automated daily leaderboard snapshot generation using a hybrid approach:</p> <ul> <li>Primary: AWS Lambda function triggered by EventBridge (scheduled)</li> <li>Secondary: Protected API endpoint in AI service (on-demand)</li> <li>Shared: Core snapshot generation logic extracted from <code>bixarena-tools</code></li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Scheduled Execution                              \u2502\n\u2502                                                                          \u2502\n\u2502  EventBridge Rule \u2500\u2500&gt; Lambda Function \u2500\u2500&gt; RDS Proxy \u2500\u2500&gt; PostgreSQL      \u2502\n\u2502  (cron: daily 2 AM)   (leaderboard-      (connection   (bixarena DB)   \u2502\n\u2502                        snapshot-gen)      pooling)                       \u2502\n\u2502                            \u2502                                             \u2502\n\u2502                            \u2514\u2500\u2500&gt; SNS Topic \u2500\u2500&gt; Slack Lambda \u2500\u2500&gt; Slack    \u2502\n\u2502                                 (success/failure notifications)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        On-Demand Execution                               \u2502\n\u2502                                                                          \u2502\n\u2502  API Gateway \u2500\u2500&gt; AI Service \u2500\u2500&gt; Lambda (async) \u2500\u2500&gt; Returns 202          \u2502\n\u2502  (authenticated)  POST /admin/        \u2502              with correlation_id\u2502\n\u2502                   generate-           \u2502                                  \u2502\n\u2502                   snapshot            \u2514\u2500\u2500&gt; RDS Proxy \u2500\u2500&gt; PostgreSQL     \u2502\n\u2502                                       \u2514\u2500\u2500&gt; SNS \u2500\u2500&gt; Slack notification    \u2502\n\u2502                                                                          \u2502\n\u2502  Response: {\"correlation_id\": \"...\", \"status\": \"initiated\"}             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502  Shared Logic                \u2502\n                              \u2502  (bixarena-leaderboard)      \u2502\n                              \u2502                              \u2502\n                              \u2502 - fetch_data()               \u2502\n                              \u2502 - compute_bradley_terry()    \u2502\n                              \u2502 - save_snapshot()            \u2502\n                              \u2502 - auto_publish=True          \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Design Notes:</p> <ul> <li>Lambda currently generates \"overall\" leaderboard daily</li> <li>Extensible design: pass <code>leaderboard_id</code> parameter for future leaderboards</li> <li>Auto-publish enabled by default (sets <code>visible=true</code>)</li> <li>Slack notifications sent on both success and failure</li> <li>On-demand execution: Async Lambda invocation (returns immediately, no timeout issues)</li> <li>Metadata tracking: <code>trigger_source</code> (scheduled/api), <code>triggered_by</code> (user email), <code>correlation_id</code></li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#overview_1","title":"Overview","text":"<p>The leaderboard snapshot system implements defense-in-depth security with multiple layers:</p> <ol> <li>Authentication &amp; Authorization: JWT-based API authentication with admin-only access</li> <li>IAM Least Privilege: Lambda and ECS task roles scoped to minimum necessary permissions</li> <li>Secrets Management: Database credentials stored in AWS Secrets Manager</li> <li>Network Isolation: Lambda in VPC private subnet, RDS Proxy with IAM authentication</li> <li>Input Validation: Whitelist-based validation of all Lambda inputs</li> <li>Audit Logging: CloudWatch logs with correlation IDs for traceability</li> </ol>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#security-measures-by-component","title":"Security Measures by Component","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#1-lambda-function-security","title":"1. Lambda Function Security","text":"<p>IAM Execution Role (least privilege):</p> <pre><code># Lambda execution role with minimal permissions\nlambda_role = iam.Role(\n    self, \"SnapshotLambdaRole\",\n    assumed_by=iam.ServicePrincipal(\"lambda.amazonaws.com\"),\n    managed_policies=[\n        iam.ManagedPolicy.from_aws_managed_policy_name(\n            \"service-role/AWSLambdaVPCAccessExecutionRole\"\n        )\n    ],\n)\n\n# Grant ONLY what's needed:\ndb_secret.grant_read(lambda_role)  # Read DB credentials\nsnapshot_events_topic.grant_publish(lambda_role)  # Publish to SNS\nrds_proxy.grant_connect(lambda_role, \"bixarena_user\")  # RDS IAM auth\n</code></pre> <p>Network Security (VPC isolation):</p> <pre><code>snapshot_function = lambda_.DockerImageFunction(\n    self,\n    \"SnapshotFunction\",\n    vpc=vpc,\n    vpc_subnets=ec2.SubnetSelection(\n        subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS\n    ),\n    security_groups=[lambda_security_group],\n)\n\n# Security group: outbound to RDS only\nlambda_security_group.add_egress_rule(\n    peer=ec2.Peer.security_group_id(rds_security_group.security_group_id),\n    connection=ec2.Port.tcp(5432),\n    description=\"Allow Lambda to access RDS via proxy\"\n)\n</code></pre> <p>Input Validation:</p> <pre><code>def validate_inputs(event: dict) -&gt; None:\n    \"\"\"Validate and sanitize Lambda event inputs.\"\"\"\n    # Whitelist allowed leaderboards\n    allowed_leaderboards = [\"overall\", \"open-source\", \"multimodal\"]\n    leaderboard_id = event.get(\"leaderboard_id\", \"overall\")\n    if leaderboard_id not in allowed_leaderboards:\n        raise ValueError(f\"Invalid leaderboard_id: {leaderboard_id}\")\n\n    # Validate bootstrap iterations (reasonable range)\n    num_bootstrap = event.get(\"num_bootstrap\", 100)\n    if not isinstance(num_bootstrap, int) or num_bootstrap &lt; 10 or num_bootstrap &gt; 10000:\n        raise ValueError(f\"Invalid num_bootstrap: {num_bootstrap}\")\n\n    # Validate trigger source\n    trigger_source = event.get(\"trigger_source\", \"scheduled\")\n    if trigger_source not in [\"scheduled\", \"api\"]:\n        raise ValueError(f\"Invalid trigger_source: {trigger_source}\")\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#2-ai-service-api-security","title":"2. AI Service API Security","text":"<p>Authentication (JWT with admin role):</p> <pre><code>async def validate_jwt_admin(\n    authorization: str = Header(...),\n) -&gt; str:\n    \"\"\"Validate JWT and ensure user has admin role.\"\"\"\n    payload = validate_jwt(authorization)  # Verify signature, expiry\n\n    # Check admin role\n    roles = payload.get(\"cognito:groups\", [])\n    if \"admin\" not in roles:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Admin role required for snapshot generation\"\n        )\n\n    return payload.get(\"email\")  # Return email for audit logging\n</code></pre> <p>Lambda Invocation Permission (scoped IAM):</p> <pre><code># AI service ECS task role\nai_service_task_role = iam.Role.from_role_arn(\n    self, \"AIServiceTaskRole\",\n    role_arn=\"arn:aws:iam::123456789012:role/BixArenaAIServiceTaskRole\"\n)\n\n# Grant permission to invoke ONLY this specific Lambda\nsnapshot_function.grant_invoke(ai_service_task_role)\n\n# Generated policy:\n# {\n#   \"Effect\": \"Allow\",\n#   \"Action\": \"lambda:InvokeFunction\",\n#   \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:snapshot-gen\"\n# }\n</code></pre> <p>Rate Limiting (API Gateway):</p> <pre><code># Prevent abuse of manual snapshot generation\napi_gateway.add_usage_plan(\n    \"SnapshotAPIUsagePlan\",\n    throttle={\n        \"rate_limit\": 10,  # 10 requests per second\n        \"burst_limit\": 20,\n    },\n    quota={\n        \"limit\": 100,  # 100 requests per day\n        \"period\": apigateway.Period.DAY,\n    },\n)\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#3-database-security","title":"3. Database Security","text":"<p>Secrets Manager (no hardcoded credentials):</p> <pre><code># Database credentials in Secrets Manager\ndb_secret = secretsmanager.Secret.from_secret_name_v2(\n    self,\n    \"DatabaseSecret\",\n    secret_name=\"bixarena/rds/credentials\"\n)\n\n# Lambda retrieves credentials at runtime\ndef get_database_url():\n    secret_arn = os.environ[\"DB_SECRET_ARN\"]\n    client = boto3.client(\"secretsmanager\")\n    response = client.get_secret_value(SecretId=secret_arn)\n    secret = json.loads(response[\"SecretString\"])\n\n    return (\n        f\"postgresql://{secret['username']}:{secret['password']}\"\n        f\"@{secret['host']}:{secret['port']}/{secret['dbname']}\"\n    )\n</code></pre> <p>RDS Proxy with IAM Authentication (no password needed):</p> <pre><code>rds_proxy = rds.DatabaseProxy(\n    self, \"RDSProxy\",\n    proxy_target=rds.ProxyTarget.from_cluster(db_cluster),\n    secrets=[db_secret],\n    vpc=vpc,\n    require_tls=True,  # Enforce TLS encryption\n    iam_auth=True,     # IAM database authentication\n)\n\n# Lambda connects via IAM (no password in environment)\nrds_proxy.grant_connect(snapshot_function, \"bixarena_user\")\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#4-audit-logging-monitoring","title":"4. Audit Logging &amp; Monitoring","text":"<p>Structured Logging with Metadata:</p> <pre><code>logger.info(\n    \"Snapshot generation started\",\n    extra={\n        \"trigger_source\": trigger_source,       # scheduled or api\n        \"triggered_by\": triggered_by,           # user email or \"system\"\n        \"correlation_id\": correlation_id,       # unique trace ID\n        \"leaderboard_id\": leaderboard_id,\n    }\n)\n</code></pre> <p>CloudWatch Alarms:</p> <pre><code># Alert on Lambda errors\nerror_alarm = cloudwatch.Alarm(\n    self,\n    \"SnapshotErrorAlarm\",\n    metric=snapshot_function.metric_errors(),\n    threshold=1,\n    evaluation_periods=1,\n    alarm_description=\"Leaderboard snapshot generation failed\",\n)\nerror_alarm.add_alarm_action(cloudwatch_actions.SnsAction(alert_topic))\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#security-checklist","title":"Security Checklist","text":"<ul> <li>\u2705 IAM: Least privilege roles for Lambda and ECS tasks</li> <li>\u2705 Secrets: Database credentials in Secrets Manager (or IAM auth)</li> <li>\u2705 Network: Lambda in private VPC subnet with security groups</li> <li>\u2705 Input Validation: Whitelist allowed values, validate types and ranges</li> <li>\u2705 Authentication: API endpoint requires JWT with admin role</li> <li>\u2705 Authorization: ECS task role scoped to invoke only this Lambda</li> <li>\u2705 Audit Logging: CloudWatch logs with trigger_source, triggered_by, correlation_id</li> <li>\u2705 Monitoring: CloudWatch alarms on Lambda errors</li> <li>\u2705 Encryption: TLS for RDS connection, encrypted secrets at rest</li> <li>\u2705 Rate Limiting: API Gateway throttling prevents abuse</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#part-1-lambda-function-packaging-options","title":"Part 1: Lambda Function Packaging Options","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#option-a-container-image-docker-recommended","title":"Option A: Container Image (Docker) \u2b50 RECOMMENDED","text":"<p>Packaging Approach:</p> <pre><code># Lambda Dockerfile\nFROM public.ecr.aws/lambda/python:3.13\n\n# Copy shared snapshot logic\nCOPY --from=build /bixarena_snapshot_lib /var/task/\n\n# Copy Lambda handler\nCOPY lambda_function.py /var/task/\n\n# Set handler\nCMD [\"lambda_function.handler\"]\n</code></pre> <p>Pros:</p> <ul> <li>\u2705 Full dependency control: No 250MB deployment package limit</li> <li>\u2705 Native dependencies: NumPy, SciPy compile correctly</li> <li>\u2705 Familiar tooling: Same Docker workflow as other services</li> <li>\u2705 Testing: Run locally with <code>docker run</code> or SAM CLI</li> <li>\u2705 Reusability: Base image can be shared across multiple Lambdas</li> <li>\u2705 No layer management: Everything bundled in one artifact</li> </ul> <p>Cons:</p> <ul> <li>\u26a0\ufe0f Cold start: ~2-3 seconds (vs. ~500ms for zip)</li> <li>\u26a0\ufe0f Image size: Up to 10GB (vs. 250MB for zip)</li> <li>\u26a0\ufe0f Build complexity: Requires Docker build in CI/CD</li> <li>\u26a0\ufe0f ECR dependency: Must push to ECR before deployment</li> </ul> <p>When to Use:</p> <ul> <li>Heavy dependencies (NumPy, SciPy, pandas)</li> <li>Need native libraries (psycopg binary mode)</li> <li>Long-running tasks (snapshot generation takes &gt;30 seconds)</li> <li>Complex build requirements</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#option-b-zip-archive-with-lambda-layers","title":"Option B: Zip Archive with Lambda Layers","text":"<p>Packaging Approach:</p> <pre><code># CDK code\nlambda_function = aws_lambda.Function(\n    self, \"LeaderboardSnapshotFunction\",\n    runtime=aws_lambda.Runtime.PYTHON_3_13,\n    code=aws_lambda.Code.from_asset(\n        \"../../tools/lambda\",\n        bundling=BundlingOptions(\n            image=Runtime.PYTHON_3_13.bundling_image,\n            command=[\"pip\", \"install\", \"-r\", \"requirements.txt\", \"-t\", \"/asset-output\"]\n        )\n    ),\n    layers=[numpy_scipy_layer]  # Pre-built layer for heavy deps\n)\n</code></pre> <p>Pros:</p> <ul> <li>\u2705 Fast cold start: ~500ms with layers</li> <li>\u2705 Simpler deployment: No Docker registry needed</li> <li>\u2705 CDK bundling: Automatic packaging via CDK</li> <li>\u2705 Layer caching: Heavy deps (NumPy/SciPy) cached across functions</li> </ul> <p>Cons:</p> <ul> <li>\u26a0\ufe0f Size limits: 250MB unzipped (50MB zipped)</li> <li>\u26a0\ufe0f Dependency conflicts: Layer versions must match</li> <li>\u26a0\ufe0f Complex bundling: pip install with --platform linux_x86_64</li> <li>\u26a0\ufe0f Layer management: Separate updates for dependencies</li> </ul> <p>When to Use:</p> <ul> <li>Simple dependencies</li> <li>Small codebase</li> <li>Prioritize cold start performance</li> <li>No native compilation requirements</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#option-c-zip-with-inline-bundling-no-layers","title":"Option C: Zip with Inline Bundling (No Layers)","text":"<p>Packaging Approach:</p> <pre><code># CDK with asset bundling\nlambda_function = aws_lambda.Function(\n    self, \"LeaderboardSnapshotFunction\",\n    runtime=aws_lambda.Runtime.PYTHON_3_13,\n    code=aws_lambda.Code.from_asset(\n        \"../../tools/dist/lambda.zip\"  # Pre-built by nx\n    )\n)\n</code></pre> <p>Pros:</p> <ul> <li>\u2705 Simplest approach: One zip file, no layers</li> <li>\u2705 Self-contained: All dependencies included</li> <li>\u2705 No layer versioning: Everything updates together</li> </ul> <p>Cons:</p> <ul> <li>\u26a0\ufe0f Every deployment: Re-uploads all dependencies</li> <li>\u26a0\ufe0f Size limits: Still 250MB unzipped</li> <li>\u26a0\ufe0f Slow deployment: Larger package = longer upload</li> </ul> <p>When to Use:</p> <ul> <li>Proof of concept</li> <li>Very small dependencies</li> <li>Infrequent updates</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#recommendation-option-a-container-image","title":"Recommendation: Option A - Container Image","text":"<p>Rationale:</p> <ol> <li>Your dependencies require it: <code>numpy</code>, <code>scipy</code>, <code>psycopg[binary]</code> are heavy (~150MB)</li> <li>Consistent with existing workflow: Already using Docker for all services</li> <li>Future-proof: Easy to add more analysis libraries</li> <li>Testing: Can run locally with same image</li> <li>Build pipeline: Already have Docker build expertise</li> </ol> <p>Estimated sizes:</p> <ul> <li>Base Python 3.13 Lambda image: ~500MB</li> <li> <ul> <li>NumPy, SciPy: ~150MB</li> </ul> </li> <li> <ul> <li>psycopg: ~20MB</li> </ul> </li> <li> <ul> <li>Your code: ~5MB</li> </ul> </li> <li>Total: ~675MB (well under 10GB limit)</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#part-2-detailed-implementation-plan","title":"Part 2: Detailed Implementation Plan","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#phase-1-extract-shared-logic-library","title":"Phase 1: Extract Shared Logic Library","text":"<p>Goal: Create reusable library for leaderboard snapshot generation logic</p>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#11-motivation","title":"1.1 Motivation","text":"<p>The current implementation in <code>apps/bixarena/tools/</code> contains business logic (Bradley-Terry ranking, snapshot generation) mixed with CLI tooling. Following monorepo best practices:</p> <p>Current state (anti-pattern):</p> <pre><code>apps/bixarena/tools/          # App containing business logic \u274c\n\u2514\u2500\u2500 bixarena_tools/\n    \u2514\u2500\u2500 leaderboard/\n        \u251c\u2500\u2500 db_helper.py      # Database queries (business logic)\n        \u251c\u2500\u2500 rank_battle.py    # Bradley-Terry algorithm (business logic)\n        \u2514\u2500\u2500 leaderboard.py    # CLI + business logic mixed\n</code></pre> <p>Problems:</p> <ul> <li>\u274c Apps cannot depend on other apps (violates monorepo rules)</li> <li>\u274c Business logic trapped in CLI tool, not reusable</li> <li>\u274c Lambda and AI service cannot share code</li> <li>\u274c Testing requires running CLI interface</li> </ul> <p>Target state (following conventions):</p> <pre><code>libs/bixarena/leaderboard/    # Library with business logic \u2705\n\u2514\u2500\u2500 python/\n    \u2514\u2500\u2500 bixarena_leaderboard/\n        \u251c\u2500\u2500 snapshot.py       # Snapshot generation (business logic)\n        \u251c\u2500\u2500 ranking.py        # Bradley-Terry algorithm (business logic)\n        \u2514\u2500\u2500 database.py       # Database queries (business logic)\n\napps/bixarena/tools/          # App consuming library \u2705\n\u2514\u2500\u2500 bixarena_tools/\n    \u2514\u2500\u2500 leaderboard/\n        \u2514\u2500\u2500 leaderboard.py    # CLI interface only (depends on lib)\n</code></pre> <p>Benefits:</p> <ul> <li>\u2705 Libraries can be consumed by both apps and other libs</li> <li>\u2705 Business logic is reusable, testable, and versioned independently</li> <li>\u2705 Apps remain thin (UI/CLI/API only)</li> <li>\u2705 Clear separation of concerns</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#12-create-domain-specific-library-package","title":"1.2 Create Domain-Specific Library Package","text":"<p>Location: <code>libs/bixarena/leaderboard/python/</code></p> <p>Naming conventions:</p> <ul> <li>Nx project name: <code>bixarena-leaderboard-python</code> (matches folder path per naming rule)</li> <li>Python/uv project name: <code>bixarena-leaderboard</code> (no \"python\" suffix in Python ecosystem)</li> <li>Package name: <code>bixarena_leaderboard</code> (Python import name)</li> </ul> <p>Structure:</p> <pre><code>libs/bixarena/leaderboard/python/\n\u251c\u2500\u2500 pyproject.toml                    # Python package definition\n\u251c\u2500\u2500 project.json                      # Nx project configuration\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 bixarena_leaderboard/             # Package (importable code)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 snapshot.py                   # SnapshotGenerator, SnapshotConfig\n\u2502   \u251c\u2500\u2500 database.py                   # DB queries (from db_helper.py)\n\u2502   \u251c\u2500\u2500 ranking.py                    # Bradley-Terry (from rank_battle.py)\n\u2502   \u2514\u2500\u2500 config.py                     # Configuration dataclasses\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 test_snapshot.py\n    \u251c\u2500\u2500 test_ranking.py\n    \u2514\u2500\u2500 test_database.py\n</code></pre> <p>Core API:</p> <pre><code># bixarena_leaderboard/snapshot.py\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass SnapshotConfig:\n    \"\"\"Configuration for snapshot generation.\"\"\"\n    database_url: str\n    leaderboard_id: str = \"overall\"\n    num_bootstrap: int = 100\n    min_evaluations: int = 0\n    rank_by_significance: bool = False\n\nclass SnapshotGenerator:\n    \"\"\"Generate leaderboard snapshots from battle evaluations.\"\"\"\n\n    def __init__(self, config: SnapshotConfig):\n        self.config = config\n\n    def generate_snapshot(self, auto_publish: bool = True) -&gt; dict:\n        \"\"\"Generate a new snapshot and save to database.\n\n        Args:\n            auto_publish: If True, set snapshot visibility to public immediately\n\n        Returns:\n            Dict with snapshot_id, entry_count, evaluation_count\n        \"\"\"\n        # Extract from existing leaderboard.py:snapshot_add()\n        # 1. Fetch data from DB\n        # 2. Compute Bradley-Terry rankings\n        # 3. Insert snapshot and entries\n        # 4. Set visibility=True if auto_publish=True\n        # 5. Return metadata\n        pass\n</code></pre> <p>pyproject.toml:</p> <pre><code>[project]\nname = \"bixarena-leaderboard\"  # No \"python\" suffix in Python ecosystem\nversion = \"1.0.0\"\ndescription = \"BixArena leaderboard snapshot generation and ranking algorithms\"\nrequires-python = \"==3.13.3\"\ndependencies = [\n    \"numpy&gt;=1.26.0\",\n    \"psycopg[binary]&gt;=3.0.0\",\n    \"scipy&gt;=1.11.0\",\n]\n\n[tool.hatch.build.targets.wheel]\npackages = [\"bixarena_leaderboard\"]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n</code></pre> <p>project.json (Nx configuration):</p> <pre><code>{\n  \"name\": \"bixarena-leaderboard-python\",\n  \"$schema\": \"../../../../node_modules/nx/schemas/project-schema.json\",\n  \"sourceRoot\": \"libs/bixarena/leaderboard/python/bixarena_leaderboard\",\n  \"projectType\": \"library\",\n  \"targets\": {\n    \"build\": {\n      \"executor\": \"@nxlv/python:build\",\n      \"outputs\": [\"{projectRoot}/dist\"],\n      \"options\": {\n        \"outputPath\": \"{projectRoot}/dist\",\n        \"publish\": false,\n        \"lockedVersions\": true,\n        \"bundleLocalDependencies\": false\n      }\n    },\n    \"test\": {\n      \"executor\": \"@nxlv/python:run-commands\",\n      \"outputs\": [],\n      \"options\": {\n        \"command\": \"uv run pytest tests/\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    }\n  },\n  \"tags\": [\"type:library\", \"scope:backend\", \"language:python\", \"domain:leaderboard\"]\n}\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#13-migrate-existing-code","title":"1.3 Migrate Existing Code","text":"<p>Move from <code>apps/bixarena/tools/bixarena_tools/leaderboard/</code>:</p> <ul> <li>\u2705 <code>db_helper.py</code> \u2192 <code>libs/bixarena/leaderboard/python/bixarena_leaderboard/database.py</code></li> <li>\u2705 <code>rank_battle.py</code> \u2192 <code>libs/bixarena/leaderboard/python/bixarena_leaderboard/ranking.py</code></li> <li>\u2705 Core logic from <code>leaderboard.py:snapshot_add()</code> \u2192 <code>bixarena_leaderboard/snapshot.py</code></li> </ul> <p>Keep in tools package (CLI becomes thin wrapper):</p> <pre><code># apps/bixarena/tools/bixarena_tools/leaderboard/leaderboard.py\nfrom bixarena_leaderboard import SnapshotGenerator, SnapshotConfig  # Import from lib\n\n@snapshot_app.command(\"add\")\ndef snapshot_add(...):\n    \"\"\"Create a new leaderboard snapshot.\"\"\"\n    with get_db_connection() as conn:\n        # Database connection still managed by CLI tool\n        database_url = build_database_url(conn)\n\n        # Use library for business logic\n        config = SnapshotConfig(\n            database_url=database_url,\n            leaderboard_id=id,\n            num_bootstrap=num_bootstrap,\n            min_evaluations=min_evals,\n            rank_by_significance=significant,\n        )\n\n        generator = SnapshotGenerator(config)\n        result = generator.generate_snapshot(auto_publish=True)\n\n        # CLI-specific display logic\n        display_leaderboard_summary(result)\n</code></pre> <p>Update tools pyproject.toml:</p> <pre><code>dependencies = [\n    # ... existing deps ...\n    \"bixarena-leaderboard\",  # New dependency on lib\n]\n\n[tool.uv.sources]\nbixarena-leaderboard = { workspace = true }\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#phase-2-lambda-function-implementation","title":"Phase 2: Lambda Function Implementation","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#21-lambda-handler-code","title":"2.1 Lambda Handler Code","text":"<pre><code>apps/bixarena/functions/leaderboard-snapshot/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 lambda_function.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 README.md\n</code></pre> <p>lambda_function.py:</p> <pre><code>\"\"\"Lambda handler for scheduled leaderboard snapshot generation.\"\"\"\nimport boto3\nimport json\nimport logging\nimport os\nimport time\nimport uuid\nfrom datetime import UTC, datetime\nfrom typing import Any\n\nfrom bixarena_leaderboard import SnapshotConfig, SnapshotGenerator\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nsns_client = boto3.client(\"sns\")\n\n\ndef validate_inputs(event: dict) -&gt; None:\n    \"\"\"Validate and sanitize Lambda event inputs.\"\"\"\n    # Whitelist allowed leaderboards\n    allowed_leaderboards = [\"overall\", \"open-source\", \"multimodal\"]\n    leaderboard_id = event.get(\"leaderboard_id\", \"overall\")\n    if leaderboard_id not in allowed_leaderboards:\n        raise ValueError(f\"Invalid leaderboard_id: {leaderboard_id}\")\n\n    # Validate bootstrap iterations (reasonable range)\n    num_bootstrap = event.get(\"num_bootstrap\", 100)\n    if not isinstance(num_bootstrap, int) or num_bootstrap &lt; 10 or num_bootstrap &gt; 10000:\n        raise ValueError(f\"Invalid num_bootstrap: {num_bootstrap}\")\n\n    # Validate trigger source\n    trigger_source = event.get(\"trigger_source\", \"scheduled\")\n    if trigger_source not in [\"scheduled\", \"api\"]:\n        raise ValueError(f\"Invalid trigger_source: {trigger_source}\")\n\n\ndef get_database_url() -&gt; str:\n    \"\"\"Retrieve database credentials from Secrets Manager.\"\"\"\n    secret_arn = os.environ[\"DB_SECRET_ARN\"]\n\n    client = boto3.client(\"secretsmanager\")\n    response = client.get_secret_value(SecretId=secret_arn)\n    secret = json.loads(response[\"SecretString\"])\n\n    return (\n        f\"postgresql://{secret['username']}:{secret['password']}\"\n        f\"@{secret['host']}:{secret['port']}/{secret['dbname']}\"\n    )\n\n\ndef handler(event: dict[str, Any], context: Any) -&gt; dict[str, Any]:\n    \"\"\"Lambda entry point for snapshot generation.\n\n    Args:\n        event: Event payload with metadata\n            - trigger_source: \"scheduled\" or \"api\"\n            - triggered_by: User email (for API) or \"system\" (for scheduled)\n            - correlation_id: Unique trace ID\n            - leaderboard_id: Leaderboard to generate\n            - num_bootstrap: Bootstrap iterations\n        context: Lambda context object\n\n    Returns:\n        Response with snapshot metadata\n    \"\"\"\n    start_time = time.time()\n\n    # Extract metadata\n    trigger_source = event.get(\"trigger_source\", \"scheduled\")\n    triggered_by = event.get(\"triggered_by\", \"system\")\n    correlation_id = event.get(\"correlation_id\", str(uuid.uuid4()))\n    leaderboard_id = event.get(\"leaderboard_id\", \"overall\")\n    num_bootstrap = event.get(\"num_bootstrap\", 100)\n\n    # Validate inputs\n    try:\n        validate_inputs(event)\n    except ValueError as e:\n        logger.error(f\"Input validation failed: {str(e)}\")\n        return {\n            \"statusCode\": 400,\n            \"body\": json.dumps({\"error\": str(e)}),\n        }\n\n    logger.info(\n        \"Snapshot generation started\",\n        extra={\n            \"trigger_source\": trigger_source,\n            \"triggered_by\": triggered_by,\n            \"correlation_id\": correlation_id,\n            \"leaderboard_id\": leaderboard_id,\n        }\n    )\n\n    # Get database URL from Secrets Manager\n    database_url = get_database_url()\n\n    # Configuration\n    config = SnapshotConfig(\n        database_url=database_url,\n        leaderboard_id=leaderboard_id,\n        num_bootstrap=num_bootstrap,\n        min_evaluations=int(os.getenv(\"MIN_EVALUATIONS\", \"0\")),\n        rank_by_significance=os.getenv(\"RANK_BY_SIGNIFICANCE\", \"false\").lower() == \"true\",\n    )\n\n    try:\n        generator = SnapshotGenerator(config)\n        result = generator.generate_snapshot(auto_publish=True)\n\n        duration_seconds = int(time.time() - start_time)\n\n        # Publish success notification to SNS\n        topic_arn = os.environ.get(\"NOTIFICATION_TOPIC_ARN\")\n        if topic_arn:\n            sns_client.publish(\n                TopicArn=topic_arn,\n                Subject=\"Leaderboard Snapshot Generated\",\n                Message=json.dumps({\n                    \"status\": \"success\",\n                    \"trigger_source\": trigger_source,\n                    \"triggered_by\": triggered_by,\n                    \"correlation_id\": correlation_id,\n                    \"leaderboard_id\": leaderboard_id,\n                    \"snapshot_id\": result[\"snapshot_id\"],\n                    \"entry_count\": result[\"entry_count\"],\n                    \"evaluation_count\": result[\"evaluation_count\"],\n                    \"duration_seconds\": duration_seconds,\n                    \"timestamp\": datetime.now(UTC).isoformat(),\n                }),\n            )\n\n        logger.info(\n            f\"Snapshot created successfully: {result}\",\n            extra={\n                \"correlation_id\": correlation_id,\n                \"snapshot_id\": result[\"snapshot_id\"],\n            }\n        )\n\n        return {\n            \"statusCode\": 200,\n            \"body\": json.dumps({\n                \"message\": \"Snapshot generated successfully\",\n                \"correlation_id\": correlation_id,\n                \"snapshot_id\": result[\"snapshot_id\"],\n                \"entry_count\": result[\"entry_count\"],\n                \"evaluation_count\": result[\"evaluation_count\"],\n                \"duration_seconds\": duration_seconds,\n            }),\n        }\n\n    except Exception as e:\n        duration_seconds = int(time.time() - start_time)\n\n        # Publish failure notification to SNS\n        topic_arn = os.environ.get(\"NOTIFICATION_TOPIC_ARN\")\n        if topic_arn:\n            sns_client.publish(\n                TopicArn=topic_arn,\n                Subject=\"Leaderboard Snapshot Failed\",\n                Message=json.dumps({\n                    \"status\": \"failure\",\n                    \"trigger_source\": trigger_source,\n                    \"triggered_by\": triggered_by,\n                    \"correlation_id\": correlation_id,\n                    \"leaderboard_id\": leaderboard_id,\n                    \"error\": str(e),\n                    \"duration_seconds\": duration_seconds,\n                    \"timestamp\": datetime.now(UTC).isoformat(),\n                }),\n            )\n\n        logger.error(\n            f\"Failed to generate snapshot: {str(e)}\",\n            extra={\n                \"correlation_id\": correlation_id,\n                \"trigger_source\": trigger_source,\n            },\n            exc_info=True\n        )\n\n        return {\n            \"statusCode\": 500,\n            \"body\": json.dumps({\n                \"message\": \"Snapshot generation failed\",\n                \"correlation_id\": correlation_id,\n                \"error\": str(e),\n            }),\n        }\n</code></pre> <p>Dockerfile:</p> <pre><code>FROM public.ecr.aws/lambda/python:3.13\n\n# Install system dependencies (if needed for psycopg)\nRUN microdnf install -y \\\n    gcc \\\n    postgresql-devel \\\n    &amp;&amp; microdnf clean all\n\n# Copy leaderboard library from workspace\nCOPY ../../../libs/bixarena/leaderboard/python /tmp/bixarena-leaderboard\nRUN pip install /tmp/bixarena-leaderboard &amp;&amp; rm -rf /tmp/bixarena-leaderboard\n\n# Copy Lambda function\nCOPY lambda_function.py ${LAMBDA_TASK_ROOT}/\nCOPY requirements.txt ${LAMBDA_TASK_ROOT}/\n\n# Install Lambda-specific dependencies\nRUN pip install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target ${LAMBDA_TASK_ROOT}\n\nCMD [\"lambda_function.handler\"]\n</code></pre> <p>requirements.txt:</p> <pre><code># Leaderboard library will be installed separately (from workspace)\n# Add any Lambda-specific dependencies here\nboto3&gt;=1.40.0  # AWS SDK (included in Lambda runtime, but pin version)\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#22-cdk-stack-implementation","title":"2.2 CDK Stack Implementation","text":"<p>File: <code>apps/bixarena/infra/cdk/bixarena_infra_cdk/shared/stacks/leaderboard_snapshot_stack.py</code></p> <pre><code>\"\"\"CDK stack for leaderboard snapshot automation.\"\"\"\nfrom aws_cdk import (\n    Duration,\n    Stack,\n    aws_ec2 as ec2,\n    aws_ecr_assets as ecr_assets,\n    aws_events as events,\n    aws_events_targets as targets,\n    aws_iam as iam,\n    aws_lambda as lambda_,\n    aws_logs as logs,\n    aws_rds as rds,\n)\nfrom constructs import Construct\n\n\nclass LeaderboardSnapshotStack(Stack):\n    \"\"\"Stack for automated leaderboard snapshot generation.\"\"\"\n\n    def __init__(\n        self,\n        scope: Construct,\n        construct_id: str,\n        vpc: ec2.IVpc,\n        database_instance: rds.IDatabaseInstance,\n        database_secret_arn: str,\n        environment_name: str,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(scope, construct_id, **kwargs)\n\n        # Build and push Docker image to ECR\n        lambda_image = ecr_assets.DockerImageAsset(\n            self,\n            \"LeaderboardSnapshotImage\",\n            directory=\"../../functions/leaderboard-snapshot\",\n            platform=ecr_assets.Platform.LINUX_AMD64,\n        )\n\n        # Lambda function\n        snapshot_function = lambda_.DockerImageFunction(\n            self,\n            \"LeaderboardSnapshotFunction\",\n            code=lambda_.DockerImageCode.from_ecr(\n                repository=lambda_image.repository,\n                tag=lambda_image.asset_hash,\n            ),\n            timeout=Duration.minutes(15),  # Max Lambda timeout\n            memory_size=2048,  # 2GB for NumPy/SciPy\n            vpc=vpc,\n            vpc_subnets=ec2.SubnetSelection(\n                subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS\n            ),\n            environment={\n                \"DATABASE_URL\": f\"postgresql://{{username}}:{{password}}@{database_instance.db_instance_endpoint_address}:{database_instance.db_instance_endpoint_port}/bixarena\",\n                \"LEADERBOARD_ID\": \"overall\",\n                \"NUM_BOOTSTRAP\": \"1000\",  # Production uses more iterations\n                \"MIN_EVALUATIONS\": \"0\",\n                \"RANK_BY_SIGNIFICANCE\": \"false\",\n                \"LOG_LEVEL\": \"INFO\",\n            },\n            log_retention=logs.RetentionDays.ONE_MONTH,\n            description=f\"Generate daily leaderboard snapshots for {environment_name}\",\n        )\n\n        # Grant database access\n        database_instance.grant_connect(snapshot_function)\n\n        # Grant access to read database credentials from Secrets Manager\n        snapshot_function.add_to_role_policy(\n            iam.PolicyStatement(\n                actions=[\"secretsmanager:GetSecretValue\"],\n                resources=[database_secret_arn],\n            )\n        )\n\n        # EventBridge rule - daily at 2 AM UTC\n        schedule_rule = events.Rule(\n            self,\n            \"LeaderboardSnapshotSchedule\",\n            schedule=events.Schedule.cron(\n                minute=\"0\",\n                hour=\"2\",\n                month=\"*\",\n                week_day=\"*\",\n                year=\"*\",\n            ),\n            description=\"Trigger daily leaderboard snapshot generation\",\n        )\n\n        # Add Lambda as target\n        schedule_rule.add_target(\n            targets.LambdaFunction(\n                snapshot_function,\n                retry_attempts=2,\n            )\n        )\n\n        # CloudWatch Dashboard (optional)\n        # TODO: Add metrics for snapshot generation success/failure\n\n        # Outputs\n        self.snapshot_function = snapshot_function\n        self.schedule_rule = schedule_rule\n</code></pre> <p>Integration in <code>dev/app.py</code>:</p> <pre><code>from bixarena_infra_cdk.shared.stacks.leaderboard_snapshot_stack import (\n    LeaderboardSnapshotStack,\n)\n\n# After database_stack is created...\nsnapshot_stack = LeaderboardSnapshotStack(\n    app,\n    f\"BixarenaSnapshotStack-{env_name}\",\n    vpc=vpc_stack.vpc,\n    database_instance=database_stack.database_instance,\n    database_secret_arn=database_stack.database_secret.secret_arn,\n    environment_name=env_name,\n    env=env,\n)\nsnapshot_stack.add_dependency(database_stack)\nsnapshot_stack.add_dependency(vpc_stack)\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#phase-3-ai-service-endpoint-implementation","title":"Phase 3: AI Service Endpoint Implementation","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#31-add-admin-endpoint","title":"3.1 Add Admin Endpoint","text":"<p>File: <code>apps/bixarena/ai-service/bixarena_ai_service/jobs/__init__.py</code></p> <pre><code>\"\"\"Background jobs and admin operations.\"\"\"\n</code></pre> <p>File: <code>apps/bixarena/ai-service/bixarena_ai_service/routers/admin.py</code></p> <pre><code>\"\"\"Admin operations router.\"\"\"\nimport boto3\nimport json\nimport logging\nimport os\nimport uuid\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field\n\nfrom bixarena_ai_service.security.jwt_validator import validate_jwt\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/admin\", tags=[\"Admin Operations\"])\nlambda_client = boto3.client(\"lambda\")\n\n\nclass SnapshotRequest(BaseModel):\n    \"\"\"Request to generate a leaderboard snapshot.\"\"\"\n\n    leaderboard_id: str = Field(\n        default=\"overall\", description=\"Leaderboard ID to generate snapshot for\"\n    )\n    num_bootstrap: int = Field(\n        default=100, ge=10, le=10000, description=\"Bootstrap iterations\"\n    )\n\n\nclass SnapshotResponse(BaseModel):\n    \"\"\"Response for async snapshot generation.\"\"\"\n\n    message: str\n    correlation_id: str\n    leaderboard_id: str\n    status: str = \"initiated\"\n\n\nasync def validate_jwt_admin(\n    jwt_claims: Annotated[dict, Depends(validate_jwt)],\n) -&gt; str:\n    \"\"\"Validate JWT and ensure user has admin role.\n\n    Returns:\n        User email for audit logging\n\n    Raises:\n        HTTPException: If user does not have admin role\n    \"\"\"\n    roles = jwt_claims.get(\"cognito:groups\", [])\n    email = jwt_claims.get(\"email\", jwt_claims.get(\"sub\"))\n\n    if \"admin\" not in roles:\n        logger.warning(\n            f\"User {email} attempted snapshot generation without admin role\",\n            extra={\"user_email\": email, \"roles\": roles}\n        )\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin role required for snapshot generation\",\n        )\n\n    return email\n\n\n@router.post(\n    \"/generate-snapshot\",\n    response_model=SnapshotResponse,\n    status_code=status.HTTP_202_ACCEPTED,\n    summary=\"Trigger leaderboard snapshot generation\",\n    description=\"\"\"\n    Asynchronously trigger leaderboard snapshot generation.\n\n    This endpoint invokes a Lambda function and returns immediately with a correlation ID.\n    The snapshot generation runs in the background (takes 2-5 minutes).\n    You will receive a Slack notification when complete.\n\n    Requires authentication with admin role.\n    \"\"\",\n)\nasync def generate_snapshot(\n    request: SnapshotRequest,\n    user_email: Annotated[str, Depends(validate_jwt_admin)],\n) -&gt; SnapshotResponse:\n    \"\"\"Trigger asynchronous leaderboard snapshot generation.\n\n    Args:\n        request: Snapshot generation parameters\n        user_email: Email of authenticated admin user\n\n    Returns:\n        Response with correlation ID for tracking\n    \"\"\"\n    correlation_id = str(uuid.uuid4())\n    lambda_function_name = os.getenv(\"SNAPSHOT_LAMBDA_NAME\")\n\n    if not lambda_function_name:\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=\"Snapshot Lambda function not configured\",\n        )\n\n    logger.info(\n        f\"Snapshot generation triggered by {user_email}\",\n        extra={\n            \"user_email\": user_email,\n            \"correlation_id\": correlation_id,\n            \"leaderboard_id\": request.leaderboard_id,\n        }\n    )\n\n    try:\n        # Invoke Lambda asynchronously\n        lambda_client.invoke(\n            FunctionName=lambda_function_name,\n            InvocationType=\"Event\",  # Async invocation (fire-and-forget)\n            Payload=json.dumps({\n                \"trigger_source\": \"api\",\n                \"triggered_by\": user_email,\n                \"correlation_id\": correlation_id,\n                \"leaderboard_id\": request.leaderboard_id,\n                \"num_bootstrap\": request.num_bootstrap,\n            }),\n        )\n\n        logger.info(\n            f\"Lambda invoked successfully for correlation_id {correlation_id}\",\n            extra={\"correlation_id\": correlation_id}\n        )\n\n        return SnapshotResponse(\n            message=(\n                \"Snapshot generation initiated. \"\n                \"You will receive a Slack notification when complete.\"\n            ),\n            correlation_id=correlation_id,\n            leaderboard_id=request.leaderboard_id,\n            status=\"initiated\",\n        )\n\n    except lambda_client.exceptions.ServiceException as e:\n        logger.error(\n            f\"Failed to invoke Lambda: {str(e)}\",\n            extra={\"correlation_id\": correlation_id},\n            exc_info=True\n        )\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Failed to trigger snapshot generation: {str(e)}\",\n        )\n</code></pre> <p>Register router in <code>app.py</code>:</p> <pre><code>from bixarena_ai_service.routers.admin import router as admin_router\n\n# After middleware setup\napp.include_router(admin_router)\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#32-update-ai-service-dependencies","title":"3.2 Update AI Service Dependencies","text":"<p>Add boto3 to <code>pyproject.toml</code>:</p> <pre><code>dependencies = [\n    # ... existing deps ...\n    \"boto3&gt;=1.40.0\",  # AWS SDK for Lambda invocation\n]\n</code></pre> <p>Add environment variable to AI service:</p> <pre><code># In CDK stack for AI service ECS task definition\ntask_definition.add_container(\n    \"AIServiceContainer\",\n    environment={\n        # ... existing env vars ...\n        \"SNAPSHOT_LAMBDA_NAME\": snapshot_lambda.function_name,\n    },\n)\n\n# Grant ECS task role permission to invoke Lambda\nsnapshot_lambda.grant_invoke(ai_service_task_role)\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#phase-4-database-connection-strategy","title":"Phase 4: Database Connection Strategy","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#option-a-rds-proxy-recommended-for-lambda","title":"Option A: RDS Proxy (Recommended for Lambda)","text":"<p>Why RDS Proxy:</p> <ul> <li>Connection pooling for Lambda (many concurrent invocations)</li> <li>Automatic failover</li> <li>IAM authentication (no password in env vars)</li> <li>Reduced DB connections</li> </ul> <p>CDK Implementation:</p> <pre><code># In leaderboard_snapshot_stack.py\nfrom aws_cdk import aws_rds as rds\n\n# Create RDS Proxy\ndb_proxy = rds.DatabaseProxy(\n    self,\n    \"LeaderboardDBProxy\",\n    proxy_target=rds.ProxyTarget.from_instance(database_instance),\n    vpc=vpc,\n    secrets=[database_secret],\n    require_tls=True,\n)\n\n# Update Lambda environment\nsnapshot_function = lambda_.DockerImageFunction(\n    # ...\n    environment={\n        \"DATABASE_URL\": f\"postgresql://{{username}}:{{password}}@{db_proxy.endpoint}/bixarena\",\n        # ...\n    },\n)\n\n# Grant proxy access\ndb_proxy.grant_connect(snapshot_function, \"postgres\")\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#option-b-direct-connection-simpler-for-ai-service","title":"Option B: Direct Connection (Simpler for AI Service)","text":"<p>AI Service can use direct connection (already configured):</p> <ul> <li>Long-lived service maintains connection pool</li> <li>No cold start issues</li> <li>Simpler configuration</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#phase-5-testing-strategy","title":"Phase 5: Testing Strategy","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#51-local-testing","title":"5.1 Local Testing","text":"<p>Test Lambda Locally:</p> <pre><code># Build Docker image\ncd apps/bixarena/functions/leaderboard-snapshot\ndocker build -t leaderboard-snapshot-local .\n\n# Run with local PostgreSQL (port forwarded)\ndocker run -p 9000:8080 \\\n  -e DATABASE_URL=\"postgresql://postgres:changeme@host.docker.internal:5432/bixarena\" \\\n  -e LEADERBOARD_ID=\"overall\" \\\n  -e NUM_BOOTSTRAP=\"10\" \\\n  leaderboard-snapshot-local\n\n# Invoke\ncurl -X POST \"http://localhost:9000/2015-03-31/functions/function/invocations\" \\\n  -d '{}'\n</code></pre> <p>Test AI Service Endpoint:</p> <pre><code># Get session cookie from browser\nSESSION_ID=\"your-session-id\"\n\n# Call admin endpoint\ncurl -X POST \"http://localhost:8113/admin/generate-snapshot\" \\\n  -H \"Cookie: JSESSIONID=${SESSION_ID}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"leaderboard_id\": \"overall\",\n    \"num_bootstrap\": 10\n  }'\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#52-integration-testing","title":"5.2 Integration Testing","text":"<p>Test in AWS Dev Environment:</p> <pre><code># Deploy stack\nnx run bixarena-infra-cdk:deploy:dev --stacks=\"*Snapshot*\"\n\n# Manually invoke Lambda\naws lambda invoke \\\n  --function-name bixarena-dev-{name}-leaderboard-snapshot \\\n  --profile bixarena-dev-Developer \\\n  response.json\n\n# Check logs\naws logs tail /aws/lambda/bixarena-dev-{name}-leaderboard-snapshot \\\n  --follow \\\n  --profile bixarena-dev-Developer\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#phase-6-deployment-workflow","title":"Phase 6: Deployment Workflow","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#61-build-pipeline","title":"6.1 Build Pipeline","text":"<p>Update <code>project.json</code> in <code>apps/bixarena/functions/leaderboard-snapshot</code>:</p> <pre><code>{\n  \"name\": \"leaderboard-snapshot-function\",\n  \"projectType\": \"application\",\n  \"targets\": {\n    \"build-image\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"docker build -t leaderboard-snapshot:latest .\",\n        \"cwd\": \"{projectRoot}\"\n      },\n      \"dependsOn\": [\"^build\"]\n    },\n    \"test-local\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"docker run --rm -p 9000:8080 leaderboard-snapshot:latest\",\n        \"cwd\": \"{projectRoot}\"\n      },\n      \"dependsOn\": [\"build-image\"]\n    }\n  },\n  \"tags\": [\"type:function\", \"scope:backend\", \"language:python\"]\n}\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#62-cicd-integration","title":"6.2 CI/CD Integration","text":"<p>GitHub Actions (example):</p> <pre><code># .github/workflows/deploy-bixarena-infra.yml\n- name: Deploy Snapshot Function\n  run: |\n    nx run bixarena-infra-cdk:deploy:dev \\\n      --stacks=\"BixarenaSnapshotStack-dev\" \\\n      --require-approval=never\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#phase-7-monitoring-observability","title":"Phase 7: Monitoring &amp; Observability","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#71-cloudwatch-metrics","title":"7.1 CloudWatch Metrics","text":"<p>Lambda Metrics (automatic):</p> <ul> <li>Invocations</li> <li>Duration</li> <li>Errors</li> <li>Throttles</li> <li>Concurrent executions</li> </ul> <p>Custom Metrics:</p> <pre><code>import boto3\n\ncloudwatch = boto3.client(\"cloudwatch\")\n\n# After snapshot generation\ncloudwatch.put_metric_data(\n    Namespace=\"BixArena/Leaderboard\",\n    MetricData=[\n        {\n            \"MetricName\": \"SnapshotEntryCount\",\n            \"Value\": result[\"entry_count\"],\n            \"Unit\": \"Count\",\n        },\n        {\n            \"MetricName\": \"SnapshotGenerationDuration\",\n            \"Value\": duration_seconds,\n            \"Unit\": \"Seconds\",\n        },\n    ],\n)\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#72-alarms","title":"7.2 Alarms","text":"<p>CDK Implementation:</p> <pre><code>from aws_cdk import aws_cloudwatch as cloudwatch, aws_sns as sns\n\n# Create SNS topic for alerts\nalert_topic = sns.Topic(self, \"SnapshotAlerts\")\n\n# Alarm on function errors\nerror_alarm = cloudwatch.Alarm(\n    self,\n    \"SnapshotErrorAlarm\",\n    metric=snapshot_function.metric_errors(),\n    threshold=1,\n    evaluation_periods=1,\n    alarm_description=\"Leaderboard snapshot generation failed\",\n)\n\nerror_alarm.add_alarm_action(cloudwatch_actions.SnsAction(alert_topic))\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#73-slack-notifications","title":"7.3 Slack Notifications","text":"<p>Architecture:</p> <pre><code>Snapshot Lambda (success) \u2192 SNS Topic \u2192 Slack Notifier Lambda \u2192 Slack Webhook API\n</code></pre> <p>Slack Message Format:</p> <pre><code>{\n  \"text\": \"\ud83c\udfaf New Leaderboard Snapshot Generated!\",\n  \"blocks\": [\n    {\n      \"type\": \"section\",\n      \"text\": {\n        \"type\": \"mrkdwn\",\n        \"text\": \"*Leaderboard Snapshot Generated* \u2705\\n\\n*Leaderboard:* Overall\\n*Snapshot ID:* snapshot_2026-01-09_02-00\\n*Models Ranked:* 42\\n*Total Evaluations:* 1,247\\n*Bootstrap Iterations:* 1000\\n*Duration:* 87 seconds\"\n      }\n    },\n    {\n      \"type\": \"actions\",\n      \"elements\": [\n        {\n          \"type\": \"button\",\n          \"text\": {\n            \"type\": \"plain_text\",\n            \"text\": \"View Leaderboard\"\n          },\n          \"url\": \"https://bixarena.io/leaderboard\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>CDK Implementation:</p> <p>Add to <code>leaderboard_snapshot_stack.py</code>:</p> <pre><code>from aws_cdk import (\n    aws_lambda as lambda_,\n    aws_sns as sns,\n    aws_sns_subscriptions as subscriptions,\n    aws_secretsmanager as secretsmanager,\n)\n\n# SNS Topic for snapshot events\nsnapshot_events_topic = sns.Topic(\n    self,\n    \"SnapshotEventsTopic\",\n    display_name=\"Leaderboard Snapshot Events\",\n)\n\n# Slack webhook URL stored in Secrets Manager\nslack_webhook_secret = secretsmanager.Secret.from_secret_name_v2(\n    self,\n    \"SlackWebhookSecret\",\n    secret_name=\"bixarena/slack/webhook-url\",\n)\n\n# Slack notification Lambda (inline for simplicity)\nslack_notifier = lambda_.Function(\n    self,\n    \"SlackNotifier\",\n    runtime=lambda_.Runtime.PYTHON_3_13,\n    handler=\"index.handler\",\n    code=lambda_.Code.from_inline(\"\"\"\nimport json\nimport os\nimport urllib.request\nimport urllib.error\n\ndef handler(event, context):\n    webhook_url = os.environ['SLACK_WEBHOOK_URL']\n\n    # Parse SNS message\n    for record in event['Records']:\n        message = json.loads(record['Sns']['Message'])\n\n        status = message.get('status', 'unknown')\n        trigger_source = message.get('trigger_source', 'scheduled')\n        triggered_by = message.get('triggered_by', 'System')\n\n        # Build title and trigger info based on source\n        if trigger_source == 'api':\n            title = f\"\ud83c\udfaf Leaderboard Snapshot {'Generated' if status == 'success' else 'Failed'} (Manual)\"\n            trigger_info = f\"*Triggered by:* {triggered_by}\"\n        else:\n            title = f\"\ud83c\udfaf Leaderboard Snapshot {'Generated' if status == 'success' else 'Failed'} (Scheduled)\"\n            trigger_info = f\"*Triggered by:* Automated daily job\"\n\n        # Build status emoji and message\n        if status == 'success':\n            status_emoji = \"\u2705\"\n            status_text = \"Snapshot Generated\"\n        else:\n            status_emoji = \"\u274c\"\n            status_text = \"Snapshot Failed\"\n\n        # Build Slack message\n        slack_message = {\n            'text': title,\n            'blocks': [\n                {\n                    'type': 'section',\n                    'text': {\n                        'type': 'mrkdwn',\n                        'text': f\"*{status_text}* {status_emoji}\\\\n\\\\n\"\n                                f\"{trigger_info}\\\\n\"\n                                f\"*Leaderboard:* {message.get('leaderboard_id', 'N/A')}\\\\n\"\n                                + (f\"*Snapshot ID:* {message.get('snapshot_id', 'N/A')}\\\\n\"\n                                   f\"*Models Ranked:* {message.get('entry_count', 'N/A')}\\\\n\"\n                                   f\"*Total Evaluations:* {message.get('evaluation_count', 'N/A')}\\\\n\"\n                                   if status == 'success' else\n                                   f\"*Error:* {message.get('error', 'Unknown error')}\\\\n\") +\n                                f\"*Duration:* {message.get('duration_seconds', 'N/A')}s\"\n                    }\n                },\n                {\n                    'type': 'context',\n                    'elements': [\n                        {\n                            'type': 'mrkdwn',\n                            'text': f\"Correlation ID: `{message.get('correlation_id', 'N/A')}`\"\n                        }\n                    ]\n                }\n            ]\n        }\n\n        # Add action button only for success\n        if status == 'success':\n            slack_message['blocks'].append({\n                'type': 'actions',\n                'elements': [\n                    {\n                        'type': 'button',\n                        'text': {'type': 'plain_text', 'text': 'View Leaderboard'},\n                        'url': 'https://bixarena.io/leaderboard'\n                    }\n                ]\n            })\n\n        # Send to Slack\n        req = urllib.request.Request(\n            webhook_url,\n            data=json.dumps(slack_message).encode('utf-8'),\n            headers={'Content-Type': 'application/json'}\n        )\n\n        try:\n            urllib.request.urlopen(req)\n            print(f\"Sent notification for snapshot: {message.get('snapshot_id')}\")\n        except urllib.error.HTTPError as e:\n            print(f\"Failed to send Slack notification: {e}\")\n            raise\n\n    return {'statusCode': 200}\n\"\"\"),\n    environment={\n        \"SLACK_WEBHOOK_URL\": slack_webhook_secret.secret_value.unsafe_unwrap(),\n    },\n    timeout=Duration.seconds(30),\n    description=\"Send Slack notifications for leaderboard snapshots\",\n)\n\n# Grant access to read Slack webhook secret\nslack_webhook_secret.grant_read(slack_notifier)\n\n# Subscribe Slack notifier to SNS topic\nsnapshot_events_topic.add_subscription(\n    subscriptions.LambdaSubscription(slack_notifier)\n)\n\n# Update main snapshot Lambda to publish to SNS\nsnapshot_function.add_environment(\n    \"NOTIFICATION_TOPIC_ARN\", snapshot_events_topic.topic_arn\n)\nsnapshot_events_topic.grant_publish(snapshot_function)\n</code></pre> <p>Update Lambda handler to publish SNS:</p> <p>In <code>lambda_function.py</code>:</p> <pre><code>import boto3\nimport time\nfrom datetime import UTC, datetime\n\nsns_client = boto3.client(\"sns\")\n\ndef handler(event: dict[str, Any], context: Any) -&gt; dict[str, Any]:\n    start_time = time.time()\n\n    # ... existing snapshot generation code ...\n\n    try:\n        generator = SnapshotGenerator(config)\n        result = generator.generate_snapshot()\n\n        duration_seconds = int(time.time() - start_time)\n\n        # Publish success notification to SNS\n        topic_arn = os.environ.get(\"NOTIFICATION_TOPIC_ARN\")\n        if topic_arn:\n            sns_client.publish(\n                TopicArn=topic_arn,\n                Subject=\"Leaderboard Snapshot Generated\",\n                Message=json.dumps({\n                    \"status\": \"success\",\n                    \"leaderboard_id\": config.leaderboard_id,\n                    \"snapshot_id\": result[\"snapshot_id\"],\n                    \"entry_count\": result[\"entry_count\"],\n                    \"evaluation_count\": result[\"evaluation_count\"],\n                    \"duration_seconds\": duration_seconds,\n                    \"timestamp\": datetime.now(UTC).isoformat(),\n                }),\n            )\n\n        logger.info(f\"Snapshot created successfully: {result}\")\n\n        return {\n            \"statusCode\": 200,\n            \"body\": json.dumps({\n                \"message\": \"Snapshot generated successfully\",\n                \"snapshot_id\": result[\"snapshot_id\"],\n                \"entry_count\": result[\"entry_count\"],\n                \"evaluation_count\": result[\"evaluation_count\"],\n                \"duration_seconds\": duration_seconds,\n            }),\n        }\n\n    except Exception as e:\n        # Publish failure notification\n        duration_seconds = int(time.time() - start_time)\n\n        topic_arn = os.environ.get(\"NOTIFICATION_TOPIC_ARN\")\n        if topic_arn:\n            sns_client.publish(\n                TopicArn=topic_arn,\n                Subject=\"Leaderboard Snapshot Failed\",\n                Message=json.dumps({\n                    \"status\": \"failure\",\n                    \"leaderboard_id\": config.leaderboard_id,\n                    \"error\": str(e),\n                    \"duration_seconds\": duration_seconds,\n                    \"timestamp\": datetime.now(UTC).isoformat(),\n                }),\n            )\n\n        logger.error(f\"Failed to generate snapshot: {str(e)}\", exc_info=True)\n        raise\n</code></pre> <p>Slack Webhook Setup:</p> <ol> <li>Create Slack App at https://api.slack.com/apps</li> <li>Enable Incoming Webhooks</li> <li>Add webhook to desired channel (e.g., <code>#bixarena-alerts</code>)</li> <li>Copy webhook URL</li> <li>Store in AWS Secrets Manager:</li> </ol> <pre><code>aws secretsmanager create-secret \\\n  --name bixarena/slack/webhook-url \\\n  --secret-string \"https://hooks.slack.com/services/YOUR/WEBHOOK/URL\" \\\n  --profile bixarena-dev-Developer\n</code></pre>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#implementation-timeline","title":"Implementation Timeline","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#week-1-foundation","title":"Week 1: Foundation","text":"<ul> <li> Create <code>bixarena-leaderboard</code> library in <code>libs/bixarena/leaderboard/python/</code></li> <li> Extract business logic from <code>apps/bixarena/tools/bixarena_tools/leaderboard/</code></li> <li> Write unit tests for library (snapshot, ranking, database modules)</li> <li> Update <code>bixarena-tools</code> CLI to depend on <code>bixarena-leaderboard</code> library</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#week-2-lambda-function","title":"Week 2: Lambda Function","text":"<ul> <li> Create Lambda function code and Dockerfile</li> <li> Test Lambda locally with Docker</li> <li> Create CDK stack for Lambda and EventBridge</li> <li> Set up Slack webhook in AWS Secrets Manager</li> <li> Add SNS topic and Slack notifier Lambda</li> <li> Deploy to dev environment</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#week-3-ai-service-integration","title":"Week 3: AI Service Integration","text":"<ul> <li> Add admin endpoint to AI service with async Lambda invocation</li> <li> Implement JWT authentication with admin role validation</li> <li> Add boto3 dependency for Lambda invocation</li> <li> Grant ECS task role permission to invoke Lambda</li> <li> Add SNAPSHOT_LAMBDA_NAME environment variable</li> <li> Test end-to-end authentication and async invocation flow</li> <li> Deploy to dev environment</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#week-4-testing-documentation","title":"Week 4: Testing &amp; Documentation","text":"<ul> <li> Integration testing in dev environment</li> <li> Test Slack notifications (success and failure scenarios)</li> <li> Performance testing (snapshot generation time)</li> <li> Verify auto-publish functionality</li> <li> Documentation and runbooks</li> <li> Deploy to stage/prod</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#cost-estimation","title":"Cost Estimation","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#lambda-function","title":"Lambda Function","text":"<ul> <li>Execution: Daily at 2 AM, ~2 minutes duration</li> <li>Memory: 2GB</li> <li>Cost per month: ~$0.05 (negligible)</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#rds-proxy-if-used","title":"RDS Proxy (if used)","text":"<ul> <li>Cost: ~$15/month per proxy</li> <li>Note: Only if connection pooling is needed</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#cloudwatch-logs","title":"CloudWatch Logs","text":"<ul> <li>Retention: 30 days</li> <li>Cost: ~$0.50/month</li> </ul> <p>Total estimated cost: &lt;$20/month</p>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#decisions-made","title":"Decisions Made","text":"<ol> <li>Snapshot visibility: \u2705 Auto-publish to public (set <code>visible=true</code> after generation)</li> <li>Notification: \u2705 Send Slack notification when snapshot completes</li> <li>Multiple leaderboards: \u2705 Start with \"overall\" leaderboard, design for future expansion (open-source, multimodal, etc.)</li> <li>Execution time: \u2705 Confirmed to be &lt; 15 minutes (within Lambda limits)</li> <li>On-demand execution: \u2705 Async Lambda invocation (returns HTTP 202 immediately, no timeout issues)</li> <li>Metadata tracking: \u2705 Track trigger_source, triggered_by, correlation_id for audit trail</li> <li>Security:<ul> <li>\u2705 IAM least privilege for Lambda and ECS task roles</li> <li>\u2705 Database credentials in AWS Secrets Manager</li> <li>\u2705 Lambda in VPC private subnet with security groups</li> <li>\u2705 Input validation (whitelist leaderboard IDs, validate ranges)</li> <li>\u2705 JWT authentication with admin role required</li> <li>\u2705 Rate limiting on API Gateway (100 requests/day)</li> </ul> </li> <li>Retention: TBD - Will address in future iteration</li> </ol>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#next-steps","title":"Next Steps","text":""},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#ready-to-begin-implementation","title":"Ready to Begin Implementation \u2705","text":"<p>All decisions have been made:</p> <ul> <li>\u2705 Lambda packaging: Container Image (Docker)</li> <li>\u2705 Auto-publish snapshots: Yes (set <code>visible=true</code>)</li> <li>\u2705 Notifications: Slack via SNS \u2192 Lambda \u2192 Webhook</li> <li>\u2705 Initial scope: \"overall\" leaderboard (extensible design)</li> <li>\u2705 Execution time: &lt; 15 minutes (within Lambda limits)</li> <li>\u2705 On-demand execution: Async Lambda invocation (HTTP 202, no timeout)</li> <li>\u2705 Security: Defense-in-depth (IAM, Secrets Manager, VPC, input validation)</li> <li>\u2705 Audit logging: Metadata tracking (trigger_source, triggered_by, correlation_id)</li> </ul>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#recommended-implementation-order","title":"Recommended Implementation Order","text":"<ol> <li> <p>Phase 1 (Week 1): Extract leaderboard library</p> <ul> <li>Create <code>libs/bixarena/leaderboard/python/</code> (Nx: <code>bixarena-leaderboard-python</code>, Python: <code>bixarena-leaderboard</code>)</li> <li>Move business logic from <code>apps/bixarena/tools/bixarena_tools/leaderboard/</code></li> <li>Add <code>auto_publish</code> parameter to <code>SnapshotGenerator.generate_snapshot()</code></li> <li>Write unit tests for snapshot, ranking, and database modules</li> </ul> </li> <li> <p>Phase 2 (Week 2): Build Lambda function</p> <ul> <li>Create Docker-based Lambda</li> <li>Add SNS notification logic</li> <li>Set up CDK stack with Slack integration</li> <li>Test locally with <code>docker run</code></li> </ul> </li> <li> <p>Phase 3 (Week 3): Add AI service endpoint</p> <ul> <li>Create <code>/admin/generate-snapshot</code> endpoint with async Lambda invocation</li> <li>Implement JWT authentication with admin role validation</li> <li>Grant ECS task role permission to invoke Lambda</li> <li>Add SNAPSHOT_LAMBDA_NAME environment variable</li> <li>Deploy to dev environment</li> </ul> </li> <li> <p>Phase 4 (Week 4): Test and deploy</p> <ul> <li>Integration testing</li> <li>Verify Slack notifications</li> <li>Deploy to stage/prod</li> </ul> </li> </ol>"},{"location":"architecture/bixarena-leaderboard-snapshot-automation-plan/#quick-start-command","title":"Quick Start Command","text":"<pre><code># Create leaderboard library\nmkdir -p libs/bixarena/leaderboard/python/bixarena_leaderboard\ncd libs/bixarena/leaderboard/python\n\n# Initialize Python package\ncat &gt; pyproject.toml &lt;&lt; 'EOF'\n[project]\nname = \"bixarena-leaderboard\"\nversion = \"1.0.0\"\ndescription = \"BixArena leaderboard snapshot generation and ranking algorithms\"\nrequires-python = \"==3.13.3\"\ndependencies = [\n    \"numpy&gt;=1.26.0\",\n    \"psycopg[binary]&gt;=3.0.0\",\n    \"scipy&gt;=1.11.0\",\n]\n\n[tool.hatch.build.targets.wheel]\npackages = [\"bixarena_leaderboard\"]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\nEOF\n\n# Initialize Nx project\ncat &gt; project.json &lt;&lt; 'EOF'\n{\n  \"name\": \"bixarena-leaderboard-python\",\n  \"projectType\": \"library\",\n  \"targets\": {\n    \"build\": {\n      \"executor\": \"@nxlv/python:build\",\n      \"outputs\": [\"{projectRoot}/dist\"]\n    },\n    \"test\": {\n      \"executor\": \"@nxlv/python:run-commands\",\n      \"options\": {\n        \"command\": \"uv run pytest tests/\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    }\n  },\n  \"tags\": [\"type:library\", \"scope:backend\", \"language:python\", \"domain:leaderboard\"]\n}\nEOF\n\n# Create package structure\nmkdir -p bixarena_leaderboard tests\ntouch bixarena_leaderboard/{__init__.py,snapshot.py,database.py,ranking.py,config.py}\ntouch tests/{__init__.py,test_snapshot.py,test_database.py,test_ranking.py}\n\n# Start implementation!\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/","title":"Integration Plan: Python FastAPI Microservice with JWT Authentication","text":""},{"location":"architecture/python-ai-service-integration-plan/#overview","title":"Overview","text":"<p>This document outlines the plan to integrate the Python AI service into the BixArena microservices architecture with JWT-based authentication. The AI service will provide a prompt validation endpoint that checks if a prompt is biomedically related.</p>"},{"location":"architecture/python-ai-service-integration-plan/#current-architecture-summary","title":"Current Architecture Summary","text":"<p>BixArena uses a Gateway-Minted JWT pattern:</p> <ol> <li>API Gateway receives session cookie from Gradio web app</li> <li>Gateway validates session with auth service (<code>/userinfo</code> endpoint)</li> <li>Gateway mints audience-specific JWT via auth service (<code>/oauth2/token?audience=urn:bixarena:ai</code>)</li> <li>Gateway strips session cookie and forwards request with JWT in <code>Authorization: Bearer &lt;token&gt;</code> header</li> <li>Microservice validates JWT using JWKS endpoint from auth service</li> </ol>"},{"location":"architecture/python-ai-service-integration-plan/#phase-1-openapi-specification-api-first-design","title":"Phase 1: OpenAPI Specification (API-First Design)","text":""},{"location":"architecture/python-ai-service-integration-plan/#11-define-new-authenticated-endpoint","title":"1.1 Define New Authenticated Endpoint","text":"<p>Create a new endpoint definition in <code>libs/bixarena/api-description/src/paths/validate-prompt.yaml</code>:</p> <pre><code>get:\n  tags:\n    - Prompt Validation\n  summary: Validate biomedical prompt\n  description: Validates whether a prompt is biomedically related and returns a confidence score (requires authentication)\n  operationId: validatePrompt\n  parameters:\n    - name: prompt\n      in: query\n      required: true\n      description: The prompt text to validate\n      schema:\n        type: string\n        minLength: 1\n        maxLength: 10000\n  responses:\n    '200':\n      description: Success\n      content:\n        application/json:\n          schema:\n            $ref: ../components/schemas/PromptValidation.yaml\n    '400':\n      $ref: ../components/responses/BadRequest.yaml\n    '401':\n      $ref: ../components/responses/Unauthorized.yaml\n    '500':\n      $ref: ../components/responses/InternalServerError.yaml\n</code></pre> <p>Key characteristics:</p> <ul> <li>\u274c No <code>x-anonymous-access: true</code> (requires authentication)</li> <li>\u2705 Includes <code>401</code> response for unauthorized access</li> <li>\u2705 Uses <code>jwtBearer</code> security (inherited from global <code>security</code> in <code>ai.openapi.yaml</code>)</li> <li>\u2705 Query parameter for prompt text with validation constraints</li> </ul>"},{"location":"architecture/python-ai-service-integration-plan/#12-create-response-schema","title":"1.2 Create Response Schema","text":"<p>Create <code>libs/bixarena/api-description/src/components/schemas/PromptValidation.yaml</code>:</p> <pre><code>type: object\nproperties:\n  prompt:\n    type: string\n    description: The original prompt that was validated\n  confidence:\n    type: number\n    format: float\n    minimum: 0.0\n    maximum: 1.0\n    description: Confidence score indicating biomedical relevance (0.0 = not biomedical, 1.0 = definitely biomedical)\n  isBiomedical:\n    type: boolean\n    description: Whether the prompt is considered biomedically related (confidence &gt;= 0.5)\nrequired:\n  - prompt\n  - confidence\n  - isBiomedical\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#13-update-main-openapi-file","title":"1.3 Update Main OpenAPI File","text":"<p>Add reference to new path in <code>libs/bixarena/api-description/src/ai.openapi.yaml</code>:</p> <pre><code>paths:\n  /health-check:\n    $ref: paths/health-check.yaml\n  /validate-prompt:\n    $ref: paths/validate-prompt.yaml\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#14-add-tag-definition","title":"1.4 Add Tag Definition","text":"<p>Add to the <code>tags</code> section in <code>libs/bixarena/api-description/src/ai.openapi.yaml</code>:</p> <pre><code>tags:\n  - name: Health Check\n    description: Operations about health checks\n    x-sage-internal: true\n  - name: Prompt Validation\n    description: Operations for validating biomedical prompts\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#phase-2-code-generation","title":"Phase 2: Code Generation","text":""},{"location":"architecture/python-ai-service-integration-plan/#21-build-openapi-descriptions","title":"2.1 Build OpenAPI Descriptions","text":"<pre><code>nx build bixarena-api-description\n</code></pre> <p>This processes the OpenAPI YAML files and prepares them for generation.</p>"},{"location":"architecture/python-ai-service-integration-plan/#22-generate-all-server-stubs-and-api-clients","title":"2.2 Generate All Server Stubs and API Clients","text":"<pre><code>nx run-many -t=generate -p=bixarena-*\n</code></pre> <p>This will generate:</p> <ul> <li>Python server stubs:<ul> <li><code>apps/bixarena/ai-service/bixarena_ai_service/apis/prompt_validation_api.py</code> (router)</li> <li><code>apps/bixarena/ai-service/bixarena_ai_service/apis/prompt_validation_api_base.py</code> (base class)</li> <li><code>apps/bixarena/ai-service/bixarena_ai_service/models/prompt_validation.py</code> (model)</li> </ul> </li> <li>API Gateway routes: Updated <code>apps/bixarena/api-gateway/src/main/resources/routes.yml</code></li> <li>API clients: Python, Java, and TypeScript clients (if configured)</li> </ul> <p>Expected route configuration in <code>routes.yml</code>:</p> <pre><code>- method: 'GET'\n  path: '/validate-prompt'\n  audience: 'urn:bixarena:ai'\n  # Note: No anonymousAccess: true (defaults to false, requires authentication)\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#phase-3-python-jwt-validation-implementation","title":"Phase 3: Python JWT Validation Implementation","text":""},{"location":"architecture/python-ai-service-integration-plan/#31-add-jwt-validation-dependencies","title":"3.1 Add JWT Validation Dependencies","text":"<p>Update <code>apps/bixarena/ai-service/pyproject.toml</code>:</p> <pre><code>dependencies = [\n  # ... existing deps ...\n  \"python-jose[cryptography]&gt;=3.3.0\",  # JWT validation with RSA support\n  \"httpx&gt;=0.28.1\",                      # Already present - for JWKS fetch\n]\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#32-create-jwt-security-module","title":"3.2 Create JWT Security Module","text":"<p>Create <code>apps/bixarena/ai-service/bixarena_ai_service/security/__init__.py</code>:</p> <pre><code>\"\"\"Security modules for JWT validation and authentication.\"\"\"\n</code></pre> <p>Create <code>apps/bixarena/ai-service/bixarena_ai_service/security/jwt_validator.py</code>:</p> <pre><code>\"\"\"JWT validation using JWKS from auth service.\n\nThis module provides FastAPI dependencies for validating JWTs issued by the\nBixArena auth service. It fetches the JWKS (JSON Web Key Set) from the auth\nservice and validates tokens according to the OAuth 2.0 Bearer Token specification.\n\"\"\"\n\nfrom typing import Annotated\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom jose import jwt, JWTError, jwk\nfrom jose.exceptions import JWKError\nimport httpx\nfrom functools import lru_cache\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Configuration (should come from env vars in production)\nAUTH_SERVICE_URL = \"http://bixarena-auth-service:8115\"\nJWKS_URL = f\"{AUTH_SERVICE_URL}/.well-known/jwks.json\"\nEXPECTED_ISSUER = \"urn:bixarena:auth\"\nEXPECTED_AUDIENCE = \"urn:bixarena:ai\"\n\n# HTTP Bearer security scheme\nsecurity = HTTPBearer()\n\n@lru_cache(maxsize=1)\ndef get_jwks() -&gt; dict:\n    \"\"\"Fetch JWKS from auth service with caching.\n\n    The JWKS (JSON Web Key Set) contains the public keys used to verify\n    JWT signatures. This function caches the result to avoid repeated\n    network calls.\n\n    Returns:\n        dict: JWKS document containing public keys\n\n    Raises:\n        HTTPException: If JWKS cannot be fetched (503 Service Unavailable)\n    \"\"\"\n    try:\n        response = httpx.get(JWKS_URL, timeout=5.0)\n        response.raise_for_status()\n        logger.info(\"Successfully fetched JWKS from auth service\")\n        return response.json()\n    except httpx.HTTPError as e:\n        logger.error(f\"Failed to fetch JWKS from {JWKS_URL}: {e}\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=\"Cannot validate token - auth service unavailable\"\n        )\n\ndef validate_jwt(\n    credentials: Annotated[HTTPAuthorizationCredentials, Depends(security)]\n) -&gt; dict:\n    \"\"\"Validate JWT token and return claims.\n\n    This function:\n    1. Extracts the JWT from the Authorization header\n    2. Fetches the JWKS from the auth service\n    3. Finds the matching public key using the token's key ID (kid)\n    4. Validates the token signature, expiration, issuer, and audience\n    5. Returns the decoded claims\n\n    Args:\n        credentials: HTTP Bearer credentials from Authorization header\n\n    Returns:\n        dict: Decoded JWT claims including sub, roles, iat, exp, etc.\n\n    Raises:\n        HTTPException: If token is invalid, expired, or has wrong audience (401)\n    \"\"\"\n    token = credentials.credentials\n\n    try:\n        # Fetch JWKS\n        jwks = get_jwks()\n\n        # Decode token header to get key ID\n        unverified_header = jwt.get_unverified_header(token)\n        kid = unverified_header.get(\"kid\")\n\n        if not kid:\n            logger.warning(\"JWT token missing key ID (kid)\")\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Token missing key ID\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n\n        # Find matching key in JWKS\n        key = None\n        for jwk_key in jwks.get(\"keys\", []):\n            if jwk_key.get(\"kid\") == kid:\n                key = jwk_key\n                break\n\n        if not key:\n            logger.warning(f\"Token key ID {kid} not found in JWKS\")\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Token key not found in JWKS\",\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n\n        # Validate and decode JWT\n        claims = jwt.decode(\n            token,\n            key,\n            algorithms=[\"RS256\"],\n            issuer=EXPECTED_ISSUER,\n            audience=EXPECTED_AUDIENCE,\n            options={\n                \"verify_signature\": True,\n                \"verify_exp\": True,\n                \"verify_iat\": True,\n                \"verify_aud\": True,\n                \"verify_iss\": True,\n            }\n        )\n\n        logger.debug(f\"Successfully validated JWT for subject: {claims.get('sub')}\")\n        return claims\n\n    except JWTError as e:\n        logger.warning(f\"JWT validation failed: {e}\")\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid or expired token\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\n# Convenience dependencies for accessing specific claims\n\nasync def get_current_user_id(\n    claims: Annotated[dict, Depends(validate_jwt)]\n) -&gt; str:\n    \"\"\"Extract user ID from validated JWT.\n\n    Args:\n        claims: Validated JWT claims\n\n    Returns:\n        str: User ID (UUID) from the 'sub' claim\n\n    Raises:\n        HTTPException: If 'sub' claim is missing (401)\n    \"\"\"\n    user_id = claims.get(\"sub\")\n    if not user_id:\n        logger.error(\"JWT missing subject (sub) claim\")\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Token missing subject claim\"\n        )\n    return user_id\n\nasync def get_current_user_roles(\n    claims: Annotated[dict, Depends(validate_jwt)]\n) -&gt; list[str]:\n    \"\"\"Extract roles from validated JWT.\n\n    Args:\n        claims: Validated JWT claims\n\n    Returns:\n        list[str]: List of role names (e.g., ['user', 'contributor'])\n    \"\"\"\n    return claims.get(\"roles\", [])\n\nasync def get_jwt_claims(\n    claims: Annotated[dict, Depends(validate_jwt)]\n) -&gt; dict:\n    \"\"\"Get all validated JWT claims.\n\n    Use this dependency when you need access to the full JWT claims.\n\n    Args:\n        claims: Validated JWT claims\n\n    Returns:\n        dict: Complete JWT claims\n    \"\"\"\n    return claims\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#33-create-configuration-module","title":"3.3 Create Configuration Module","text":"<p>Create <code>apps/bixarena/ai-service/bixarena_ai_service/config.py</code>:</p> <pre><code>\"\"\"Configuration management for AI service.\n\nThis module provides a centralized configuration using Pydantic settings\nthat can be loaded from environment variables.\n\"\"\"\n\nfrom pydantic_settings import BaseSettings\nfrom functools import lru_cache\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings loaded from environment variables.\n\n    All settings can be overridden using environment variables with the\n    BIXARENA_AI_ prefix. For example:\n    - BIXARENA_AI_AUTH_SERVICE_URL=http://auth:8115\n    - BIXARENA_AI_JWT_EXPECTED_ISSUER=urn:bixarena:auth\n    \"\"\"\n\n    # Auth service configuration\n    auth_service_url: str = \"http://bixarena-auth-service:8115\"\n\n    # JWT validation configuration\n    jwt_expected_issuer: str = \"urn:bixarena:auth\"\n    jwt_expected_audience: str = \"urn:bixarena:ai\"\n\n    # Prompt validation configuration\n    prompt_validation_confidence_threshold: float = 0.5\n    prompt_max_length: int = 10000\n\n    class Config:\n        env_prefix = \"BIXARENA_AI_\"\n        case_sensitive = False\n\n@lru_cache\ndef get_settings() -&gt; Settings:\n    \"\"\"Get cached settings instance.\n\n    Returns:\n        Settings: Application settings singleton\n    \"\"\"\n    return Settings()\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#34-implement-authenticated-endpoint","title":"3.4 Implement Authenticated Endpoint","text":"<p>Create <code>apps/bixarena/ai-service/bixarena_ai_service/impl/prompt_validation_impl.py</code>:</p> <pre><code>\"\"\"Runtime implementation for prompt validation endpoint.\n\nThis file lives under the ``impl`` package which is scanned dynamically by the\ngenerated router (see ``apis/prompt_validation_api.py``). Any subclass of\n``BasePromptValidationApi`` discovered at import time is used to service incoming\nrequests. By placing our implementation here we avoid modifying generated\ncode that would be overwritten on regeneration.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Annotated\n\nfrom fastapi import Depends\nfrom bixarena_ai_service.apis.prompt_validation_api_base import BasePromptValidationApi\nfrom bixarena_ai_service.models.prompt_validation import PromptValidation\nfrom bixarena_ai_service.security.jwt_validator import (\n    get_current_user_id,\n    get_current_user_roles\n)\nfrom bixarena_ai_service.config import get_settings, Settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass PromptValidationApiImpl(BasePromptValidationApi):\n    \"\"\"Concrete prompt validation implementation.\n\n    This implementation provides a placeholder that returns a static confidence\n    value. In the future, this will be replaced with actual ML-based validation\n    using biomedical NLP models.\n    \"\"\"\n\n    async def validate_prompt(\n        self,\n        prompt: str,\n        user_id: Annotated[str, Depends(get_current_user_id)],\n        roles: Annotated[list[str], Depends(get_current_user_roles)],\n        settings: Annotated[Settings, Depends(get_settings)]\n    ) -&gt; PromptValidation:\n        \"\"\"Validate whether a prompt is biomedically related.\n\n        Args:\n            prompt: The prompt text to validate\n            user_id: Authenticated user ID from JWT\n            roles: User roles from JWT\n            settings: Application settings\n\n        Returns:\n            PromptValidation: Validation result with confidence score\n        \"\"\"\n        logger.info(\n            f\"Prompt validation requested by user {user_id} with roles {roles}\"\n        )\n        logger.debug(f\"Prompt text (length={len(prompt)}): {prompt[:100]}...\")\n\n        # TODO: Replace with actual ML-based validation\n        # For now, return a static confidence value\n        static_confidence = 0.75\n\n        validation_result = PromptValidation(\n            prompt=prompt,\n            confidence=static_confidence,\n            is_biomedical=static_confidence &gt;= settings.prompt_validation_confidence_threshold\n        )\n\n        logger.info(\n            f\"Validation result: confidence={validation_result.confidence}, \"\n            f\"isBiomedical={validation_result.is_biomedical}\"\n        )\n\n        return validation_result\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#35-update-main-application","title":"3.5 Update Main Application","text":"<p>Update <code>apps/bixarena/ai-service/bixarena_ai_service/main.py</code> to include the new router:</p> <pre><code># coding: utf-8\n\n\"\"\"\nBixArena AI Service\n\nAdvance bioinformatics by evaluating and ranking AI agents.\n\nThe version of the OpenAPI document: 1.0.0\nGenerated by OpenAPI Generator (https://openapi-generator.tech)\n\nDo not edit the class manually.\n\"\"\"  # noqa: E501\n\nfrom fastapi import FastAPI\n\nfrom bixarena_ai_service.apis.health_check_api import router as HealthCheckApiRouter\nfrom bixarena_ai_service.apis.prompt_validation_api import router as PromptValidationApiRouter\n\napp = FastAPI(\n    title=\"BixArena AI Service\",\n    description=\"Advance bioinformatics by evaluating and ranking AI agents.\",\n    version=\"1.0.0\",\n)\n\napp.include_router(HealthCheckApiRouter)\napp.include_router(PromptValidationApiRouter)\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#phase-4-api-gateway-configuration","title":"Phase 4: API Gateway Configuration","text":""},{"location":"architecture/python-ai-service-integration-plan/#41-add-ai-service-route","title":"4.1 Add AI Service Route","text":"<p>Update <code>apps/bixarena/api-gateway/src/main/resources/application.yml</code>:</p> <pre><code>spring:\n  cloud:\n    gateway:\n      routes:\n        # ... existing routes ...\n\n        - id: bixarena-ai-service\n          uri: http://bixarena-ai-service:8116\n          predicates:\n            - Path=/ai/**,/health-check,/validate-prompt\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#42-gateway-jwt-minting-no-changes-required","title":"4.2 Gateway JWT Minting (No Changes Required)","text":"<p>The API Gateway's <code>SessionToJwtFilter</code> already supports audience-specific JWT minting. When it processes a request to <code>/validate-prompt</code>, it will:</p> <ol> <li>Check route config and find <code>audience: urn:bixarena:ai</code></li> <li>Verify the endpoint does not have <code>anonymousAccess: true</code></li> <li>Extract session cookie from request</li> <li>Validate session by calling auth service <code>/userinfo</code> endpoint</li> <li>Mint audience-specific JWT by calling auth service <code>POST /oauth2/token?audience=urn:bixarena:ai</code></li> <li>Strip session cookie and add JWT as <code>Authorization: Bearer &lt;token&gt;</code> header</li> <li>Forward request to AI service</li> </ol> <p>JWT Structure Received by Python Service:</p> <pre><code>{\n  \"iss\": \"urn:bixarena:auth\",\n  \"aud\": \"urn:bixarena:ai\",\n  \"sub\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"roles\": [\"user\", \"contributor\"],\n  \"iat\": 1234567890,\n  \"exp\": 1234568490\n}\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#phase-5-docker-deployment","title":"Phase 5: Docker &amp; Deployment","text":""},{"location":"architecture/python-ai-service-integration-plan/#51-create-dockerfile","title":"5.1 Create Dockerfile","text":"<p>Create <code>apps/bixarena/ai-service/Dockerfile</code>:</p> <pre><code>FROM python:3.13.3-slim\n\nWORKDIR /app\n\n# Copy dependency files\nCOPY pyproject.toml ./\n\n# Install dependencies\nRUN pip install --no-cache-dir -e .\n\n# Copy application code\nCOPY bixarena_ai_service ./bixarena_ai_service\n\n# Expose port\nEXPOSE 8116\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD python -c \"import httpx; httpx.get('http://localhost:8116/health-check', timeout=2.0)\"\n\n# Run with uvicorn (using uvloop and httptools for performance)\nCMD [\"uvicorn\", \"bixarena_ai_service.main:app\", \\\n     \"--host\", \"0.0.0.0\", \\\n     \"--port\", \"8116\", \\\n     \"--loop\", \"uvloop\", \\\n     \"--http\", \"httptools\"]\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#52-update-docker-compose","title":"5.2 Update Docker Compose","text":"<p>Add to your docker-compose file:</p> <pre><code>services:\n  # ... existing services ...\n\n  bixarena-ai-service:\n    build:\n      context: ./apps/bixarena/ai-service\n      dockerfile: Dockerfile\n    container_name: bixarena-ai-service\n    ports:\n      - '8116:8116'\n    environment:\n      - BIXARENA_AI_AUTH_SERVICE_URL=http://bixarena-auth-service:8115\n      - BIXARENA_AI_JWT_EXPECTED_ISSUER=urn:bixarena:auth\n      - BIXARENA_AI_JWT_EXPECTED_AUDIENCE=urn:bixarena:ai\n      - BIXARENA_AI_PROMPT_VALIDATION_CONFIDENCE_THRESHOLD=0.5\n    depends_on:\n      - bixarena-auth-service\n    networks:\n      - bixarena-network\n    restart: unless-stopped\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#phase-6-testing-strategy","title":"Phase 6: Testing Strategy","text":""},{"location":"architecture/python-ai-service-integration-plan/#61-unit-tests","title":"6.1 Unit Tests","text":"<p>Create <code>apps/bixarena/ai-service/tests/test_jwt_validator.py</code>:</p> <pre><code>\"\"\"Tests for JWT validation logic.\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock, patch, MagicMock\nfrom fastapi import HTTPException\nfrom jose import jwt\nfrom bixarena_ai_service.security.jwt_validator import (\n    validate_jwt,\n    get_current_user_id,\n    get_current_user_roles,\n    get_jwks\n)\n\n# Test JWKS response\nMOCK_JWKS = {\n    \"keys\": [\n        {\n            \"kid\": \"test-key-id\",\n            \"kty\": \"RSA\",\n            \"use\": \"sig\",\n            \"n\": \"...\",\n            \"e\": \"AQAB\"\n        }\n    ]\n}\n\n@patch('bixarena_ai_service.security.jwt_validator.httpx.get')\ndef test_get_jwks_success(mock_get):\n    \"\"\"Test successful JWKS fetch.\"\"\"\n    mock_response = MagicMock()\n    mock_response.json.return_value = MOCK_JWKS\n    mock_get.return_value = mock_response\n\n    # Clear cache\n    get_jwks.cache_clear()\n\n    result = get_jwks()\n    assert result == MOCK_JWKS\n    mock_get.assert_called_once()\n\n@patch('bixarena_ai_service.security.jwt_validator.httpx.get')\ndef test_get_jwks_failure(mock_get):\n    \"\"\"Test JWKS fetch failure.\"\"\"\n    mock_get.side_effect = Exception(\"Connection error\")\n\n    # Clear cache\n    get_jwks.cache_clear()\n\n    with pytest.raises(HTTPException) as exc_info:\n        get_jwks()\n\n    assert exc_info.value.status_code == 503\n\ndef test_validate_jwt_missing_kid():\n    \"\"\"Test JWT validation with missing key ID.\"\"\"\n    mock_credentials = Mock()\n    mock_credentials.credentials = \"fake.jwt.token\"\n\n    with patch('bixarena_ai_service.security.jwt_validator.jwt.get_unverified_header') as mock_header:\n        mock_header.return_value = {}  # No 'kid' field\n\n        with pytest.raises(HTTPException) as exc_info:\n            validate_jwt(mock_credentials)\n\n        assert exc_info.value.status_code == 401\n        assert \"key ID\" in exc_info.value.detail\n\n@pytest.mark.asyncio\nasync def test_get_current_user_id_success():\n    \"\"\"Test extracting user ID from claims.\"\"\"\n    claims = {\"sub\": \"test-user-123\", \"roles\": [\"user\"]}\n\n    user_id = await get_current_user_id(claims)\n    assert user_id == \"test-user-123\"\n\n@pytest.mark.asyncio\nasync def test_get_current_user_id_missing():\n    \"\"\"Test handling missing subject claim.\"\"\"\n    claims = {\"roles\": [\"user\"]}\n\n    with pytest.raises(HTTPException) as exc_info:\n        await get_current_user_id(claims)\n\n    assert exc_info.value.status_code == 401\n\n@pytest.mark.asyncio\nasync def test_get_current_user_roles():\n    \"\"\"Test extracting roles from claims.\"\"\"\n    claims = {\"sub\": \"test-user-123\", \"roles\": [\"user\", \"admin\"]}\n\n    roles = await get_current_user_roles(claims)\n    assert roles == [\"user\", \"admin\"]\n\n@pytest.mark.asyncio\nasync def test_get_current_user_roles_empty():\n    \"\"\"Test handling missing roles claim.\"\"\"\n    claims = {\"sub\": \"test-user-123\"}\n\n    roles = await get_current_user_roles(claims)\n    assert roles == []\n</code></pre> <p>Create <code>apps/bixarena/ai-service/tests/test_prompt_validation_impl.py</code>:</p> <pre><code>\"\"\"Tests for prompt validation implementation.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, MagicMock\nfrom bixarena_ai_service.impl.prompt_validation_impl import PromptValidationApiImpl\nfrom bixarena_ai_service.config import Settings\n\n@pytest.mark.asyncio\nasync def test_validate_prompt_static_response():\n    \"\"\"Test that validate_prompt returns static confidence value.\"\"\"\n    impl = PromptValidationApiImpl()\n    settings = Settings()\n\n    result = await impl.validate_prompt(\n        prompt=\"What are the effects of aspirin on cardiovascular health?\",\n        user_id=\"test-user-123\",\n        roles=[\"user\"],\n        settings=settings\n    )\n\n    assert result.prompt == \"What are the effects of aspirin on cardiovascular health?\"\n    assert result.confidence == 0.75\n    assert result.is_biomedical is True\n\n@pytest.mark.asyncio\nasync def test_validate_prompt_long_text():\n    \"\"\"Test validation with long prompt text.\"\"\"\n    impl = PromptValidationApiImpl()\n    settings = Settings()\n\n    long_prompt = \"A\" * 5000\n\n    result = await impl.validate_prompt(\n        prompt=long_prompt,\n        user_id=\"test-user-123\",\n        roles=[\"user\"],\n        settings=settings\n    )\n\n    assert result.prompt == long_prompt\n    assert 0.0 &lt;= result.confidence &lt;= 1.0\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#62-integration-tests","title":"6.2 Integration Tests","text":"<p>Create <code>apps/bixarena/ai-service/tests/test_integration.py</code>:</p> <pre><code>\"\"\"Integration tests for authenticated endpoints.\"\"\"\n\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import patch, MagicMock\nfrom bixarena_ai_service.main import app\n\nclient = TestClient(app)\n\ndef test_health_check_no_auth():\n    \"\"\"Health check should work without authentication.\"\"\"\n    response = client.get(\"/health-check\")\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"pass\"\n\ndef test_validate_prompt_without_auth():\n    \"\"\"Endpoint should reject requests without JWT.\"\"\"\n    response = client.get(\"/validate-prompt?prompt=test\")\n    assert response.status_code == 403  # No Authorization header\n\ndef test_validate_prompt_with_invalid_jwt():\n    \"\"\"Endpoint should reject invalid JWT.\"\"\"\n    response = client.get(\n        \"/validate-prompt?prompt=test\",\n        headers={\"Authorization\": \"Bearer invalid-token\"}\n    )\n    assert response.status_code in [401, 503]  # 503 if JWKS fetch fails\n\n@patch('bixarena_ai_service.security.jwt_validator.get_jwks')\n@patch('bixarena_ai_service.security.jwt_validator.jwt.decode')\ndef test_validate_prompt_with_valid_jwt(mock_jwt_decode, mock_get_jwks):\n    \"\"\"Endpoint should accept valid JWT.\"\"\"\n    # Mock JWKS response\n    mock_get_jwks.return_value = {\n        \"keys\": [{\"kid\": \"test-key\", \"kty\": \"RSA\"}]\n    }\n\n    # Mock JWT decode to return valid claims\n    mock_jwt_decode.return_value = {\n        \"sub\": \"test-user-123\",\n        \"roles\": [\"user\"],\n        \"iss\": \"urn:bixarena:auth\",\n        \"aud\": \"urn:bixarena:ai\"\n    }\n\n    response = client.get(\n        \"/validate-prompt?prompt=What+is+diabetes\",\n        headers={\"Authorization\": \"Bearer valid-test-token\"}\n    )\n\n    assert response.status_code == 200\n    data = response.json()\n    assert \"prompt\" in data\n    assert \"confidence\" in data\n    assert \"isBiomedical\" in data\n    assert data[\"prompt\"] == \"What is diabetes\"\n    assert 0.0 &lt;= data[\"confidence\"] &lt;= 1.0\n\ndef test_validate_prompt_missing_parameter():\n    \"\"\"Endpoint should reject requests without prompt parameter.\"\"\"\n    with patch('bixarena_ai_service.security.jwt_validator.get_jwks'):\n        with patch('bixarena_ai_service.security.jwt_validator.jwt.decode') as mock_decode:\n            mock_decode.return_value = {\n                \"sub\": \"test-user\",\n                \"roles\": [\"user\"],\n                \"iss\": \"urn:bixarena:auth\",\n                \"aud\": \"urn:bixarena:ai\"\n            }\n\n            response = client.get(\n                \"/validate-prompt\",  # Missing prompt parameter\n                headers={\"Authorization\": \"Bearer valid-test-token\"}\n            )\n\n            assert response.status_code == 422  # Validation error\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#63-end-to-end-tests","title":"6.3 End-to-End Tests","text":"<p>Test the full flow through the API Gateway (requires running infrastructure):</p> <pre><code>\"\"\"E2E tests requiring full stack (API Gateway + Auth Service + AI Service).\"\"\"\n\nimport pytest\nimport httpx\n\n# These tests require the full infrastructure to be running\npytestmark = pytest.mark.e2e\n\n@pytest.fixture\ndef session_cookie():\n    \"\"\"Fixture that performs login and returns session cookie.\"\"\"\n    # TODO: Implement login flow to get real session cookie\n    # This would interact with the Gradio app or auth service directly\n    pass\n\n@pytest.mark.asyncio\nasync def test_full_authentication_flow(session_cookie):\n    \"\"\"Test complete flow: Session -&gt; Gateway -&gt; JWT -&gt; AI Service.\"\"\"\n\n    # Make request to API Gateway with session cookie\n    async with httpx.AsyncClient(base_url=\"http://localhost:8080\") as client:\n        response = await client.get(\n            \"/validate-prompt\",\n            params={\"prompt\": \"What causes cancer?\"},\n            cookies={\"JSESSIONID\": session_cookie}\n        )\n\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"prompt\"] == \"What causes cancer?\"\n        assert \"confidence\" in data\n        assert \"isBiomedical\" in data\n\n@pytest.mark.asyncio\nasync def test_anonymous_request_rejected():\n    \"\"\"Test that requests without session cookie are rejected.\"\"\"\n    async with httpx.AsyncClient(base_url=\"http://localhost:8080\") as client:\n        response = await client.get(\n            \"/validate-prompt\",\n            params={\"prompt\": \"test\"}\n        )\n\n        assert response.status_code == 401\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#phase-7-observability-monitoring","title":"Phase 7: Observability &amp; Monitoring","text":""},{"location":"architecture/python-ai-service-integration-plan/#71-structured-logging","title":"7.1 Structured Logging","text":"<p>Configure logging in <code>apps/bixarena/ai-service/bixarena_ai_service/logging_config.py</code>:</p> <pre><code>\"\"\"Logging configuration for AI service.\"\"\"\n\nimport logging\nimport sys\nfrom typing import Any\n\ndef configure_logging(log_level: str = \"INFO\") -&gt; None:\n    \"\"\"Configure structured logging for the application.\n\n    Args:\n        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n    \"\"\"\n    logging.basicConfig(\n        level=getattr(logging, log_level.upper()),\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(sys.stdout)\n        ]\n    )\n\n    # Set specific logger levels\n    logging.getLogger(\"uvicorn\").setLevel(logging.INFO)\n    logging.getLogger(\"fastapi\").setLevel(logging.INFO)\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n</code></pre> <p>Update <code>main.py</code> to configure logging on startup:</p> <pre><code>from bixarena_ai_service.logging_config import configure_logging\n\n# Configure logging\nconfigure_logging(log_level=\"INFO\")\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#72-health-info-endpoints","title":"7.2 Health &amp; Info Endpoints","text":"<p>The health check endpoint already exists. Consider adding an info endpoint in the OpenAPI spec:</p> <pre><code># In ai.openapi.yaml\npaths:\n  /actuator/info:\n    get:\n      x-anonymous-access: true\n      x-sage-internal: true\n      tags:\n        - Actuator\n      summary: Get service information\n      description: Returns information about the service version and configuration\n      operationId: getInfo\n      responses:\n        '200':\n          description: Success\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  name:\n                    type: string\n                  version:\n                    type: string\n                  environment:\n                    type: string\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#73-request-logging-middleware","title":"7.3 Request Logging Middleware","text":"<p>Add middleware to log all requests:</p> <pre><code># In main.py\nfrom fastapi import Request\nimport time\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@app.middleware(\"http\")\nasync def log_requests(request: Request, call_next):\n    \"\"\"Log all HTTP requests with timing information.\"\"\"\n    start_time = time.time()\n\n    # Log request\n    logger.info(f\"Request: {request.method} {request.url.path}\")\n\n    # Process request\n    response = await call_next(request)\n\n    # Log response\n    process_time = time.time() - start_time\n    logger.info(\n        f\"Response: {request.method} {request.url.path} \"\n        f\"status={response.status_code} duration={process_time:.3f}s\"\n    )\n\n    return response\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#74-metrics-future-enhancement","title":"7.4 Metrics (Future Enhancement)","text":"<p>Consider adding Prometheus metrics for:</p> <ul> <li>JWT validation attempts (success/failure)</li> <li>JWT validation duration</li> <li>JWKS fetch duration</li> <li>Prompt validation request count</li> <li>Prompt validation duration</li> </ul> <p>Libraries to consider:</p> <ul> <li><code>prometheus-client</code> - Official Prometheus Python client</li> <li><code>prometheus-fastapi-instrumentator</code> - FastAPI-specific metrics</li> </ul>"},{"location":"architecture/python-ai-service-integration-plan/#phase-8-documentation","title":"Phase 8: Documentation","text":""},{"location":"architecture/python-ai-service-integration-plan/#81-api-documentation","title":"8.1 API Documentation","text":"<p>The FastAPI application automatically generates interactive API documentation:</p> <ul> <li>Swagger UI: <code>http://localhost:8116/docs</code></li> <li>ReDoc: <code>http://localhost:8116/redoc</code></li> <li>OpenAPI JSON: <code>http://localhost:8116/openapi.json</code></li> </ul>"},{"location":"architecture/python-ai-service-integration-plan/#82-readme","title":"8.2 README","text":"<p>Create <code>apps/bixarena/ai-service/README.md</code>:</p> <pre><code># BixArena AI Service\n\nPython-based microservice for AI-powered features in the BixArena platform.\n\n## Features\n\n- **Prompt Validation**: Validates whether prompts are biomedically related\n- **JWT Authentication**: Integrates with BixArena auth service\n- **OpenAPI-Driven**: API spec-first development with code generation\n\n## Running Locally\n\n### Prerequisites\n\n- Python 3.13.3\n- Running BixArena auth service\n\n### Installation\n\n```bash\ncd apps/bixarena/ai-service\npip install -e .\n```\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#configuration","title":"Configuration","text":"<p>Set environment variables:</p> <pre><code>export BIXARENA_AI_AUTH_SERVICE_URL=http://localhost:8115\nexport BIXARENA_AI_JWT_EXPECTED_ISSUER=urn:bixarena:auth\nexport BIXARENA_AI_JWT_EXPECTED_AUDIENCE=urn:bixarena:ai\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#running","title":"Running","text":"<pre><code>uvicorn bixarena_ai_service.main:app --reload --port 8116\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#api-endpoints","title":"API Endpoints","text":""},{"location":"architecture/python-ai-service-integration-plan/#get-health-check","title":"<code>GET /health-check</code>","text":"<p>Health check endpoint (no authentication required).</p>"},{"location":"architecture/python-ai-service-integration-plan/#get-validate-promptprompttext","title":"<code>GET /validate-prompt?prompt={text}</code>","text":"<p>Validates if a prompt is biomedically related (requires authentication).</p> <p>Parameters:</p> <ul> <li><code>prompt</code> (query, required): The prompt text to validate</li> </ul> <p>Response:</p> <pre><code>{\n  \"prompt\": \"What causes diabetes?\",\n  \"confidence\": 0.75,\n  \"isBiomedical\": true\n}\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#architecture","title":"Architecture","text":"<p>This service integrates with the BixArena authentication architecture:</p> <ol> <li>Client sends request with session cookie to API Gateway</li> <li>Gateway validates session and mints JWT with audience <code>urn:bixarena:ai</code></li> <li>Gateway forwards request to AI service with JWT in Authorization header</li> <li>AI service validates JWT using JWKS from auth service</li> <li>AI service processes request and returns response</li> </ol>"},{"location":"architecture/python-ai-service-integration-plan/#development","title":"Development","text":""},{"location":"architecture/python-ai-service-integration-plan/#code-generation","title":"Code Generation","text":"<p>When OpenAPI specs change, regenerate code:</p> <pre><code>nx build bixarena-api-description\nnx run-many -t=generate -p=bixarena-*\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#project-structure","title":"Project Structure","text":"<pre><code>apps/bixarena/ai-service/\n\u251c\u2500\u2500 bixarena_ai_service/\n\u2502   \u251c\u2500\u2500 apis/              # Generated API routers\n\u2502   \u251c\u2500\u2500 models/            # Generated Pydantic models\n\u2502   \u251c\u2500\u2500 impl/              # Custom implementations (not generated)\n\u2502   \u251c\u2500\u2500 security/          # JWT validation logic\n\u2502   \u251c\u2500\u2500 config.py          # Configuration management\n\u2502   \u2514\u2500\u2500 main.py            # FastAPI application\n\u251c\u2500\u2500 tests/                 # Unit and integration tests\n\u251c\u2500\u2500 pyproject.toml         # Python dependencies\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#implementation-pattern","title":"Implementation Pattern","text":"<ol> <li>OpenAPI spec defines the API contract</li> <li>Code generation creates router stubs in <code>apis/</code></li> <li>Custom implementations go in <code>impl/</code> as subclasses</li> <li>Routers automatically discover and use implementations</li> </ol> <p>Do not edit generated files - they are marked in <code>.openapi-generator/FILES</code>.</p> <pre><code>## Summary Checklist\n\n### Phase 1: OpenAPI Specification\n- [ ] Create `libs/bixarena/api-description/src/paths/validate-prompt.yaml`\n- [ ] Create `libs/bixarena/api-description/src/components/schemas/PromptValidation.yaml`\n- [ ] Update `libs/bixarena/api-description/src/ai.openapi.yaml` with new path reference\n- [ ] Add \"Prompt Validation\" tag to OpenAPI spec\n\n### Phase 2: Code Generation\n- [ ] Run `nx build bixarena-api-description`\n- [ ] Run `nx run-many -t=generate -p=bixarena-*`\n- [ ] Verify generated files in `apps/bixarena/ai-service/bixarena_ai_service/`\n- [ ] Verify updated `apps/bixarena/api-gateway/src/main/resources/routes.yml`\n\n### Phase 3: Python Implementation\n- [ ] Update `apps/bixarena/ai-service/pyproject.toml` with `python-jose` dependency\n- [ ] Create `bixarena_ai_service/security/__init__.py`\n- [ ] Create `bixarena_ai_service/security/jwt_validator.py`\n- [ ] Create `bixarena_ai_service/config.py`\n- [ ] Create `bixarena_ai_service/impl/prompt_validation_impl.py`\n- [ ] Update `bixarena_ai_service/main.py` to include new router\n\n### Phase 4: API Gateway Configuration\n- [ ] Update `apps/bixarena/api-gateway/src/main/resources/application.yml` with AI service route\n- [ ] Verify route configuration in generated `routes.yml`\n\n### Phase 5: Docker &amp; Deployment\n- [ ] Create `apps/bixarena/ai-service/Dockerfile`\n- [ ] Update docker-compose configuration\n- [ ] Test local deployment\n\n### Phase 6: Testing\n- [ ] Create unit tests for JWT validation\n- [ ] Create unit tests for prompt validation implementation\n- [ ] Create integration tests for API endpoints\n- [ ] Create E2E tests for full authentication flow\n- [ ] Run test suite and verify coverage\n\n### Phase 7: Observability\n- [ ] Configure structured logging\n- [ ] Add request logging middleware\n- [ ] Verify health check endpoint\n- [ ] (Optional) Add Prometheus metrics\n\n### Phase 8: Documentation\n- [ ] Create `apps/bixarena/ai-service/README.md`\n- [ ] Document API endpoints\n- [ ] Document authentication flow\n- [ ] Document development workflow\n\n## Key Architecture Decisions\n\n### Why This Approach Works\n\n1. **Leverages Existing Infrastructure**: Uses auth service's `/oauth2/token?audience=urn:bixarena:ai` endpoint without any changes to auth service\n2. **Consistent Security Model**: Same JWT validation pattern as Java services, just implemented in Python\n3. **API-First Design**: OpenAPI spec drives both server stubs and route configuration, ensuring consistency\n4. **Zero Gateway Changes**: Gateway's `SessionToJwtFilter` already supports audience-specific JWTs through route configuration\n5. **Python-Native**: Uses FastAPI + `python-jose` instead of trying to replicate Spring Security\n6. **Separation of Concerns**: Generated code stays separate from custom implementations via the `impl/` pattern\n\n### Authentication Flow Diagram\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 Session Cookie \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Gradio \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 API Gateway \u2502 \u2502 App \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 1. Validate session \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Auth Service\u2502 \u2502 /userinfo \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 2. Mint JWT \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Auth Service\u2502 \u2502 /oauth2/ \u2502 \u2502 token?aud= \u2502 \u2502 urn:bix \u2502 \u2502 arena:ai \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 JWT (Bearer token) \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 AI Service \u2502 \u2502 (Python) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 3. Validate JWT \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Auth Service\u2502 \u2502 /.well-known\u2502 \u2502 /jwks.json \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <pre><code>### JWT Structure\n\nThe AI service receives JWTs with the following structure:\n\n```json\n{\n  \"iss\": \"urn:bixarena:auth\",\n  \"aud\": \"urn:bixarena:ai\",\n  \"sub\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"roles\": [\"user\", \"contributor\"],\n  \"iat\": 1234567890,\n  \"exp\": 1234568490,\n  \"nbf\": 1234567890\n}\n</code></pre>"},{"location":"architecture/python-ai-service-integration-plan/#security-considerations","title":"Security Considerations","text":"<ol> <li>JWT Validation: Every protected endpoint validates the JWT signature, expiration, issuer, and audience</li> <li>JWKS Caching: JWKS is cached using <code>@lru_cache</code> to reduce load on auth service</li> <li>No Session Cookies: API Gateway strips all cookies before forwarding to AI service</li> <li>Audience Isolation: Each microservice has its own audience, preventing JWT reuse across services</li> <li>Short-Lived Tokens: JWTs expire after 10 minutes, requiring fresh minting</li> </ol>"},{"location":"architecture/python-ai-service-integration-plan/#next-steps-after-implementation","title":"Next Steps After Implementation","text":"<ol> <li>Replace Static Implementation: Update <code>prompt_validation_impl.py</code> with actual ML model</li> <li>Add Rate Limiting: Consider per-user rate limits on prompt validation</li> <li>Add Caching: Cache validation results for identical prompts</li> <li>Add Metrics: Implement Prometheus metrics for monitoring</li> <li>Add More Endpoints: Expand AI service with additional ML-powered features</li> <li>Performance Testing: Load test the authentication flow and JWT validation</li> <li>Security Audit: Review JWT handling and validation logic</li> </ol>"},{"location":"architecture/template/","title":"[Architecture Plan Title]","text":""},{"location":"architecture/template/#overview","title":"Overview","text":"<p>[High-level description of what this architecture implements]</p> <p>[Brief summary of the key components and their interactions]</p>"},{"location":"architecture/template/#related-documentation","title":"Related Documentation","text":"<ul> <li>Source RFC: [Link to RFC if this originated from one]</li> <li>Related ADRs: [Links to relevant Architecture Decision Records]</li> </ul>"},{"location":"architecture/template/#architecture-diagram","title":"Architecture Diagram","text":"<p>[Include a diagram showing the system architecture. You can use:]</p> <ul> <li>Mermaid diagrams (embedded in markdown)</li> <li>Static images from <code>diagrams/</code> directory (<code>.gif</code>, <code>.png</code>, <code>.svg</code>)</li> <li>ASCII art for simple diagrams</li> </ul> <p>Example Mermaid diagram:</p> <pre><code>graph TD\n    A[Component A] --&gt; B[Component B]\n    B --&gt; C[Component C]\n    C --&gt; D[Data Store]</code></pre> <p>Example static image:</p> <pre><code>![Architecture Diagram](./diagrams/my-architecture.svg)\n</code></pre> <p>Design Notes:</p> <ul> <li>[Key architectural decisions]</li> <li>[Important constraints or considerations]</li> <li>[Extensibility points]</li> </ul>"},{"location":"architecture/template/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/template/#overview_1","title":"Overview","text":"<p>[High-level security approach and principles]</p>"},{"location":"architecture/template/#security-measures-by-component","title":"Security Measures by Component","text":""},{"location":"architecture/template/#component-1","title":"Component 1","text":"<p>[Security considerations and implementation]</p>"},{"location":"architecture/template/#component-2","title":"Component 2","text":"<p>[Security considerations and implementation]</p>"},{"location":"architecture/template/#secrets-management","title":"Secrets Management","text":"<p>[How sensitive data is stored and accessed]</p>"},{"location":"architecture/template/#network-security","title":"Network Security","text":"<p>[VPC configuration, security groups, network isolation]</p>"},{"location":"architecture/template/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<p>[How users/services are authenticated and authorized]</p>"},{"location":"architecture/template/#audit-compliance","title":"Audit &amp; Compliance","text":"<p>[Logging, monitoring, and compliance considerations]</p>"},{"location":"architecture/template/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/template/#component-1-name","title":"Component 1: [Name]","text":"<p>Purpose: [What this component does]</p> <p>Technology: [Language, framework, service]</p> <p>Key Files:</p> <ul> <li><code>path/to/file1.py</code> - [Description]</li> <li><code>path/to/file2.ts</code> - [Description]</li> </ul> <p>Implementation:</p> <p>[Detailed implementation notes, code examples, configuration]</p>"},{"location":"architecture/template/#component-2-name","title":"Component 2: [Name]","text":"<p>[Similar structure as Component 1]</p>"},{"location":"architecture/template/#database-schema","title":"Database Schema","text":""},{"location":"architecture/template/#tables","title":"Tables","text":""},{"location":"architecture/template/#table_name","title":"<code>table_name</code>","text":"<p>[Description of the table]</p> <p>Columns:</p> Column Type Constraints Description <code>id</code> <code>bigint</code> PRIMARY KEY Unique identifier <code>name</code> <code>varchar(255)</code> NOT NULL [Description] <code>created_at</code> <code>timestamp</code> NOT NULL, DEFAULT NOW() Creation timestamp <p>Indexes:</p> <ul> <li><code>idx_name</code> on <code>(column1, column2)</code> - [Purpose]</li> </ul> <p>Relationships:</p> <ul> <li>Foreign key to <code>other_table.id</code></li> </ul>"},{"location":"architecture/template/#infrastructure-cdkterraform","title":"Infrastructure (CDK/Terraform)","text":""},{"location":"architecture/template/#aws-resources","title":"AWS Resources","text":"<p>[List of AWS resources to be created]</p>"},{"location":"architecture/template/#cdk-stack-structure","title":"CDK Stack Structure","text":"<pre><code>apps/[product]/infra/cdk/\n\u251c\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 [product]-leaderboard-snapshot-stack.ts\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"architecture/template/#key-cdk-constructs","title":"Key CDK Constructs","text":"<p>[Code examples showing CDK construct definitions]</p>"},{"location":"architecture/template/#configuration","title":"Configuration","text":""},{"location":"architecture/template/#environment-variables","title":"Environment Variables","text":"Variable Description Example Required <code>DATABASE_URL</code> PostgreSQL connection string <code>postgresql://...</code> Yes <code>FEATURE_FLAG</code> Enable feature <code>true</code> No"},{"location":"architecture/template/#secrets-aws-secrets-manager","title":"Secrets (AWS Secrets Manager)","text":"Secret Name Description Format <code>[product]/db/credentials</code> Database credentials JSON: <code>{username, password, host, port, database}</code>"},{"location":"architecture/template/#error-handling-monitoring","title":"Error Handling &amp; Monitoring","text":""},{"location":"architecture/template/#error-handling-strategy","title":"Error Handling Strategy","text":"<p>[How errors are caught and handled]</p>"},{"location":"architecture/template/#logging","title":"Logging","text":"<p>[What is logged and where]</p>"},{"location":"architecture/template/#metrics","title":"Metrics","text":"<p>[Key metrics to monitor]</p>"},{"location":"architecture/template/#alerts","title":"Alerts","text":"<p>[What alerts should be configured]</p>"},{"location":"architecture/template/#notifications","title":"Notifications","text":"<p>[How teams are notified of issues]</p>"},{"location":"architecture/template/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/template/#unit-tests","title":"Unit Tests","text":"<p>[What should be unit tested]</p>"},{"location":"architecture/template/#integration-tests","title":"Integration Tests","text":"<p>[What should be integration tested]</p>"},{"location":"architecture/template/#end-to-end-tests","title":"End-to-End Tests","text":"<p>[What should be tested end-to-end]</p>"},{"location":"architecture/template/#manual-testing","title":"Manual Testing","text":"<p>[Steps for manual verification]</p>"},{"location":"architecture/template/#deployment-plan","title":"Deployment Plan","text":""},{"location":"architecture/template/#prerequisites","title":"Prerequisites","text":"<ul> <li> Prerequisite 1</li> <li> Prerequisite 2</li> </ul>"},{"location":"architecture/template/#phase-1-phase-name","title":"Phase 1: [Phase Name]","text":"<p>Timeline: [Date range]</p> <p>Steps:</p> <ol> <li>Step 1</li> <li>Step 2</li> </ol> <p>Validation:</p> <ul> <li> Validation criterion 1</li> <li> Validation criterion 2</li> </ul>"},{"location":"architecture/template/#phase-2-phase-name","title":"Phase 2: [Phase Name]","text":"<p>[Similar structure as Phase 1]</p>"},{"location":"architecture/template/#rollback-plan","title":"Rollback Plan","text":"<p>[How to rollback if issues occur]</p>"},{"location":"architecture/template/#migration-plan-if-applicable","title":"Migration Plan (if applicable)","text":""},{"location":"architecture/template/#data-migration","title":"Data Migration","text":"<p>[How existing data will be migrated]</p>"},{"location":"architecture/template/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>[How to maintain compatibility during transition]</p>"},{"location":"architecture/template/#migration-validation","title":"Migration Validation","text":"<p>[How to verify migration success]</p>"},{"location":"architecture/template/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/template/#expected-load","title":"Expected Load","text":"<p>[Expected traffic/usage patterns]</p>"},{"location":"architecture/template/#scalability","title":"Scalability","text":"<p>[How the system scales]</p>"},{"location":"architecture/template/#performance-targets","title":"Performance Targets","text":"<ul> <li>Response time: [Target]</li> <li>Throughput: [Target]</li> <li>Availability: [Target]</li> </ul>"},{"location":"architecture/template/#bottlenecks","title":"Bottlenecks","text":"<p>[Potential performance bottlenecks and mitigation strategies]</p>"},{"location":"architecture/template/#cost-analysis","title":"Cost Analysis","text":""},{"location":"architecture/template/#estimated-monthly-costs","title":"Estimated Monthly Costs","text":"Resource Cost Justification Lambda invocations $X [Calculation] Data storage $Y [Calculation] Total $Z"},{"location":"architecture/template/#cost-optimization","title":"Cost Optimization","text":"<p>[Strategies to minimize costs]</p>"},{"location":"architecture/template/#future-enhancements","title":"Future Enhancements","text":"<p>[Potential future improvements or extensions]</p> <ol> <li>Enhancement 1</li> <li>Enhancement 2</li> </ol>"},{"location":"architecture/template/#open-questions","title":"Open Questions","text":"<p>[Questions that need to be resolved during implementation]</p> <ol> <li>Question 1?</li> <li>Question 2?</li> </ol>"},{"location":"architecture/template/#references","title":"References","text":"<ul> <li>[Link to related documentation]</li> <li>[Link to external resources]</li> <li>[Link to related PRs/issues]</li> </ul>"},{"location":"architecture/diagrams/","title":"Architecture Diagrams","text":"<p>This directory contains visual diagrams for system architecture and design documentation.</p>"},{"location":"architecture/diagrams/#supported-formats","title":"Supported Formats","text":"<ul> <li>GIF (<code>.gif</code>) - Animated diagrams, flowcharts</li> <li>PNG (<code>.png</code>) - Static images, screenshots</li> <li>SVG (<code>.svg</code>) - Scalable vector graphics (preferred for diagrams)</li> <li>HTML (<code>.html</code>) - Interactive diagrams (e.g., Mermaid, PlantUML exports)</li> </ul>"},{"location":"architecture/diagrams/#current-diagrams","title":"Current Diagrams","text":"<ul> <li>bixarena-architecture.gif - Current BixArena system architecture</li> <li>openchallenges-architecture.gif - Current OpenChallenges system architecture</li> </ul>"},{"location":"architecture/diagrams/#guidelines","title":"Guidelines","text":"<ul> <li>Naming convention: Use kebab-case with dashes (e.g., <code>feature-name-architecture.svg</code>)</li> <li>Use descriptive filenames that clearly identify the system or component</li> <li>Include version numbers for major revisions (e.g., <code>bixarena-architecture-v2.svg</code>)</li> <li>Prefer SVG for diagrams (better for version control and scaling)</li> <li>Update the parent README.md when adding new diagrams</li> <li>Keep source files (e.g., <code>.drawio</code>, <code>.mmd</code>) in a separate <code>source/</code> subdirectory if needed</li> </ul>"},{"location":"architecture/diagrams/#creating-diagrams","title":"Creating Diagrams","text":"<p>Recommended Tools:</p> <ul> <li>Mermaid - Text-based diagrams (can be embedded in markdown)</li> <li>Excalidraw - Hand-drawn style diagrams</li> <li>draw.io - Professional diagrams</li> <li>PlantUML - UML diagrams from text</li> </ul> <p>Mermaid Example (embed directly in markdown):</p> <pre><code>graph TD\n    A[Client] --&gt; B[API Gateway]\n    B --&gt; C[Lambda]\n    C --&gt; D[Database]</code></pre> <p>For Complex Diagrams:</p> <ol> <li>Create diagram using your preferred tool</li> <li>Export as SVG (preferred) or PNG</li> <li>Save in this directory</li> <li>Reference from architecture docs using relative path</li> </ol>"},{"location":"blog/","title":"Blog","text":"<p>Welcome to the Sage Monorepo blog! Here you'll find the latest updates, insights, and stories from our development teams.</p>"},{"location":"blog/#latest-posts","title":"Latest Posts","text":""},{"location":"blog/#coming-soon","title":"Coming Soon","text":"<p>We're setting up our blog to share:</p> <ul> <li>Technical Deep Dives: In-depth explorations of our architecture, tools, and methodologies</li> <li>Product Updates: Major feature releases and improvements across our platform</li> <li>Developer Stories: Behind-the-scenes looks at how we solve complex problems</li> <li>Community Highlights: Showcasing contributions and collaborations</li> <li>Best Practices: Lessons learned and recommendations for modern development</li> </ul> <p>Stay tuned for our first posts!</p>"},{"location":"blog/#subscribe","title":"Subscribe","text":"<p>To stay updated with our latest blog posts:</p> <ul> <li>Watch this repository on GitHub</li> <li>Follow our Updates page for regular announcements</li> <li>Check back here regularly for new content</li> </ul> <p>Want to contribute a blog post? Check out our Contributing Guidelines to learn how to share your story with the community.</p>"},{"location":"contributions/overview/","title":"Contributing","text":"<p>Thank you contributing to the Sage Monorepo! We welcome any and all feedback, ideas, and suggestions.</p> <p>Contributions can be made via issues and pull requests (PRs).</p> <ul> <li>Reporting a bug</li> <li>Adding a feature</li> </ul>"},{"location":"contributions/overview/#code-of-conduct","title":"Code of Conduct","text":"<p>We take our open source community seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.</p>"},{"location":"develop/quick-start/","title":"Quick Start","text":"<p>Content comming soon!</p>"},{"location":"develop/remote-dev/","title":"Remote Development","text":"<p>Content comming soon!</p>"},{"location":"develop/advanced/creating-a-commit-with-multiple-authors/","title":"Creating a commit with multiple authors","text":""},{"location":"develop/advanced/creating-a-commit-with-multiple-authors/#introduction","title":"Introduction","text":"<p>You can attribute a commit to more than one author by adding one or more <code>Co-authored-by</code> trailers to the commit's message. Co-authored commits are visible on GitHub.</p>"},{"location":"develop/advanced/creating-a-commit-with-multiple-authors/#when-to-add-co-authors-to-a-commit","title":"When to add co-authors to a commit?","text":"<p>Annotating commits with <code>Co-authored-by</code> is generally encouraged when copying code from other developers.</p> <p>Annotating commits with <code>Co-authored-by</code> is required when you are committing previously-untracked code written by other users.</p> <p>Example</p> <ol> <li>Bea develops a new feature and submits it as a pull request (PR).</li> <li>Eddy is tasked with refactoring Bea's PR into multiple PRs.</li> <li>Eddy copies and pastes code written by Bea and adapts it.</li> <li>Eddy adds Bea as a co-author to the commits that include code she wrote.</li> </ol>"},{"location":"develop/advanced/creating-a-commit-with-multiple-authors/#adding-co-authors-to-a-commit","title":"Adding co-authors to a commit","text":"<p>Follow the instructions described here.</p> <p>Preview of the commit creation:</p> <pre><code>$ git commit -m \"Refactor usability tests.\n&gt;\n&gt;\nCo-authored-by: NAME &lt;EMAIL&gt;\nCo-authored-by: ANOTHER-NAME &lt;ANOTHER-EMAIL&gt;\"\n</code></pre>"},{"location":"develop/advanced/creating-a-commit-with-multiple-authors/#names-and-emails-of-sage-monorepo-contributors","title":"Names and emails of Sage Monorepo contributors","text":"<p>The name and \"no-reply\" emails of the Sage Monorepo contributors (sorted alphabetically):</p> <pre><code>Co-authored-by: andrewelamb &lt;7220713+andrewelamb@users.noreply.github.com&gt;\n</code></pre> <pre><code>Co-authored-by: gaiaandreoletti &lt;46945609+gaiaandreoletti@users.noreply.github.com&gt;\n</code></pre> <pre><code>Co-authored-by: Lingling &lt;55448354+linglp@users.noreply.github.com&gt;\n</code></pre> <pre><code>Co-authored-by: mdsage1 &lt;122999770+mdsage1@users.noreply.github.com&gt;\n</code></pre> <pre><code>Co-authored-by: Rongrong Chai &lt;73901500+rrchai@users.noreply.github.com&gt;\n</code></pre> <pre><code>Co-authored-by: sagely1 &lt;114952739+sagely1@users.noreply.github.com&gt;\n</code></pre> <pre><code>Co-authored-by: Thomas Schaffter &lt;3056480+tschaffter@users.noreply.github.com&gt;\n</code></pre> <pre><code>Co-authored-by: Verena Chung &lt;9377970+vpchung@users.noreply.github.com&gt;\n</code></pre> <p>Note</p> <p>The names, usernames and user IDs of the contributors are collected from their GitHub profile pages. If a contributor does not specify their name on the profile page, the listing uses their username instead of their name. The user ID can be found in the URL of the user's avatar.</p>"},{"location":"develop/advanced/creating-a-commit-with-multiple-authors/#references","title":"References","text":"<ul> <li>Creating a commit with multiple   authors</li> </ul>"},{"location":"develop/advanced/developing-on-a-remote-host/","title":"Developing on a remote host","text":""},{"location":"develop/advanced/developing-on-a-remote-host/#introduction","title":"Introduction","text":"<p>Team members who develop locally may not benefit from the same compute resources. The most notable resources that can impact the productivity of developers are the number and frequency of the CPU cores, the memory available and internet speed. The worse case is when a machine does not have the resources to run the apps that the team develops, for example when not enough memory is available. On other times, the time required to complete a task may be many times slower on a computer with lower CPU resources.</p> <p>Moreover, working remotely means that developers no longer benefit from the same internet speed, either because of the quality of the internet connection available at their location or because the speed is shared among the members of a household. As a result, tasks that involve downloading or uploading artifacts, like pulling or pushing Docker images, may take significantly longer to complete.</p> <p>This page describes how to setup a environment that enables developers to use VS Code while using the compute resources of a remote host.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#motivation","title":"Motivation","text":"<p>To illustrate the benefit of developing on a remote host, this table summarizes the local compute resources available to the developers of OpenChallenges in 2023. The same information is displayed for two types of Amazon EC2 instances and one type of GitHub Codespace instance that were selected as candidate alternative development environments for the team members. The table also includes the runtimes in seconds of different tasks such as linting or testing all the projects included in the monorepo (the method used to generate these results is described in the next section).</p> Shirou Rin Sakura m5.2xlarge t3a.xlarge 4-core Codespace 8-core Codespace Computer Type Desktop PC MacBook Pro MacBook Pro Amazon EC2 Amazon EC2 GitHub Codespace GitHub Codespace Architecture 64-bit (x86) 64-bit (x86) 64-bit (x86) 64-bit (x86) 64-bit (x86) 64-bit (x86) 64-bit (x86) CPU Count 8 4 4 8 4 4 8 CPU Frequency (GHz) 3.6 2.4 1.7 2.5 2.2 2.7 2.8 Memory (GB) 32 16 16 32 16 8 16 Runtime: Lint All Projects (s) 15.4 208.9 183.8 18.6 33.4 24.6 16.9 Runtime: Build All Projects (s) 19.4 196.2 162.2 26.7 44.9 32.3 14.1 Runtime: Test All Projects (s) 12.4 117.1 82.8 15.3 29.2 31.6 24.5 Runtime: Test api (s) 6.2 29.6 21.3 7.2 10.4 6.5 6.5 Runtime: Test web-app (s) 5.3 43.0 35.0 6.5 9.2 6.7 6.0 Download speed (Mbit/s) 395.9 52.1 160.1 2165.0 1606.7 8571 8603 Upload speed (Mbit/s) 183.3 15.6 10.3 1861.0 1030.2 4893 5125 On-Demand Cost ($/day) n/a n/a n/a 9.2 3.6 8.64 (1,2) 17.28 (1,2) On-Demand Cost ($/year) n/a n/a n/a 3363.8 1317.5 3153.6 (1,2) 6307.2 (1,2) <p>(1) GitHub codespaces stop automatically after 1h of inactivity. A codespace used by an full-time engineer (8h/day) - without taking into account vacation for the sake of simplicity - would cost 8 hours/day _ 5 days/week _ 52 weeks * $0.36/hour (4-core) = $748/year (see Codespaces pricing). Similarly, the cost for an 8-core codespace would become $1496/year. In addition, GitHub bills $0.07 of GB of storage independently on whether the codespace is running or stopped. Pricing valid on 2023-12-31.</p> <p>(2) GitHub offers core hours and storage. For example, a Free user can use a 2-core instance for 60 hours per month for free or an 8-core instance for 15 hours. You will be notified by email when you have used 75%, 90%, and 100% of your included quotas.</p> <ul> <li>Free users: 120 core hours/month and 15 GB month of storage</li> <li>Pro users: 180 core hours/month and 20 GB month of storage</li> </ul> <p>Note</p> <p>Note that developers have been asked to measure runtimes and internet speeds while keeping open the applications that are usually running when they develop (e.g. Spotify, several instances of VS Code, browser with many tabs open). This could be one reason why runtimes reported by a developer are larger that those reported by another developer who has less compute resources available.</p> <p>The table below shows the number of times a task ran by a developer is faster than the slowest runtime (denoted by \"1.0\").</p> Shirou Rin Sakura m5.2xlarge t3a.xlarge Runtime: Lint All Projects 13.6 1.0 1.1 11.2 6.3 Runtime: Build All Projects 10.1 1.0 1.2 7.3 4.4 Runtime: Test All Projects 9.4 1.0 1.4 7.6 4.0 Runtime: Test api 4.8 1.0 1.4 4.1 2.8 Runtime: Test web-app 8.0 1.0 1.2 6.6 4.6 Download speed 7.6 1.0 3.1 41.5 30.8 Upload speed 17.8 1.5 1.0 180.5 99.9 <p>For example, linting all the projects of this monorepo is 13.6 times faster on Shirou's computer than on Rin's. Moreover, all the developers can benefit from improved download speeds (up to 41.5 faster for Rin) and upload speeds (up to 180.5 times faster for Sakura) when developing on an EC2 instance. This table illustrates well the diversity in compute resources available locally to developers, and how relying on remote hosts like EC2 instances can provide a better working environment to developers.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#collectings-os-info-and-benchmarking-tasks","title":"Collectings OS info and benchmarking tasks","text":"<p>Runtimes are obtained from this commit.</p> <p>Identification of the compute resources.</p> <pre><code>$ nproc\n$ cat /proc/cpuinfo\n$ cat /proc/meminfo\n</code></pre> <p>Runtimes are averaged over 10 runs that follow a warmup run using hyperfine.</p> <pre><code>$ hyperfine --warmup 1 --runs 10 'nx run-many --all --target=lint --skip-nx-cache'\n$ hyperfine --warmup 1 --runs 10 'nx run-many --all --target=build --skip-nx-cache'\n$ hyperfine --warmup 1 --runs 10 'nx run-many --all --target=test --skip-nx-cache'\n$ hyperfine --warmup 1 --runs 10 'nx test api --skip-nx-cache'\n$ hyperfine --warmup 1 --runs 10 'nx test web-ui --skip-nx-cache'\n</code></pre> <p>Internet speeds are measured with speedtest-cli.</p> <pre><code>$ speedtest\n</code></pre>"},{"location":"develop/advanced/developing-on-a-remote-host/#preparing-the-remote-host-aws-ec2","title":"Preparing the remote host - AWS EC2","text":"<p>This section describes how to instantiate an AWS EC2 as the remote host. Steps outlined below will assume you have access to the Sage AWS Service Catalog.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#creating-the-ec2-instance","title":"Creating the EC2 instance","text":"<ol> <li>Log in to the Service Catalog with your Synapse credentials.</li> <li>From the list of Products, select EC2: Linux Docker. On the Product page, click on Launch    product in the upper-right corner.</li> <li>On the next page, fill out the wizard as follows:<ul> <li>Provisioned product name<ul> <li>Name: <code>{GitHub username}-devcontainers-{yyyymmdd}</code></li> <li>Example: <code>tschaffter-devcontainers-20240404</code></li> </ul> </li> <li>Parameters<ul> <li>EC2 Instance Type: <code>t3a.2xlarge</code></li> <li>Base Image: <code>AmazonLinuxDocker</code> (leave default)</li> <li>Disk Size: 80</li> </ul> </li> <li>Manage tags<ul> <li><code>CostCenter</code>: Select the Cost Center associated to your project</li> </ul> </li> <li>Enable event notifications: SKIP - DO NOT MODIFY</li> </ul> </li> <li>Click on Launch product. Your instance will take anywhere between 3-5 minutes to deploy. You    can either wait on this page until \"EC2Instance\" shows up on the list under Resources, or you can    leave and come back at a later time.</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#stopping-the-ec2-instance","title":"Stopping the EC2 instance","text":"<p>It's not something you should do now as part of this tutorial. This section serves as a reminder that AWS charges for evey hour the EC2 instance is running. As soon as you identify that you will no longer need the instance for the rest of the day, open the Service Catalog to stop it.</p> <ol> <li>Open the Service Catalog, then select Provisioned products.</li> <li>Select the EC2 instance.</li> <li>Click on the button Actions &gt; Service actions &gt; Stop.</li> <li>Confirm the action.</li> </ol> <p>After a few seconds, the EC2 instance will be stopped.</p> <p>Note</p> <p>AWS still charges us for the storage space that the EC2 instance takes even when it's not running. Consider destroying the EC2 instance when you decide that you will no longer need it.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#connecting-to-the-ec2-instance-with-aws-console","title":"Connecting to the EC2 instance with AWS Console","text":"<p>We will now use the AWS Console to open a terminal to the EC2 instance and setup your public SSH key.</p> <p>Note</p> <p>This section assumes that you already have a public and private SSH key created on your local machine from where you are running VS Code.</p> <ol> <li>Open the Service Catalog, then select Provisioned products.</li> <li>In the section Resources, click on the link for \"EC2Instance\".</li> <li>Click on the checkbox of the new EC2 instance created.</li> <li>Click on the button Actions &gt; Connect.<ul> <li>The error \"Failed to describe security groups\" shown by AWS can be ignored.</li> </ul> </li> <li>Click on the tab Session Manager.</li> <li>Click on Connect.</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#configuring-the-ssh-public-key-on-the-ec2-instance","title":"Configuring the SSH public key on the EC2 instance","text":"<ol> <li>Login as the user <code>ec2-user</code> and move to its home directory.    <pre><code>$ sudo -s\n# su ec2-user\n$ cd\n</code></pre></li> <li>Create the folder <code>~/.ssh</code> (if needed).    <pre><code>$ mkdir ~/.ssh\n$ chmod 700 ~/.ssh\n</code></pre></li> <li>Create the file <code>~/.ssh/authorized_keys</code> (if needed).    <pre><code>$ touch ~/.ssh/authorized_keys\n$ chmod 644 ~/.ssh/authorized_keys\n</code></pre></li> <li>Copy and paste your public SSH key at the end of <code>~/.ssh/authorized_keys</code>.</li> <li>Click on the button Terminate to terminate the session and confirm the action.</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#configuring-ssh-on-the-local-machine","title":"Configuring SSH on the local machine","text":"<p>This section describes how to create a profile for the EC2 instance in your local <code>~/.ssh/config</code> file.</p> <p>Note</p> <p>This section assumes that you already have a public and private SSH key created on your local machine from where you are running VS Code.</p> <p>First, you need to identify the private IP address of the EC2 instance.</p> <ol> <li>Open the Service Catalog, then select Provisioned products.</li> <li>In the section Outputs, the private IP address is the value associated to    \"EC2InstancePrivateIpAddress\".</li> </ol> <p>Then, on your local machine:</p> <ol> <li>Create the file <code>~/.ssh/config</code> (if needed).    <pre><code>$ touch ~/.ssh/config\n$ chmod 600 ~/.ssh/config\n</code></pre></li> <li>Add the following content to your local <code>~/.ssh/config</code>.    <pre><code>Host {alias}\n    HostName {private ip}\n    User ec2-user\n    IdentityFile {path to your private SSH key, e.g. ~/.ssh/id_rsa}\n</code></pre>    where the placeholder values <code>{...}</code> should be replaced with the correct values.</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#connecting-to-the-ec2-instance-with-vs-code","title":"Connecting to the EC2 instance with VS Code","text":"<ol> <li>Connect to the Sage VPN.</li> <li>Open VS Code.</li> <li>Install the VS Code extension pack \"Remote Development\".</li> <li>Open the command palette with <code>Ctrl+Shit+P</code>.</li> <li><code>Remote-SSH: Connect to Host...</code> &gt; Select the host.</li> <li>Answer the prompts</li> </ol> <p>You are now connected to the EC2 instance! \ud83d\ude80</p> <p>Tip</p> <p>Please remember to stop the EC2 instance at the end of your working day to save on costs.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#next","title":"Next","text":"<p>Go to the section XXX for the instructions on how to setup your environment to contribute to Sage Monorepo.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#preparing-the-remote-host-github-codespace","title":"Preparing the remote host - GitHub Codespace","text":"<p>This section describes how to open your fork of Sage Monorepo in a GitHub Codespaces instance.</p> <p>Note</p> <p>In practice, we will prefer to develop in an EC2 instance created from the Service Catalog for security and budget reasons. Please refer to the instructions given above. Using a GitHub Codespace has been proven to be ponctually useful for quick tests that require a fresh environment, as one of Codespaces benefits is that they can be created and destroyed faster than EC2 instances.</p> <ol> <li>Open your browser and go to GitHub Codespaces.</li> <li>Click on the \"New codespace\".</li> <li>Enter the information requested:<ul> <li>Repository: Select your fork of the monorepo</li> <li>Branch: Select the default branch</li> <li>Dev container configuration: Select the dev container definition</li> <li>Region: Select your preferred region</li> <li>Machine type: Select the machine type</li> </ul> </li> <li>Click on \"Create codespace\".</li> <li>Wait for the codespace to be created.</li> <li>Configure the monorepo and install its dependencies (see README).</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#stopping-a-codespace-instance","title":"Stopping a Codespace instance","text":"<p>If your codespace is open in your browser, you can stop it with the following step. Note that a codespace stops automatically after one hour of inactivity.</p> <ol> <li>Click on the button \"Codespaces\" located in the bottom-left corner.</li> <li>Click on \"Stop Current Codespace\".</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#opening-a-codespace-with-vs-code","title":"Opening a Codespace with VS Code","text":"<p>If you prefer to develop with VS Code rather than inside your browser:</p> <ol> <li>Open your browser and go to GitHub Codespaces.</li> <li>Find the codespace that you want to open with VS Code.</li> <li>Click on the three-dot menu &gt; \"Open in ...\" &gt; \"Open in Visual Studio Code\"</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#changing-the-machine-type","title":"Changing the machine type","text":"<p>The type of machine used by a codespace can be changed at any time, for example when a beefier codespace instance is needed. To change the machine type of an existing codespace.</p> <ol> <li>Stop the codespace.</li> <li>Open your browser and go to GitHub Codespaces.</li> <li>Find the codespace that you want to open with VS Code.</li> <li>Click on the three-dot menu &gt; \"Change machine type\".</li> <li>Update the properties of the machine and click on \"Update codespace\".</li> </ol>"},{"location":"develop/advanced/developing-on-a-remote-host/#accessing-apps-and-services","title":"Accessing apps and services","text":"<p>The devcontainer provided with this project uses the VS Code devcontainer feature <code>docker-in-docker</code>. In addition to isolating the Docker engine running in the devcontainer from the engine running on the host, this feature enables VS Code to forward the ports defined in <code>devcontainer.json</code> to the local envrionment of the developer. Therefore, apps and services can be accessed using the address <code>http://localhost</code> even though they are running on the remote host!</p> <p>Accessing the apps and services using the IP address of the remote host won't work, unless you replace the feature <code>docker-in-docker</code> by <code>docker-from-docker</code>. In this case, <code>http://localhost</code> can no longer be used to access the apps and services.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#uploading-files","title":"Uploading files","text":"<p>Simply drag and drop files to the VS Code explorer to upload files from your local environment to the remote host.</p>"},{"location":"develop/advanced/developing-on-a-remote-host/#closing-the-remote-connection","title":"Closing the remote connection","text":"<p>Click on the button in the bottom-left corner of VS Code and select one of these options:</p> <ul> <li><code>Close Remote Connection</code> to close the connection with the remote host.</li> <li><code>Reopen Folder in SSH</code> if you want to stop the devcontainer but stay connected to the remote host.</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/","title":"Terraform Infrastructure Architecture","text":""},{"location":"develop/architecture/terraform-infrastructure/#overview","title":"Overview","text":"<p>The Sage Monorepo uses OpenTofu (with Terragrunt) to manage cloud infrastructure as code, with a focus on reusability, consistency, and best practices. This architecture enables teams to deploy cloud resources efficiently while maintaining security and compliance standards.</p>"},{"location":"develop/architecture/terraform-infrastructure/#understanding-the-tools","title":"Understanding the Tools","text":"<p>Before diving in, it's important to understand the relationship between these tools:</p> <p>Terraform is the original open-source Infrastructure as Code (IaC) tool created by HashiCorp. It uses HCL (HashiCorp Configuration Language) to define and provision infrastructure.</p> <p>OpenTofu is an open-source fork of Terraform, created after HashiCorp changed Terraform's license in 2023. OpenTofu is:</p> <ul> <li>Fully compatible with Terraform syntax and modules</li> <li>Binary-compatible (drop-in replacement for the <code>terraform</code> command)</li> <li>Maintained by the Linux Foundation</li> <li>What this monorepo actually uses (version 1.10.6)</li> </ul> <p>Terragrunt is a thin wrapper around Terraform/OpenTofu that adds features:</p> <ul> <li>DRY (Don't Repeat Yourself) configuration</li> <li>Layered configuration inheritance</li> <li>Automatic backend initialization</li> <li>Dependency management between modules</li> </ul> <p>Why OpenTofu + Terragrunt?</p> <ul> <li>Infrastructure as Code (IaC): Version control your infrastructure alongside application code</li> <li>Reusable Modules: Create once, use everywhere approach for common infrastructure patterns</li> <li>DRY Principle: Terragrunt eliminates configuration duplication across environments</li> <li>State Management: Centralized, secure state storage with locking</li> <li>Multi-Environment: Deploy identical infrastructure to dev, staging, and production</li> <li>Open Source: OpenTofu ensures the tooling remains community-driven and free</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#key-technologies","title":"Key Technologies","text":"Technology Version Purpose OpenTofu 1.10.6 Infrastructure provisioning (Terraform-compatible) Terragrunt 0.87.5 DRY configuration wrapper for OpenTofu/Terraform AWS Provider 5.88.0 AWS resource management <p>Terminology in This Documentation</p> <p>While we use OpenTofu, the directory structure and common terminology still refer to \"Terraform projects\" (e.g., <code>apps/bixarena/infra/terraform/</code>). In this documentation:</p> <pre><code>- **Terraform project** = An Nx project that manages infrastructure using OpenTofu/Terragrunt\n- **Module** = A logical infrastructure component (VPC, database, etc.)\n- **Terraform** and **OpenTofu** are used interchangeably (they're compatible)\n- Commands shown use `terragrunt` which wraps the `tofu` binary\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#architecture-principles","title":"Architecture Principles","text":""},{"location":"develop/architecture/terraform-infrastructure/#1-layered-configuration-model","title":"1. Layered Configuration Model","text":"<p>Infrastructure configuration follows a three-layer hierarchy to maximize reusability and minimize duplication:</p> <pre><code>workspace.hcl (root)              # Organization-wide defaults\n    \u2193\nproject.hcl (per Nx project)      # Project-specific settings\n    \u2193\nterragrunt.hcl (per module)       # Module-specific configuration\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#workspace-layer-workspacehcl","title":"Workspace Layer (<code>workspace.hcl</code>)","text":"<p>Located at the repository root, defines global constants:</p> <pre><code>locals {\n  workspace_root = get_repo_root()\n  organization   = \"sage\"\n\n  project_paths = {\n    terraform = \"${local.workspace_root}/libs/platform/infra/terraform\"\n  }\n}\n\ninputs = {\n  organization  = local.organization\n  project_paths = local.project_paths\n}\n</code></pre> <p>Responsibilities:</p> <ul> <li>Organization name and branding</li> <li>Shared module registry paths</li> <li>Global tagging schema</li> <li>Default conventions</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#project-layer-projecthcl","title":"Project Layer (<code>project.hcl</code>)","text":"<p>Each Nx infrastructure project has a <code>project.hcl</code>:</p> <pre><code>locals {\n  workspace_vars = read_terragrunt_config(find_in_parent_folders(\"workspace.hcl\"))\n  _config_yaml   = yamldecode(file(\"config.yaml\"))\n\n  project_vars = {\n    product     = get_env(\"PRODUCT\", local._config_yaml.product)\n    application = get_env(\"APPLICATION\", local._config_yaml.application)\n    environment = get_env(\"ENVIRONMENT\", local._config_yaml.environment)\n  }\n}\n</code></pre> <p>Responsibilities:</p> <ul> <li>Project name and metadata</li> <li>Environment definitions</li> <li>Remote state backend configuration</li> <li>Provider settings</li> <li>Load project-specific <code>config.yaml</code></li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#module-layer-moduleterragrunthcl","title":"Module Layer (<code>&lt;module&gt;/terragrunt.hcl</code>)","text":"<p>Each logical component (VPC, database, load balancer) has its own module:</p> <pre><code>include \"project\" {\n  path = find_in_parent_folders(\"project.hcl\")\n}\n\nterraform {\n  source = \"${include.project.inputs.project_paths.terraform}//modules/terraform-s3-backend\"\n}\n\ninputs = {\n  region     = \"us-east-1\"\n  component  = \"backend\"\n  # Module-specific configuration\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#2-configuration-precedence","title":"2. Configuration Precedence","text":"<p>Values cascade with clear precedence (highest to lowest):</p> <ol> <li>Environment Variables - Runtime overrides (e.g., <code>TERRAFORM_BACKEND_BUCKET_NAME</code>)</li> <li>config.yaml - Project-specific values</li> <li>Default Values - Module variable defaults</li> </ol> <p>This allows flexibility for CI/CD pipelines, local development, and different environments.</p>"},{"location":"develop/architecture/terraform-infrastructure/#3-reusable-module-pattern","title":"3. Reusable Module Pattern","text":"<p>Modules are organized into two categories:</p>"},{"location":"develop/architecture/terraform-infrastructure/#shared-modules-libsplatforminfraterraformmodules","title":"Shared Modules (<code>libs/platform/infra/terraform/modules/</code>)","text":"<p>Generic, reusable infrastructure components:</p> <pre><code>libs/platform/infra/terraform/modules/\n\u251c\u2500\u2500 resource-label/          # Consistent naming and tagging\n\u251c\u2500\u2500 terraform-s3-backend/    # Terraform state backend\n\u251c\u2500\u2500 vpc/                     # Network infrastructure (planned)\n\u251c\u2500\u2500 ecs-cluster/             # Container orchestration (planned)\n\u2514\u2500\u2500 rds-postgres/            # Database instances (planned)\n</code></pre> <p>Characteristics:</p> <ul> <li>Technology/product agnostic</li> <li>Parameterized inputs with sensible defaults</li> <li>Comprehensive validation</li> <li>Well-documented outputs</li> <li>Security best practices built-in</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#project-modules-appsproductinfraterraformprojectmodules","title":"Project Modules (<code>apps/&lt;product&gt;/infra/terraform/&lt;project&gt;/modules/</code>)","text":"<p>Product-specific infrastructure (planned):</p> <pre><code>apps/bixarena/infra/terraform/stack/modules/\n\u251c\u2500\u2500 api-service/             # BixArena-specific API configuration\n\u251c\u2500\u2500 worker-pool/             # Custom worker setup\n\u2514\u2500\u2500 cdn-distribution/        # Content delivery\n</code></pre> <p>Characteristics:</p> <ul> <li>Tailored to specific product requirements</li> <li>May combine multiple shared modules</li> <li>Business logic specific to the product</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#4-resource-naming-convention","title":"4. Resource Naming Convention","text":"<p>All resources follow a consistent naming pattern for discoverability and IAM scoping:</p> <pre><code>${organization}-${product}-${application}-${component}-${region}-${environment}\n</code></pre> <p>Example: <code>sage-bixarena-api-alb-use1-prod</code></p> <p>Components:</p> <ul> <li>organization: <code>sage</code> (from workspace.hcl)</li> <li>product: <code>bixarena</code> (from config.yaml or env var)</li> <li>application: <code>api</code> (from config.yaml)</li> <li>component: <code>alb</code> (from module input)</li> <li>region: <code>use1</code> (shortened from us-east-1)</li> <li>environment: <code>prod</code> (from config.yaml or env var)</li> </ul> <p>The resource-label module handles this naming automatically, including:</p> <ul> <li>Length truncation (S3 63-char limit, etc.)</li> <li>Region abbreviation</li> <li>Character filtering (alphanumeric + hyphens only)</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#5-tagging-strategy","title":"5. Tagging Strategy","text":"<p>Every resource receives standard tags:</p> <pre><code>tags = {\n  Organization = \"Sage Bionetworks\"\n  Product      = \"BixArena\"\n  Application  = \"API\"\n  Component    = \"LoadBalancer\"\n  Environment  = \"prod\"\n  ManagedBy    = \"terraform\"\n  CostCenter   = \"Research\"\n  Owner        = \"platform-team@sagebase.org\"\n}\n</code></pre> <p>These tags enable:</p> <ul> <li>Cost allocation and tracking</li> <li>Resource filtering and queries</li> <li>Compliance auditing</li> <li>Ownership identification</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#6-bootstrap-exception-pattern","title":"6. Bootstrap Exception Pattern","text":"<p>The Terraform backend module has a special requirement: it creates the S3 bucket and DynamoDB table used for remote state, so it cannot use remote state itself.</p> <p>Solution: Bootstrap modules use a local backend initially:</p> <pre><code># terraform-backend/terragrunt.hcl\nremote_state {\n  backend = \"local\"\n  config = {\n    path = \"${get_terragrunt_dir()}/terraform.tfstate\"\n  }\n}\n</code></pre> <p>After deployment, other modules reference the remote backend via <code>project.hcl</code>:</p> <pre><code># project.hcl (for all other modules)\nremote_state {\n  backend = \"s3\"\n  config = {\n    bucket         = \"sage-bixarena-terraform-backend-use1-prod\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"sage-bixarena-terraform-backend-lock-use1-prod\"\n  }\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#project-structure","title":"Project Structure","text":""},{"location":"develop/architecture/terraform-infrastructure/#typical-nx-infrastructure-project-layout","title":"Typical Nx Infrastructure Project Layout","text":"<pre><code>apps/bixarena/infra/terraform/terraform-backend/\n\u251c\u2500\u2500 project.json                      # Nx project configuration\n\u251c\u2500\u2500 project.hcl                       # Project-level Terragrunt configuration\n\u251c\u2500\u2500 config.yaml                       # Environment/module settings\n\u251c\u2500\u2500 README.md                         # Project documentation\n\u251c\u2500\u2500 .env.example                      # Environment variable template\n\u2514\u2500\u2500 terraform-backend/                # Module directory\n    \u2514\u2500\u2500 terragrunt.hcl                # Module configuration\n\nlibs/platform/infra/terraform/modules/\n\u251c\u2500\u2500 resource-label/                   # Shared: naming convention\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 versions.tf\n\u2502   \u2514\u2500\u2500 README.md\n\u2514\u2500\u2500 terraform-s3-backend/             # Shared: state backend\n    \u251c\u2500\u2500 main.tf\n    \u251c\u2500\u2500 variables.tf\n    \u251c\u2500\u2500 outputs.tf\n    \u251c\u2500\u2500 policies.tf\n    \u251c\u2500\u2500 context.tf                    # Embeds resource-label\n    \u251c\u2500\u2500 versions.tf\n    \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#configyaml-structure","title":"config.yaml Structure","text":"<pre><code># Product/Environment identification\nproduct: bixarena\napplication: infra\nenvironment: prod\n\n# Terraform backend (output from bootstrap)\nterraform_backend:\n  bucket_name: 'sage-bixarena-terraform-backend-use1-prod'\n  bucket_region: 'us-east-1'\n  dynamodb_table: 'sage-bixarena-terraform-backend-lock-use1-prod'\n\n# Module-specific configuration\nmodules:\n  terraform_backend:\n    aws_provider:\n      region: 'us-east-1'\n\n  vpc:\n    cidr_block: '10.0.0.0/16'\n    availability_zones: ['us-east-1a', 'us-east-1b', 'us-east-1c']\n\n  database:\n    instance_class: 'db.t3.medium'\n    allocated_storage: 100\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#module-dependencies","title":"Module Dependencies","text":"<p>Use Terragrunt <code>dependency</code> blocks to reference outputs from other modules:</p> <pre><code># apps/bixarena/infra/stack/database/terragrunt.hcl\ndependency \"vpc\" {\n  config_path = \"../network\"\n}\n\ninputs = {\n  vpc_id             = dependency.vpc.outputs.vpc_id\n  subnet_ids         = dependency.vpc.outputs.private_subnet_ids\n  security_group_ids = [dependency.vpc.outputs.database_security_group_id]\n}\n</code></pre> <p>Dependency Rules:</p> <ul> <li>Only depend on foundational/lower-layer modules</li> <li>Avoid circular dependencies</li> <li>Keep dependency chains shallow (max 3 levels)</li> <li>Use explicit outputs, not entire objects</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#state-management","title":"State Management","text":""},{"location":"develop/architecture/terraform-infrastructure/#state-file-organization","title":"State File Organization","text":"<p>Each module has its own isolated state file:</p> <pre><code>s3://sage-bixarena-terraform-backend-use1-prod/\n\u251c\u2500\u2500 terraform-backend/terraform.tfstate         # Bootstrap (local)\n\u251c\u2500\u2500 network/terraform.tfstate                   # VPC, subnets, security groups\n\u251c\u2500\u2500 database/terraform.tfstate                  # RDS instances\n\u251c\u2500\u2500 compute/terraform.tfstate                   # ECS/EC2 resources\n\u2514\u2500\u2500 monitoring/terraform.tfstate                # CloudWatch, alarms\n</code></pre> <p>Benefits:</p> <ul> <li>Isolated blast radius (changes don't affect other modules)</li> <li>Parallel development (different teams work on different modules)</li> <li>Faster planning (smaller state graphs)</li> <li>Easier troubleshooting</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#state-locking","title":"State Locking","text":"<p>DynamoDB provides distributed locking to prevent concurrent modifications:</p> <pre><code>+----------------+     Lock Request      +------------------+\n| Developer A    | -------------------&gt;  | DynamoDB Table   |\n| (terraform)    | &lt;------------------- |  (LockID: key)   |\n+----------------+   Lock Acquired       +------------------+\n                                                 |\n                                                 | Lock Denied\n                                                 v\n                                         +----------------+\n                                         | Developer B    |\n                                         | (waiting...)   |\n                                         +----------------+\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#security-best-practices","title":"Security Best Practices","text":""},{"location":"develop/architecture/terraform-infrastructure/#1-encryption","title":"1. Encryption","text":"<ul> <li>S3 State Storage: SSE-KMS encryption enforced</li> <li>DynamoDB Tables: Server-side encryption enabled</li> <li>TLS: All API calls require HTTPS</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#2-access-control","title":"2. Access Control","text":"<ul> <li>S3 Bucket Policies: Enforce encryption headers, deny unencrypted uploads</li> <li>IAM Roles: Least-privilege access for CI/CD pipelines</li> <li>State File: Contains sensitive data; never commit to version control</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#3-version-pinning","title":"3. Version Pinning","text":"<pre><code>terraform {\n  required_version = \"&gt;= 1.8.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"= 5.88.0\"  # Exact version for reproducibility\n    }\n  }\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#4-validation","title":"4. Validation","text":"<pre><code>variable \"s3_bucket_name\" {\n  type = string\n\n  validation {\n    condition     = length(var.s3_bucket_name) &lt; 64\n    error_message = \"S3 bucket name must be fewer than 64 characters.\"\n  }\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"develop/architecture/terraform-infrastructure/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<pre><code>- name: Terraform Plan\n  run: |\n    aws sso login --profile ${{ env.AWS_PROFILE }}\n    nx run ${{ env.PROJECT }}:plan:${{ env.ENVIRONMENT }}\n\n- name: Terraform Apply\n  if: github.ref == 'refs/heads/main'\n  run: |\n    nx run ${{ env.PROJECT }}:deploy:${{ env.ENVIRONMENT }}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#nx-project-configuration","title":"Nx Project Configuration","text":"<pre><code>{\n  \"targets\": {\n    \"init\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt init --working-dir &lt;module&gt;\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    },\n    \"plan\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt plan --working-dir &lt;module&gt;\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    },\n    \"deploy\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt apply --working-dir &lt;module&gt;\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#migration-from-cdk","title":"Migration from CDK","text":"<p>For teams migrating from AWS CDK, key differences:</p> Aspect CDK OpenTofu + Terragrunt Language TypeScript/Python HCL State CloudFormation S3 + DynamoDB Modularity Constructs Modules Configuration Code HCL + YAML Reusability Npm packages Local/remote modules <p>Migration Strategy:</p> <ol> <li>Deploy Terraform backend (bootstrap)</li> <li>Create shared modules for common patterns</li> <li>Migrate one stack at a time</li> <li>Import existing resources where possible</li> <li>Run CDK and OpenTofu/Terragrunt in parallel during transition</li> </ol>"},{"location":"develop/architecture/terraform-infrastructure/#examples","title":"Examples","text":""},{"location":"develop/architecture/terraform-infrastructure/#example-1-terraform-backend-project","title":"Example 1: Terraform Backend Project","text":"<p>The terraform-backend project demonstrates:</p> <ul> <li>Bootstrap exception pattern (local state)</li> <li>Reusable module usage</li> <li>Secure S3 and DynamoDB configuration</li> <li>Output values for downstream projects</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#example-2-reusable-module-terraform-s3-backend","title":"Example 2: Reusable Module (terraform-s3-backend)","text":"<p>The terraform-s3-backend module shows:</p> <ul> <li>Context pattern (resource-label integration)</li> <li>Conditional resource creation</li> <li>Security policies (encryption, TLS)</li> <li>Comprehensive outputs</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#common-patterns","title":"Common Patterns","text":""},{"location":"develop/architecture/terraform-infrastructure/#pattern-1-data-sources-for-existing-resources","title":"Pattern 1: Data Sources for Existing Resources","text":"<pre><code>data \"aws_vpc\" \"existing\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"sage-bixarena-vpc-prod\"]\n  }\n}\n\ninputs = {\n  vpc_id = data.aws_vpc.existing.id\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#pattern-2-conditional-resource-creation","title":"Pattern 2: Conditional Resource Creation","text":"<pre><code>resource \"aws_s3_bucket\" \"optional\" {\n  count = var.create_bucket ? 1 : 0\n\n  bucket = var.bucket_name\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#pattern-3-dynamic-blocks","title":"Pattern 3: Dynamic Blocks","text":"<pre><code>dynamic \"cors_rule\" {\n  for_each = var.cors_rules\n\n  content {\n    allowed_headers = cors_rule.value.allowed_headers\n    allowed_methods = cors_rule.value.allowed_methods\n    allowed_origins = cors_rule.value.allowed_origins\n  }\n}\n</code></pre>"},{"location":"develop/architecture/terraform-infrastructure/#troubleshooting","title":"Troubleshooting","text":""},{"location":"develop/architecture/terraform-infrastructure/#common-issues","title":"Common Issues","text":"<p>Issue: <code>Error: Backend configuration changed</code> Solution: Run <code>terragrunt init -reconfigure</code></p> <p>Issue: <code>Error acquiring the state lock</code> Solution: Another process is running. Wait or manually release lock in DynamoDB.</p> <p>Issue: <code>Error: Invalid provider configuration</code> Solution: Ensure AWS SSO session is active: <code>aws sso login --profile &lt;profile&gt;</code></p> <p>Issue: Module source not found Solution: Verify <code>project_paths</code> in workspace.hcl points to correct location</p>"},{"location":"develop/architecture/terraform-infrastructure/#related-documentation","title":"Related Documentation","text":"<ul> <li>OpenTofu Documentation</li> <li>Terragrunt Documentation</li> <li>AWS Provider Reference</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#example-projects-in-the-monorepo","title":"Example Projects in the Monorepo","text":"<ul> <li>Terraform Backend: <code>apps/bixarena/infra/terraform/terraform-backend/</code> - Example of bootstrap pattern</li> <li>Copilot Instructions: <code>.github/instructions/terraform.instructions.md</code> - Terraform conventions for AI assistants</li> </ul>"},{"location":"develop/architecture/terraform-infrastructure/#next-steps","title":"Next Steps","text":"<p>Ready to start working with infrastructure as code? Check out our tutorials:</p> <ul> <li>Create a Terraform Backend - Bootstrap your first Terraform project</li> <li>Create a Reusable Module - Build shared infrastructure modules</li> <li>Deploy Infrastructure (coming soon) - Deploy complete stacks to AWS</li> </ul>"},{"location":"develop/architecture/what-is-devcontainer/","title":"Dev Containers","text":"<p>Content comming soon!</p>"},{"location":"develop/architecture/what-is-nx/","title":"Nx Workspace","text":"<p>Content comming soon!</p>"},{"location":"develop/maintenance/","title":"Dependency &amp; Base Image Maintenance","text":"<p>This section documents how dependencies and Docker base images are maintained across the monorepo. It provides:</p> <ul> <li>Shared principles &amp; workflows</li> <li>Tooling and automation overview (CI validation)</li> <li>Per-ecosystem maintenance guides (Java, Node/TypeScript, Python, Docker, etc.)</li> </ul>"},{"location":"develop/maintenance/#scope","title":"Scope","text":"<p>Covers version management, update cadence, validation steps, and security considerations for:</p> <ul> <li>Application &amp; library runtime dependencies</li> <li>Build tooling (compilers, test frameworks, linters)</li> <li>Infrastructure/runtime images (Docker base images)</li> <li>Version catalogs and lock files</li> </ul>"},{"location":"develop/maintenance/#goals","title":"Goals","text":"<ul> <li>Ensure consistent, reproducible builds</li> <li>Minimize upgrade risk via automation + validation</li> <li>Surface and remediate security vulnerabilities quickly</li> <li>Provide contributors a clear, repeatable process</li> </ul>"},{"location":"develop/maintenance/#update-cadence-baseline","title":"Update Cadence (Baseline)","text":"Category Patch/Minor Major Security/Critical Java Libraries Weekly (batched) Quarterly or as needed ASAP (hotfix branch) Node/TypeScript Weekly (Renovate) Framework-aligned (e.g., Angular/TS) ASAP Python Bi-weekly or on-demand Semiannual ASAP Docker Base Images Weekly digest scan As runtime LTS shifts ASAP Dev Tooling (lint/test) Weekly With ecosystem majors ASAP <p>(Adjust these values if policy differs.)</p>"},{"location":"develop/maintenance/#roles-responsibilities","title":"Roles &amp; Responsibilities","text":"Actor Responsibility Automation (?) Proposes routine updates, groups safe changes Reviewer (Maintainer) Validates breaking changes, merges safe PRs Security Lead (optional) Prioritizes CVE remediation Release Manager Includes notable upgrades in release notes"},{"location":"develop/maintenance/#standard-workflow","title":"Standard Workflow","text":"<ol> <li>Automation or maintainer opens a branch / PR.</li> <li>CI runs: build, tests, lint, security scan (if configured).</li> <li>Reviewer validates:<ul> <li>No unexpected API breaks</li> <li>Performance or memory not degraded (spot check)</li> <li>Release notes for majors referenced</li> </ul> </li> <li>Merge with conventional commit message.</li> <li>Aggregate dependency changes into periodic changelog / release notes.</li> </ol>"},{"location":"develop/maintenance/#branch-commit-conventions","title":"Branch &amp; Commit Conventions","text":"<ul> <li>Branch: <code>chore/deps/&lt;ecosystem&gt;-&lt;yyyymmdd&gt;</code> or <code>security/deps/&lt;id&gt;</code></li> <li>Commit: <code>chore(deps): bump &lt;lib&gt; to &lt;version&gt;</code></li> <li>For majors with breaking changes: include a <code>BREAKING CHANGE:</code> footer summarizing migration notes.</li> </ul>"},{"location":"develop/maintenance/#risk-categorization","title":"Risk Categorization","text":"Risk Level Indicators Action Low Patch updates, clearly backward-compatible Auto-merge (if policy allows) Medium Minor updates with possible transitive shifts Manual review High Major version, peer dependency shifts, build tool changes Dedicated test focus, maybe staging deploy Critical Security vulnerability fix Expedite, limited scope branch"},{"location":"develop/maintenance/#validation-checklist-pre-merge","title":"Validation Checklist (Pre-Merge)","text":"<ul> <li> All relevant tests pass</li> <li> Build artifacts produced successfully</li> <li> No new lint/type errors</li> <li> Version catalog / lock file updated (if applicable)</li> <li> For majors: migration notes reviewed</li> <li> For Docker: image builds locally and starts successfully</li> </ul>"},{"location":"develop/maintenance/#ecosystem-guides","title":"Ecosystem Guides","text":"<ul> <li>Java Dependencies</li> <li>Node / TypeScript Dependencies</li> <li>Python Dependencies</li> <li>R Dependencies</li> <li>Docker Base Images</li> <li>Dev Container Updates</li> <li>Security &amp; Compliance</li> <li>Troubleshooting &amp; FAQ</li> </ul> <p>Continue to the Java guide to begin contributing detailed instructions.</p>"},{"location":"develop/maintenance/dev-container/","title":"Dev Container Updates","text":"<p>This guide provides comprehensive instructions for updating the development container used in the Sage Monorepo. The dev container provides a consistent development environment across all contributors and includes all necessary tools and dependencies.</p>"},{"location":"develop/maintenance/dev-container/#overview","title":"Overview","text":"<p>Updating the dev container is an intentionally two-step, two-PR workflow that separates image BUILD from image ACTIVATION. This decoupling lets us publish and test a new image across multiple branches before the whole team is switched over.</p>"},{"location":"develop/maintenance/dev-container/#terminology","title":"Terminology","text":"<ul> <li>Dockerfile (<code>.github/.devcontainer/Dockerfile</code>): Defines the base Ubuntu LTS image, pinned   build ARG versions, and any tools installed directly via shell/apt scripts.</li> <li>Build Definition (Intermediate) devcontainer.json (<code>.github/.devcontainer/devcontainer.json</code>):   Extends the Dockerfile with Dev Container Features (Docker-in-Docker, Java, Go, Kubernetes tooling, etc.).   This file is used only to BUILD &amp; PUBLISH the final composite image to GHCR. It is NOT what VS Code opens.</li> <li>Active devcontainer.json (<code>.devcontainer/devcontainer.json</code>): The definition actually consumed   by VS Code / <code>devcontainer up</code> when developers open the monorepo. It references an already-published   image tag (from GHCR) and should remain stable except when activating a new image.</li> <li>Published Image: <code>ghcr.io/sage-bionetworks/sage-monorepo-devcontainer:&lt;tag&gt;</code> produced by Step 1.</li> </ul>"},{"location":"develop/maintenance/dev-container/#two-step-workflow","title":"Two-Step Workflow","text":"Step PR Focus Touched Files Result Why Separate? 1 BUILD &amp; PUBLISH new image <code>.github/.devcontainer/Dockerfile</code>, <code>.github/.devcontainer/devcontainer.json</code> New image pushed to GHCR (tag = commit SHA or version) Allows iterative hardening &amp; branch testing without forcing all developers onto a possibly unstable image 2 ACTIVATE published image <code>.devcontainer/devcontainer.json</code> Team starts using the new image Minimal diff; easy rollback by reverting single tag change"},{"location":"develop/maintenance/dev-container/#rationale-for-separation","title":"Rationale for Separation","text":"<ol> <li>Safety \u2013 Risky base upgrades (Ubuntu date tag, Docker engine, language majors) are isolated from the day-to-day environment until validated.</li> <li>Parallel Testing \u2013 Multiple feature branches can temporarily reference the newly published image for smoke tests before activation.</li> <li>Fast Rollback \u2013 If problems surface after activation, reverting one commit (image tag change) restores the previous environment.</li> <li>Auditability \u2013 PR 1 documents image construction changes; PR 2 documents operational adoption.</li> </ol>"},{"location":"develop/maintenance/dev-container/#typical-flow","title":"Typical Flow","text":"<ol> <li> <p>Create branch <code>chore/devcontainer/build-&lt;date&gt;</code> and update:</p> <p>- Dockerfile (Ubuntu date tag + tool ARG bumps + removals)   - Build Definition devcontainer file (feature version bumps, add/remove Features)</p> </li> <li> <p>Open PR (BUILD). Opening and updating the PR triggers a build of the Docker images.</p> </li> <li>Merge PR (PUBLISH). Builds &amp; publishes image \u2192 GHCR tag.</li> <li>(Optional) Test in other branches by temporarily overriding the active file locally (do NOT commit those overrides) or using <code>devcontainer build/up/exec</code> commands referencing the new image.</li> <li>Once validated, create branch <code>chore/devcontainer/activate-&lt;tag&gt;</code> updating only <code>.devcontainer/devcontainer.json</code> to the new image tag.</li> <li>Open PR (ACTIVATE). After merge, developers rebuild / reopen in container and begin using the new environment.</li> </ol> <p>Note</p> <p>Keep any temporary local tag substitutions (e.g., <code>:local</code>) out of committed code. Only SHA or explicitly versioned tags should appear in the activated definition.</p>"},{"location":"develop/maintenance/dev-container/#summary","title":"Summary","text":"<p>You modify two different <code>devcontainer.json</code> files for two distinct purposes:</p> File Purpose When Edited <code>.github/.devcontainer/devcontainer.json</code> Compose &amp; bake the final published image together with Features Step 1 (BUILD) only <code>.devcontainer/devcontainer.json</code> Point developer workflows at an existing published image Step 2 (ACTIVATE) only <p>The remainder of this guide details how to update the Docker base image, tool versions, Features, test the resulting image, and safely activate it.</p>"},{"location":"develop/maintenance/dev-container/#key-files","title":"Key Files","text":"File / Path Role / Purpose Maintenance Notes <code>.github/.devcontainer/Dockerfile</code> Base layer (Ubuntu LTS date tag + apt + direct installs of some tools). Step 1 only. Pin ARG versions; prefer removing rather than leaving unused tooling. Keep image reproducible (no moving tags). <code>.github/.devcontainer/devcontainer.json</code> BUILD Definition: aggregates the Dockerfile + Dev Container Features to bake the final image that will be published to GHCR. Not used directly by developers in VS Code. Edit in Step 1 only. Use explicit feature versions. Treat as build recipe; never reference a production image tag here\u2014always <code>build</code> from the Dockerfile. <code>.devcontainer/devcontainer.json</code> ACTIVE Definition: points to the published image tag developers consume locally and in CI. Edit in Step 2 only. Single-line diff (image tag) ideally. Keep clean\u2014should not contain build-time Feature composition once image is published. <code>dev-env.sh</code> Workspace initialization (env vars, helper functions, wrappers). Review after major tool/runtime bumps. Sync deprecated tool names or paths. <code>tools/prepare-*-envs.js</code> Language ecosystem setup (Node, Python, Java, R, etc.). Validate they still succeed after base image &amp; feature updates (Node majors, Python minors, JDK changes). Update literals referencing versions/paths. <code>.github/workflows/*devcontainer*.yml</code>* CI workflows that build &amp; push the Step 1 image. Ensure cache key changes when ARGs change. Confirm push includes immutable tag (SHA) and optional semantic tag. <p>* Actual workflow filenames may vary; search for <code>devcontainer</code> in <code>.github/workflows</code> when adjusting build logic.</p> <p>Maintenance Guide Scope</p> <p>Only files directly influencing the container build, configuration, or activation workflow are listed. Application/service Dockerfiles are out of scope for this maintenance guide.</p>"},{"location":"develop/maintenance/dev-container/#step-1a-update-the-docker-base-image","title":"Step 1a: Update the Docker Base Image","text":"<p>Tip</p> <p>See the Update Checklist at the end of this document for an actionable summary of this section.</p>"},{"location":"develop/maintenance/dev-container/#base-image-update","title":"Base Image Update","text":"<p>The dev container uses Ubuntu as its base image. To update:</p> <pre><code># Current example:\nFROM ubuntu:noble-20250910\n\n# To update, change to newer tag with release date:\nFROM ubuntu:noble-20251015  # Use date-specific tag\n</code></pre> <p>Ubuntu Version Requirements &amp; Tagging:</p> <p>Use only Ubuntu LTS date-specific tags for reproducibility (e.g. <code>ubuntu:noble-20250910</code>). Avoid moving tags (<code>noble</code>, <code>latest</code>) and non\u2011LTS releases. Update periodically (e.g. monthly or when security fixes land). Reference: official LTS list at https://releases.ubuntu.com and available date tags at Docker Hub. Always test locally before committing and track EOL to plan major migrations.</p>"},{"location":"develop/maintenance/dev-container/#tool-version-updates","title":"Tool Version Updates","text":"<p>The Dockerfile includes numerous tools with pinned versions. Update the build arguments at the top of the file:</p> <pre><code># Example updates:\nARG hyperfineVersion=\"1.19.0\"        # Check: https://github.com/sharkdp/hyperfine\nARG devcontainerCliVersion=\"0.80.0\"  # Check: https://www.npmjs.com/package/@devcontainers/cli\nARG uvVersion=\"0.7.21\"               # Check: https://github.com/astral-sh/uv\nARG rVersion=\"4.5.1\"                 # Check: https://docs.posit.co/resources/install-r/\nARG trivyVersion=\"0.64.1\"            # Check: https://aquasecurity.github.io/trivy\nARG nodeVersionMajor=\"22\"            # Check: https://nodejs.org/en/about/previous-releases\nARG pnpmVersion=\"10.13.1\"            # Check: https://github.com/pnpm/pnpm/releases\n</code></pre> <p>Selection Criteria (condensed): Prefer LTS where applicable, skip anything &lt;24h old, avoid pre-releases, prefer versions with security fixes, and note breaking changes on major bumps.</p> <p>Process: Check release page \u2192 confirm age \u22651 day \u2192 confirm LTS (when relevant) \u2192 update ARG \u2192 build &amp; smoke test critical tooling.</p> <p>Node.js: Use even-numbered LTS majors (18/20/22). Avoid odd majors. Check the Node.js release schedule for current LTS + EOL.</p>"},{"location":"develop/maintenance/dev-container/#system-package-updates","title":"System Package Updates","text":"<p>System packages are installed via apt. While most use latest available versions, some may be pinned:</p> <pre><code>RUN apt-get update -qq -y &amp;&amp; export DEBIAN_FRONTEND=noninteractive \\\n  &amp;&amp; apt-get install --no-install-recommends -qq -y \\\n    ca-certificates curl git bash-completion gnupg2 lsb-release ssh sudo \\\n    python3-pip python3-dev python-is-python3 pipx openjdk-17-jdk \\\n    # ... other packages\n</code></pre> <p>Considerations:</p> <ul> <li>Most packages will automatically use latest versions from Ubuntu repositories</li> <li>Major Ubuntu version changes may affect package availability</li> <li>Test package installations if changing base Ubuntu version</li> </ul>"},{"location":"develop/maintenance/dev-container/#tool-management-and-removal","title":"Tool Management and Removal","text":"<p>To maintain security and reduce the attack surface, the dev container tool set should be periodically reviewed and pruned.</p> <p>Tool Removal Guidelines:</p> <ul> <li>Security First: Remove rarely used tools to reduce potential attack vectors</li> <li>Usage Analysis: Regularly assess which tools are actively used by the development team</li> <li>Team Consensus: Tool removal requires consensus among team members before implementation</li> <li>Impact Assessment: Evaluate potential workflow disruption before removing any tool</li> </ul> <p>Review Process:</p> <ol> <li>Quarterly Reviews: Conduct tool usage reviews every 3-4 months</li> <li>Usage Tracking: Monitor which tools are being used in CI/CD pipelines and development workflows (future work)</li> <li>Team Survey: Poll team members about tool usage and necessity</li> <li>Deprecation Notice: Provide advance notice before removing tools to allow workflow adjustments</li> <li>Documentation: Update documentation when tools are removed</li> </ol> <p>Tools to Monitor for Removal:</p> <ul> <li>Development utilities with overlapping functionality</li> <li>Language-specific tools for unused programming languages</li> <li>CLI tools that haven't been used in recent projects</li> <li>Legacy tools that have modern replacements</li> </ul> <p>Before Removing a Tool:</p> <ul> <li> Confirm tool is not used in any CI/CD pipelines</li> <li> Check if tool is referenced in documentation</li> <li> Verify no active projects depend on the tool</li> <li> Get explicit approval from team leads</li> <li> Plan migration path if tool replacement is needed</li> </ul>"},{"location":"develop/maintenance/dev-container/#step-1b-update-dev-container-features","title":"Step 1b: Update Dev Container Features","text":"<p>Tip</p> <p>See the Update Checklist at the end of this document for an actionable summary of this section.</p> <p>After updating the Dockerfile (Step 1a), update <code>.github/.devcontainer/devcontainer.json</code> (feature set &amp; runtime configuration). This remains part of Step 1 (BUILD &amp; PUBLISH).</p>"},{"location":"develop/maintenance/dev-container/#feature-set","title":"Feature Set","text":"<p>The <code>devcontainer.json</code> file references the updated Dockerfile and uses pre-built features from the Dev Containers specification:</p> <pre><code>\"features\": {\n  \"ghcr.io/devcontainers/features/docker-in-docker:2.12.2\": {\n    \"version\": \"28.3.2\",\n    \"installDockerComposeSwitch\": false\n  },\n  \"ghcr.io/devcontainers/features/java:1.6.3\": {\n    \"version\": \"21.0.7-ms\",\n    \"additionalVersions\": [\"21.0.7-graal\"]\n  },\n  \"ghcr.io/devcontainers/features/go:1.3.2\": {\n    \"version\": \"1.24.5\",\n    \"golangciLintVersion\": \"2.2.2\"\n  },\n  \"ghcr.io/devcontainers/features/kubectl-helm-minikube:1.2.2\": {\n    \"version\": \"1.33.3\",\n    \"helm\": \"3.17.0\",\n    \"minikube\": \"1.36.0\"\n  }\n}\n</code></pre> <p>Selection &amp; Trust (condensed): Prefer official sources (<code>devcontainers</code>, <code>microsoft</code>, <code>docker</code>, major vendors). For community features: \u226550 stars, active in last 3 months, multi-contributor, semantic releases, documented, responsive issues, passes source/security skim. Always pin immutable version tags.</p> <p>Mini Process: Look up latest \u2192 validate trust \u2192 pin explicit version \u2192 ensure cross-feature compatibility \u2192 test build.</p>"},{"location":"develop/maintenance/dev-container/#community-feature-evaluation-checklist","title":"Community Feature Evaluation Checklist","text":"<p>When considering features from smaller communities, use this evaluation checklist:</p> <p>Tip</p> <p>The checkboxes below are clickable\u2014check them as you evaluate a community feature for inclusion in the dev container to track your review progress.</p> <p>Repository Assessment:</p> <ul> <li> GitHub repository has 50+ stars</li> <li> Repository is actively maintained (commits within last 3 months)</li> <li> Issues are actively triaged and responded to</li> <li> Multiple contributors (not single-person projects)</li> <li> Clear contribution guidelines and code of conduct</li> </ul> <p>Release Management:</p> <ul> <li> Regular release cadence (at least quarterly)</li> <li> Semantic versioning is used</li> <li> Release notes document changes clearly</li> <li> No breaking changes without major version bumps</li> </ul> <p>Code Quality:</p> <ul> <li> Comprehensive README with usage examples</li> <li> Automated testing (CI/CD pipelines)</li> <li> Security scanning in place</li> <li> Code follows security best practices</li> <li> Dependencies are up-to-date and secure</li> </ul> <p>Community Health:</p> <ul> <li> Active maintainer response to issues</li> <li> Community discussions and engagement</li> <li> Clear project roadmap or vision</li> <li> Responsive to security vulnerability reports</li> </ul> <p>Feature-Specific Review:</p> <ul> <li> Source code review for security vulnerabilities</li> <li> Feature functionality aligns with monorepo needs</li> <li> No unnecessary privileges or access requirements</li> <li> Compatible with existing features and tools</li> <li> Performance impact is acceptable</li> </ul> <p>Approval Process:</p> <ol> <li>Complete evaluation checklist</li> <li>Document findings and recommendation</li> <li>Get approval from security team (if applicable)</li> <li>Test feature in isolated environment</li> <li>Get team consensus before adding to production</li> </ol>"},{"location":"develop/maintenance/dev-container/#feature-specific-notes","title":"Feature-Specific Notes","text":""},{"location":"develop/maintenance/dev-container/#docker-in-docker","title":"Docker-in-Docker","text":"<ul> <li>Update Docker version to latest stable</li> </ul>"},{"location":"develop/maintenance/dev-container/#java","title":"Java","text":"<ul> <li>Coordinate with project Java version requirements</li> <li>Update both primary and additional JDK versions</li> <li>Ensure compatibility with Gradle configurations</li> </ul>"},{"location":"develop/maintenance/dev-container/#go","title":"Go","text":"<ul> <li>Update Go version for latest language features</li> <li>Update golangci-lint for latest linting rules</li> </ul>"},{"location":"develop/maintenance/dev-container/#kubernetes-tools","title":"Kubernetes Tools","text":"<ul> <li>Update kubectl to match target Kubernetes versions</li> <li>Update Helm for latest chart compatibility</li> <li>Update Minikube for local development</li> </ul>"},{"location":"develop/maintenance/dev-container/#step-1c-test-the-dev-container-locally","title":"Step 1c: Test the Dev Container Locally","text":"<p>Tip</p> <p>See the Update Checklist at the end of this document for an actionable summary of this section.</p>"},{"location":"develop/maintenance/dev-container/#manual-dev-container-testing","title":"Manual Dev Container Testing","text":"<p>After updating both the Dockerfile and <code>devcontainer.json</code>, perform comprehensive testing using the devcontainer CLI.</p>"},{"location":"develop/maintenance/dev-container/#build-the-dev-container-image","title":"Build the Dev Container Image","text":"<p>The dev container includes the devcontainer CLI, so you can build the new container from within an existing dev container.</p> <p>From the root workspace directory:</p> <pre><code>devcontainer build \\\n  --workspace-folder .github \\\n  --image-name ghcr.io/sage-bionetworks/sage-monorepo-devcontainer:local\n</code></pre> <p>Build Troubleshooting:</p> <ul> <li>Docker Version Issues: If build fails, the Docker engine version in <code>devcontainer.json</code> might be too recent<pre><code>- Try specifying an older Docker version in the `docker-in-docker` feature\n- Example: Change from `\"version\": \"28.3.2\"` to `\"version\": \"27.3.2\"` or earlier\n- Check [Docker releases](https://docs.docker.com/engine/release-notes/) for stable versions\n</code></pre> </li> </ul>"},{"location":"develop/maintenance/dev-container/#test-container-startup-and-functionality","title":"Test Container Startup and Functionality","text":"<ol> <li> <p>Start the dev container:     <pre><code>devcontainer up --workspace-folder .github\n</code></pre></p> </li> <li> <p>Connect to the running container:     <pre><code>docker exec -it sage-monorepo-devcontainer-prebuild bash\n</code></pre></p> </li> <li> <p>Verify tool installations, especially new and updates tools:     <pre><code># Test key tools are available and working\ndocker --version\ngo version\njava --version\nkubectl version --client\nnode --version\npython --version\ntrivy --version\nuv --version\n</code></pre></p> </li> <li> <p>Exit the container:     <pre><code># Use Ctrl+C to exit the container session or enter:\nexit\n</code></pre></p> </li> <li> <p>Clean up the test container:     <pre><code>docker rm -f sage-monorepo-devcontainer-prebuild\n</code></pre></p> </li> </ol>"},{"location":"develop/maintenance/dev-container/#headless-monorepo-runtime-testing","title":"Headless Monorepo Runtime Testing","text":"<p>After basic functionality testing, run a comprehensive, CLI\u2011driven test pass with the entire monorepo mounted into the container using the devcontainer CLI without opening the workspace in VS Code (i.e. VS Code is not attached to this container yet). This validates the baked image + features + toolchain against real workspace code (builds, tests, scripts) but does not exercise:</p> <ul> <li>VS Code extension set / settings sync</li> <li>Editor\u2011integrated language servers started via the active <code>devcontainer.json</code></li> <li>Any VS Code tasks, launch configs, or UI workflows</li> </ul> <p>For those editor integration aspects, see the later section IDE &amp; CI Integration Testing which performs an actual VS Code rebuild using the active dev container definition.</p> <p>Why 'Headless'?</p> <p>\"Headless\" here means the container is running and the repo is mounted, but VS Code has not been reopened against it, so no editor extensions, language servers, tasks, or settings from the active devcontainer definition have initialized. It\u2019s a fast runtime/tooling validation step before paying the cost of a full VS Code rebuild &amp; attachment.</p>"},{"location":"develop/maintenance/dev-container/#verify-local-image-availability","title":"Verify Local Image Availability","text":"<p>Confirm the dev container image was built successfully:</p> <pre><code>docker images | grep devcontainer\n# Expected output:\n# ghcr.io/sage-bionetworks/sage-monorepo-devcontainer   local   &lt;image-id&gt;   &lt;time&gt;   ~5GB\n</code></pre>"},{"location":"develop/maintenance/dev-container/#update-dev-container-definition","title":"Update Dev Container Definition","text":"<p>Update the main dev container configuration file (<code>.devcontainer/devcontainer.json</code>) to reference the new local image if needed for testing.</p> <p>Example diff (do NOT commit this change; for local testing only):</p> <pre><code> // .devcontainer/devcontainer.json\n {\n    \"name\": \"Sage Monorepo Dev Container\",\n-  \"image\": \"ghcr.io/sage-bionetworks/sage-monorepo-devcontainer:sha-6269b9b3d863831c296f843edd84b2c7e1d4733d\",\n+  \"image\": \"ghcr.io/sage-bionetworks/sage-monorepo-devcontainer:local\",\n    // ... rest of configuration\n }\n</code></pre> <p>After testing, revert the image line back to the published SHA tag before opening or updating any PRs.</p>"},{"location":"develop/maintenance/dev-container/#deploy-with-monorepo-codebase","title":"Deploy with Monorepo Codebase","text":"<p>Instead of manually stepping into the test container, you can run the comprehensive test from your current dev container:</p> <p>Workspace Side Effects</p> <p>The steps below mount your current monorepo working copy into a throwaway dev container. Any tasks you run (e.g. <code>workspace-install</code>, Nx builds, tests) will mutate files on your host, not inside an isolated scratch layer. Typical side effects include updates to:</p> <ul> <li><code>node_modules/</code></li> <li>Nx / build caches (e.g. <code>.nx/cache</code>)</li> <li>Generated artifacts (dist / build folders)</li> <li>Potential lockfile adjustments if dependency resolution differs</li> </ul> <p>These changes persist after you remove the test container. To avoid unexpected diffs or performance regressions when you return to other feature work, choose one of the following strategies:</p> <p>Safer Approaches</p> <p>1. Use a temporary clone or worktree:     - <code>git worktree add ../sage-devcontainer-test &lt;branch&gt;</code>     - Run all container tests from that directory     - Remove when done: <code>git worktree remove ../sage-devcontainer-test</code>   2. Copy the repo for testing (choose one):      - Tar stream (preserves permissions, excludes .git):        <pre><code>mkdir -p ../sage-devcontainer-test-copy\ntar --exclude .git -cf - . | tar -C ../sage-devcontainer-test-copy -xf -\n</code></pre>      - Simple cp fallback (copies .git then removes it):        <pre><code>cp -a . ../sage-devcontainer-test-copy\nrm -rf ../sage-devcontainer-test-copy/.git\n</code></pre>   3. If testing in-place, plan cleanup:     - Remove build caches: <code>rm -rf .nx/cache</code> (and any <code>dist/</code> or <code>build/</code> dirs)     - Recreate dependencies fresh: <code>rm -rf node_modules &amp;&amp; workspace-install</code>     - Discard unintended changes: <code>git restore .</code> / <code>git clean -fdX</code> (review before running)</p> <p>If you notice unexplained build speed regressions after testing, clear caches as above before investigating further.</p> <ol> <li> <p>Start dev container with monorepo:</p> <pre><code># From the root of the monorepo\ndevcontainer up --workspace-folder .\n</code></pre> </li> <li> <p>Execute comprehensive test directly:</p> <pre><code>devcontainer exec --workspace-folder . bash -c \". ./dev-env.sh \\\n  &amp;&amp; workspace-install \\\n  &amp;&amp; nx run-many --target=create-config,build,test --skip-nx-cache\"\n</code></pre> </li> <li> <p>Clean up test container:</p> <pre><code># After the command completes, remove the test container\ndocker rm -f sage-monorepo-devcontainer\n</code></pre> </li> </ol> <p>Benefits of this approach:</p> <ul> <li>Streamlined: No need to manually connect and navigate inside the test container</li> <li>Automated: Single command runs the entire test suite</li> <li>Clean: Remains in your active development environment throughout</li> <li>Efficient: Uses <code>--skip-nx-cache</code> to ensure fresh builds and tests</li> </ul>"},{"location":"develop/maintenance/dev-container/#ide-ci-integration-testing","title":"IDE &amp; CI Integration Testing","text":"<ol> <li> <p>Test in VS Code:</p> <p>- Open the project in VS Code   - Select \"Rebuild and Reopen in Container\"   - Verify all extensions load correctly   - Test basic development workflows</p> </li> <li> <p>Test CI/CD compatibility:</p> <p>- Ensure GitHub Actions continue to work   - Verify any external integrations still function</p> </li> </ol>"},{"location":"develop/maintenance/dev-container/#step-1d-build-publish-image-pr-1","title":"Step 1d: Build &amp; Publish Image (PR 1)","text":"<p>The checklist later includes an abbreviated version of these PR 1 steps.</p>"},{"location":"develop/maintenance/dev-container/#creating-the-build-pr","title":"Creating the Build PR","text":"<p>After successful local testing:</p> <ol> <li> <p>Revert local testing changes:</p> <p>- Critical: Revert any image tag changes in <code>.devcontainer/devcontainer.json</code> made for local testing   - Ensure the file references the current production image tag, not \"local\" or test configurations   - Example: Change from <code>\"image\": \"ghcr.io/sage-bionetworks/sage-monorepo-devcontainer:local\"</code> back to the current SHA-based tag</p> </li> <li> <p>Create feature branch PR:</p> <p>- Create a pull request with all dev container changes   - Include comprehensive description of updates made   - Document any breaking changes or new requirements   - Verify <code>.devcontainer/devcontainer.json</code> uses production image reference</p> </li> <li> <p>PR triggers automated build:</p> <p>- GitHub workflow will build the new dev container image   - Automated tests will validate the container functionality   - Review any build failures or test issues</p> </li> </ol>"},{"location":"develop/maintenance/dev-container/#publishing-the-new-image","title":"Publishing the New Image","text":"<p>Upon merging the PR to the main branch:</p> <ol> <li> <p>Automated image build and publish:</p> <p>- Docker image is built automatically via GitHub Actions   - Image is published to GitHub Container Registry (GHCR)   - New image receives a unique tag based on commit SHA</p> </li> <li> <p>Image availability:</p> <p>- Published image becomes available at <code>ghcr.io/sage-bionetworks/sage-monorepo-devcontainer:&lt;tag&gt;</code>   - Tag format typically includes commit SHA or version identifier</p> </li> </ol>"},{"location":"develop/maintenance/dev-container/#step-2-activate-published-image-pr-2","title":"Step 2: Activate Published Image (PR 2)","text":"<p>Tip</p> <p>See the Update Checklist at the end of this document for an actionable summary of this section.</p>"},{"location":"develop/maintenance/dev-container/#activation-overview","title":"Activation Overview","text":"<p>Activation PR</p> <p>Important: Using the new dev container requires a second PR. This activation PR is what actually enables the new image for all contributors. Until it merges, only users who temporarily point their local <code>.devcontainer/devcontainer.json</code> at the new image (or use a local <code>:local</code> tag) will experience the changes.</p> <p>After the activation PR is merged into <code>main</code>:</p> <ul> <li>When contributors pull the updated <code>main</code> branch, VS Code detects the changed dev container   image reference and pops a notification prompting them to \"Rebuild\" / \"Reopen in Container\".</li> <li>A reminder line is also printed in their terminal the next time the workspace starts inside   the prior container, advising a rebuild to pick up the new environment.</li> <li>Rebuilding pulls the published GHCR image directly (does not re-run the full Docker build   locally unless the layer cache is cold).</li> </ul> <p>If a developer dismisses the prompt, they can manually trigger it later via the command palette (\"Dev Containers: Rebuild and Reopen in Container\"). They remain on the previous environment until they rebuild, which can explain version skew in support questions.</p> <ol> <li> <p>Update image reference:</p> <p>- Create a new PR updating <code>.devcontainer/devcontainer.json</code>   - Change the image tag to reference the newly published image   - Example: Update from previous tag to new GHCR tag</p> </li> <li> <p>Two-step deployment rationale:</p> <p>- Safety: Separates image building from activation   - Validation: Allows testing of published image before activation   - Rollback: Easy to revert to previous working image if issues arise</p> </li> <li> <p>Activation process: <pre><code>// In .devcontainer/devcontainer.json\n{\n  \"name\": \"Sage Monorepo Dev Container\",\n  \"image\": \"ghcr.io/sage-bionetworks/sage-monorepo-devcontainer:&lt;new-tag&gt;\",\n  // ... rest of configuration\n}\n</code></pre></p> </li> </ol>"},{"location":"develop/maintenance/dev-container/#post-deployment-verification","title":"Post-Deployment Verification","text":"<p>After the activation PR is merged:</p> <ol> <li> <p>Team notification:</p> <p>- (Auto) Notify team members of the new dev container availability   - Provide any migration notes or new requirements   - Document any workflow changes</p> </li> <li> <p>Monitor for issues:</p> <p>- Watch for team reports of container problems   - Check CI/CD pipeline functionality   - Be prepared to quickly rollback if critical issues arise</p> </li> </ol>"},{"location":"develop/maintenance/dev-container/#update-checklist","title":"Update Checklist","text":"<p>Interactive Checklist</p> <p>The checkboxes below are clickable in rendered docs. Use them to track your progress while performing an update; you can clear them to rehearse or review the process later. Their state lives only in your browser and is not persisted or shared. Opening the page in another browser or device, or after clearing site data, resets all boxes.</p>"},{"location":"develop/maintenance/dev-container/#pre-update","title":"Pre-Update","text":"<ul> <li> Document current versions for rollback reference</li> <li> Check changelogs for breaking changes in tools being updated</li> <li> Coordinate with team on timing of updates</li> <li> Review tool usage and identify candidates for removal</li> <li> Get team consensus on any tool removals</li> </ul>"},{"location":"develop/maintenance/dev-container/#during-update","title":"During Update","text":"<ul> <li> Update base Ubuntu image tag (LTS with date-specific tag only)</li> <li> Verify all tool releases are at least 1 day old</li> <li> Ensure Node.js uses LTS version (even-numbered major versions)</li> <li> Update all tool versions in Dockerfile build args to latest stable releases</li> <li> Update dev container features using non-moving version tags only</li> <li> Verify all features are from trusted organizations or approved community sources</li> <li> Complete community feature evaluation for any new features</li> <li> Update any hardcoded version references in scripts</li> <li> Remove approved unused tools to reduce attack surface</li> </ul>"},{"location":"develop/maintenance/dev-container/#testing","title":"Testing","text":""},{"location":"develop/maintenance/dev-container/#basic-functionality-testing","title":"Basic Functionality Testing","text":"<ul> <li> Build prebuild image</li> <li> If build fails, consult \"Build Troubleshooting\" (adjust Docker version if needed)</li> <li> Start container</li> <li> Attach shell</li> <li> Spot\u2011check core tools (all versions print successfully)</li> <li> Exercise representative workflows (package install, Gradle tasks, Docker, kubectl) \u2014 no errors</li> <li> Remove prebuild container</li> </ul>"},{"location":"develop/maintenance/dev-container/#headless-monorepo-runtime-testing_1","title":"Headless Monorepo Runtime Testing","text":"<p>Headless = dev container started via CLI (devcontainer up / exec) with the monorepo mounted, but VS Code not reopened inside that container yet. Editor extensions/settings activation is validated later under \"VS Code Integration Testing\".</p> <p>Option A: Manual Testing (step-by-step)</p> <ul> <li> (Optional) Temporarily point <code>.devcontainer/devcontainer.json</code> to <code>:local</code> for this session \u2014 do NOT commit</li> <li> (If using interactive flow) Attach a shell to the running container</li> <li> Initialize environment &amp; install dependencies (source <code>dev-env.sh</code>, then install; see prior section for details)</li> <li> Run full create-config / build / test targets (refer to combined Nx targets described earlier)</li> <li> Verify all projects succeed (review output; no failures reported)</li> <li> Remove the throwaway test container</li> </ul> <p>Option B: Streamlined Testing (automated)</p> <ul> <li> Confirm local image built</li> <li> Run the single exec-based end\u2011to\u2011end test flow</li> <li> Review output for failures (all targets should pass)</li> <li> Remove test container</li> </ul>"},{"location":"develop/maintenance/dev-container/#vs-code-integration-testing","title":"VS Code Integration Testing","text":"<ul> <li> Test in VS Code dev container environment (\"Rebuild and Reopen in Container\")</li> <li> Verify all extensions load correctly and development workflows function</li> </ul>"},{"location":"develop/maintenance/dev-container/#post-update","title":"Post-Update","text":""},{"location":"develop/maintenance/dev-container/#first-pr-dev-container-changes","title":"First PR - Dev Container Changes","text":"<ul> <li> Revert local testing changes: Ensure <code>.devcontainer/devcontainer.json</code> uses production image tag, not \"local\"</li> <li> Update documentation if new tools added/removed</li> <li> Create PR with clear description of changes (Dockerfile, devcontainer.json, etc.)</li> <li> Verify PR includes only production-ready configurations</li> <li> Ensure PR includes comprehensive changelog of all updates</li> <li> Monitor GitHub Actions build of new dev container image</li> <li> Verify successful image publication to GHCR after merge</li> </ul>"},{"location":"develop/maintenance/dev-container/#second-pr-activate-new-container","title":"Second PR - Activate New Container","text":"<ul> <li> Create second PR updating <code>.devcontainer/devcontainer.json</code> image reference</li> <li> Update image tag to newly published GHCR image</li> <li> Test the published image before merging activation PR</li> <li> Monitor CI/CD pipelines after activation merge</li> <li> Notify team of new dev container availability and any workflow changes</li> </ul>"},{"location":"develop/maintenance/dev-container/#troubleshooting","title":"Troubleshooting","text":""},{"location":"develop/maintenance/dev-container/#common-issues","title":"Common Issues","text":""},{"location":"develop/maintenance/dev-container/#build-failures","title":"Build Failures","text":"<ul> <li>Package not found: Check if package names changed in new Ubuntu version</li> <li>Version conflicts: Ensure all pinned versions are still available</li> <li>Architecture issues: Verify all tools support the target architecture</li> </ul>"},{"location":"develop/maintenance/dev-container/#dev-container-build-failures","title":"Dev Container Build Failures","text":"Issue Description Solution Additional Checks / Notes Docker version compatibility Latest Docker versions in <code>docker-in-docker</code> feature may cause build failures Specify an older, stable Docker version (e.g. <code>\"version\": \"27.3.2\"</code> instead of <code>\"28.3.2\"</code>) Check logs for Docker daemon startup errors or compatibility issues Feature conflicts Multiple features trying to install the same tools Remove duplicate installations or use feature-specific configurations Review feature documentation for known conflicts Memory / disk space Large feature installations may exceed available resources Increase Docker Desktop memory/disk limits or use smaller base images Monitor Docker system resource usage during build Network issues Features downloading from external sources may fail Retry build or check network connectivity Use cached or mirror sources when available Permission errors Features may fail to install due to user permission issues Ensure proper user configuration in <code>devcontainer.json</code> Verify the <code>remoteUser</code> setting matches the Dockerfile user"},{"location":"develop/maintenance/dev-container/#dev-container-runtime-issues","title":"Dev Container Runtime Issues","text":"Issue Description Solution Additional Checks / Notes Container won't start <code>devcontainer up</code> fails to start container Verify all required ports are available and not in use Ensure Docker daemon is running and accessible Tools not available Installed tools not found in PATH Check if feature installation completed successfully Connect to container and manually verify tool locations VS Code integration fails Container starts but VS Code can't connect Rebuild container with \"Rebuild and Reopen in Container\" Verify VS Code Dev Containers extension is updated"},{"location":"develop/maintenance/dev-container/#runtime-issues","title":"Runtime Issues","text":"<ul> <li>Tool missing: Check if installation commands succeeded</li> <li>Permission errors: Verify user permissions and sudo access</li> <li>Path issues: Ensure all tools are in PATH correctly</li> </ul>"},{"location":"develop/maintenance/dev-container/#performance-issues","title":"Performance Issues","text":"<ul> <li>Slow builds: Consider multi-stage builds or caching strategies</li> <li>Large image size: Remove unused packages and clean up in same RUN command</li> <li>Memory usage: Monitor resource consumption of new tools</li> </ul>"},{"location":"develop/maintenance/dev-container/#recovery-process","title":"Recovery Process","text":"<p>If issues arise after an update:</p> <ol> <li>Immediate rollback: Revert to previous Dockerfile/devcontainer.json</li> <li>Isolate the issue: Test changes incrementally</li> <li>Check compatibility: Verify tool versions work together</li> <li>Consult documentation: Review tool-specific upgrade guides</li> </ol>"},{"location":"develop/maintenance/dev-container/#security-considerations","title":"Security Considerations","text":""},{"location":"develop/maintenance/dev-container/#image-security","title":"Image Security","text":"<ul> <li>Regularly scan base images for vulnerabilities</li> <li>Use minimal base images when possible</li> <li>Keep system packages updated</li> </ul>"},{"location":"develop/maintenance/dev-container/#tool-security","title":"Tool Security","text":"<ul> <li>Verify checksums/signatures when downloading tools</li> <li>Use official distribution sources</li> <li>Monitor security advisories for installed tools</li> </ul>"},{"location":"develop/maintenance/dev-container/#access-security","title":"Access Security","text":"<ul> <li>Limit container privileges</li> <li>Use non-root user when possible</li> <li>Secure secrets and credentials</li> </ul>"},{"location":"develop/maintenance/dev-container/#automation-opportunities","title":"Automation Opportunities","text":"<p>Consider automating parts of the update process:</p> <ul> <li>Dependabot: For automatic dependency updates</li> <li>Scheduled builds: Regular container rebuilds with latest packages</li> <li>Security scanning: Automated vulnerability scanning</li> <li>Version checking: Scripts to check for tool updates</li> </ul>"},{"location":"develop/maintenance/dev-container/#resources","title":"Resources","text":"<ul> <li>Dev Containers Specification</li> <li>Dev Container Features</li> <li>Ubuntu Docker Images</li> <li>VS Code Dev Containers Documentation</li> </ul> <p>For questions or issues with dev container updates, please create an issue in the repository or reach out to the maintenance team.</p>"},{"location":"develop/maintenance/docker-images/","title":"Docker Base Image Maintenance","text":"<p>(Placeholder \u2013 to be populated later.)</p>"},{"location":"develop/maintenance/java/","title":"Java Build &amp; Dependency Maintenance","text":"<p>This guide covers governance of Java dependencies and maintenance of the Java build toolchain (Gradle wrapper and JDK) for the monorepo. It emphasizes a centralized version catalog for consistency plus clearly isolated procedures for upgrading Gradle and the JDK.</p> <p>The dependency model is described below; build tool &amp; JDK upgrade procedures live at the end under \"Build Tool &amp; JDK Upgrades\".</p>"},{"location":"develop/maintenance/java/#dependency-model","title":"Dependency Model","text":"<p>Java dependencies are managed primarily via the Gradle Version Catalog located at <code>gradle/libs.versions.toml</code>. Individual <code>build.gradle.kts</code> files reference aliases defined there. Centralizing versions:</p> <ul> <li>Ensures alignment across services and libraries</li> <li>Simplifies upgrades and security remediation</li> <li>Reduces dependency drift</li> </ul>"},{"location":"develop/maintenance/java/#key-files","title":"Key Files","text":"File / Path Role / Purpose Maintenance Notes <code>gradle/libs.versions.toml</code> Central Gradle Version Catalog (aliases, bundles, plugin &amp; lib versions) Primary source of truth for versions <code>settings.gradle.kts</code> Declares included projects &amp; (optionally) plugin management Keep aligned with catalog usage <code>build.gradle.kts</code> (root) Common repositories, shared plugin/application of conventions Avoid hard\u2011coded versions; delegate to catalog <code>apps/*/build.gradle.kts</code>, <code>libs/*/build.gradle.kts</code> Module build scripts consuming catalog aliases Should not redefine versions directly <code>gradle.properties</code> JVM &amp; Gradle configuration (memory, flags, toolchain hints) Version bumps rarely here <code>buildSrc/</code> Local convention plugins (Java, Spring Boot, Lombok, Jacoco, publishing) Contains a few hard\u2011coded versions (JUnit, Lombok, Jacoco) that must be manually synced <p>buildSrc maintenance</p> <p>Individual plugin scripts inside <code>buildSrc</code> are intentionally not listed here; treat the directory as a single maintenance surface. During updates, grep for literal versions inside <code>buildSrc</code> and align them with the catalog.</p>"},{"location":"develop/maintenance/java/#version-catalog-conventions","title":"Version Catalog Conventions","text":"<p>The goal of the catalog is to provide a single, predictable naming scheme that makes it obvious which family a dependency belongs to and how versions are shared. Consistent aliasing reduces diffs, improves discoverability, and prevents accidental version divergence.</p>"},{"location":"develop/maintenance/java/#general-rules","title":"General Rules","text":"<ol> <li>Derive library family aliases from the dependency <code>group</code> (not the full <code>group:artifact</code>) when multiple artifacts share a version (e.g., <code>com.fasterxml.jackson.core</code> \u2192 <code>jackson</code>).</li> <li>Use kebab-case for all aliases: <code>spring-boot</code>, <code>spring-cloud</code>, <code>jackson</code>, <code>logback</code>, <code>micrometer</code>, <code>junit</code>, <code>lombok</code>.</li> <li>Use a single version key per family in <code>[versions]</code>; individual artifacts reference it via <code>version.ref</code>.</li> <li>Do not encode version numbers in alias names (avoid <code>jackson-2-18</code>).</li> <li>Use suffixes only when they add semantic clarity:<ul> <li><code>-bom</code> for Bill of Materials / platform entries (e.g., <code>spring-boot-bom</code>).</li> <li><code>-plugin</code> for Gradle plugin entries where an ID could collide (<code>spotless-plugin</code>).</li> </ul> </li> <li>Keep aliases stable; renaming causes repo\u2011wide churn. Only rename to correct a clear inconsistency.</li> <li>Prefer short, unambiguous roots. If a group has a long prefix chain (<code>io.github.resilience4j</code>), choose <code>resilience4j</code>.</li> <li>For a single artifact with no family context, derive alias from the artifact, trimming redundant prefixes (e.g., <code>micrometer-registry-prometheus</code> \u2192 <code>micrometer-prometheus</code>).</li> </ol>"},{"location":"develop/maintenance/java/#recommended-catalog-structure-excerpt","title":"Recommended Catalog Structure (Excerpt)","text":"<pre><code>[versions]\njackson = \"2.18.0\"\nspring-boot = \"3.3.3\"\njunit = \"5.10.2\"\n\n[libraries]\njackson-core = { group = \"com.fasterxml.jackson.core\", name = \"jackson-core\", version.ref = \"jackson\" }\njackson-databind = { group = \"com.fasterxml.jackson.core\", name = \"jackson-databind\", version.ref = \"jackson\" }\njunit-jupiter = { group = \"org.junit.jupiter\", name = \"junit-jupiter\", version.ref = \"junit\" }\n\n[bundles]\njackson = [\"jackson-core\", \"jackson-databind\"]\n\n[plugins]\nspring-boot = { id = \"org.springframework.boot\", version.ref = \"spring-boot\" }\n</code></pre>"},{"location":"develop/maintenance/java/#naming-transformation-examples","title":"Naming Transformation Examples","text":"Group / Artifact Rationale Alias / Pattern <code>com.fasterxml.jackson.core</code> (family) Shared version across multiple artifacts <code>jackson</code> (version key) <code>org.springframework.boot</code> Core Spring Boot ecosystem / plugin <code>spring-boot</code> <code>org.springframework.cloud</code> Distinct Spring sub-ecosystem <code>spring-cloud</code> <code>io.micrometer:micrometer-registry-prometheus</code> Artifact-specific; family root <code>micrometer</code> <code>micrometer-prometheus</code> <code>ch.qos.logback</code> Simple unique family <code>logback</code> <code>org.projectlombok:lombok</code> Single artifact; group &amp; artifact align <code>lombok</code>"},{"location":"develop/maintenance/java/#antipatterns-vs-preferred","title":"Anti\u2011Patterns vs Preferred","text":"Anti\u2011Pattern Issue Preferred <code>com-fasterxml-jackson-core</code> Leaks full group, verbose <code>jackson</code> <code>springBoot</code> Mixed casing, not kebab <code>spring-boot</code> <code>jackson-databind-version</code> key Version name tied to specific artifact <code>jackson</code> (family version) <code>micrometerRegistryPrometheus</code> CamelCase and artifact untrimmed <code>micrometer-prometheus</code> <p>When to introduce a new version key</p> <p>Create a new version entry only when artifacts within the same group must intentionally diverge in version. Document the divergence with an inline comment to avoid future \u201ccleanup\u201d PRs that accidentally re\u2011align them.</p> <p>Renaming Aliases</p> <p>Renaming an existing alias creates a large diff across the repository. Avoid unless the current name is misleading or blocks adoption of a consistent pattern.</p>"},{"location":"develop/maintenance/java/#update-sources","title":"Update Sources","text":"<p>Planned (Cadence) Updates</p> <p>Run a scheduled dependency refresh (e.g., monthly for patch/minor; quarterly review for majors / platform upgrades) even if no urgent trigger appears. This prevents large, risky jumps later.</p> <p>Typical triggers for an update:</p> <ul> <li>Scheduled maintenance window (planned cadence refresh)</li> <li>Security advisory (CVE) or transitive vulnerability exposure</li> <li>Framework / BOM alignment (e.g., Spring Boot + Spring Cloud compatibility)</li> <li>New feature / API needed in a newer library version</li> <li>Upcoming EOL (JDK, framework, or library support window closing)</li> <li>Performance / memory / stability regression fix available upstream</li> <li>Build warnings or deprecations indicating future breakage</li> <li>(Future) Automated bot PR (Renovate / Dependabot) once enabled</li> </ul>"},{"location":"develop/maintenance/java/#workflow-routine-upgrade-patch-minor","title":"Workflow: Routine Upgrade (Patch / Minor)","text":"<ol> <li>Identify candidate updates with <code>./gradlew dependencyUpdates --no-parallel</code>.</li> <li>Edit the target version(s) in <code>gradle/libs.versions.toml</code>.</li> <li>Manually synchronize any hard\u2011coded versions in <code>buildSrc</code> (JUnit, JUnit Platform launcher, Lombok, Jacoco).    Update those literals to match the catalog (or the selected new version) and add an inline comment if deliberate divergence.</li> <li>Rerun <code>./gradlew dependencyUpdates --no-parallel</code> to confirm that the updates have been applied.</li> <li>Run a clean build:    <pre><code>./gradlew clean build\n</code></pre></li> <li>Run tests selectively if change is scoped, otherwise full test suite.</li> <li>Commit with message: <code>chore(deps): bump &lt;lib&gt; to &lt;version&gt;</code>.</li> <li>Push branch &amp; open/refresh PR.</li> </ol>"},{"location":"develop/maintenance/java/#batching-patch-minor-updates","title":"Batching Patch &amp; Minor Updates","text":"<p>Batching several low\u2011risk version bumps into a single PR reduces review overhead and keeps the catalog tidy. Follow these guidelines to keep risk controlled:</p> <p>Safe to Batch When</p> <ul> <li>All changes are patch or minor (no majors / milestones / RCs / betas).</li> <li>No code changes or API migrations required (catalog + lock + <code>buildSrc</code> literals only).</li> <li>All libraries belong to a small number of logical families (e.g., testing, logging, serialization) OR are clearly unrelated but trivially safe (pure patch).</li> <li>Build, tests, and lint produce no new warnings that demand immediate action.</li> </ul> <p>Good Grouping Examples</p> <ul> <li>Testing stack: JUnit, Mockito, AssertJ, Testcontainers.</li> <li>Jackson family (core / databind / annotations) at the same patch.</li> <li>Spring Boot + compatible Spring Cloud patch/minor (after checking release notes).</li> <li>Observability: Micrometer core + registry modules.</li> </ul> <p>When to Split Instead</p> <ul> <li>Any major version is involved.</li> <li>A dependency has known behavioral change even in a minor (e.g., Hibernate SQL generation tweaks, Netty event loop adjustments).</li> <li>A security fix (CVE) needs fast\u2011track isolation for audit clarity.</li> <li>A build tool / Gradle plugin upgrade could impact cache keys or task wiring.</li> <li>You must add resolution strategy rules or exclusions to resolve conflicts.</li> </ul> <p>Practical Limits</p> <ul> <li>Aim for \u2264 ~15 edited version lines (catalog + <code>buildSrc</code>) per batch PR.</li> <li>If reviewing the diff requires excessive scrolling, split by family.</li> </ul> <p>Commit / PR Style</p> <ul> <li>Single commit: <code>chore(deps): batch patch/minor updates (testing + jackson)</code></li> <li>Or one commit per family inside one PR if you want granular blame.</li> </ul> <p>Checklist for a Batched PR</p> <ul> <li> Only patch/minor versions included</li> <li> <code>gradle/libs.versions.toml</code> updated &amp; tidy (no orphaned commented versions)</li> <li> <code>buildSrc</code> literals (JUnit / Jacoco / Lombok) synced if touched</li> <li> <code>./gradlew dependencyUpdates --no-parallel</code> now shows only pending majors (or nothing relevant)</li> <li> <code>./gradlew clean build</code> passes</li> <li> Targeted / affected integration tests run (if any)</li> <li> No new deprecation floods (or documented in PR description)</li> <li> PR description lists families updated + highlights anything security\u2011related</li> </ul> <p>Avoid over\u2011broad batches</p> <p>Mixing unrelated ecosystems (e.g., database driver + logging + Spring + build plugins) increases risk and review fatigue. Smaller, coherent batches are merged faster.</p> <p>Automate grouping later</p> <p>A future enhancement could script grouping by scanning the catalog diff and clustering changes by group prefix (e.g., <code>com.fasterxml.jackson.*</code>).</p>"},{"location":"develop/maintenance/java/#workflow-major-upgrade","title":"Workflow: Major Upgrade","text":"<ol> <li>Review upstream release notes &amp; migration guide.</li> <li>Create a dedicated branch: <code>chore/deps/java-&lt;lib&gt;-&lt;major&gt;-upgrade</code>.</li> <li>Update version in <code>libs.versions.toml</code>.</li> <li>Run build &amp; inspect compilation/runtime failures.</li> <li>Apply required code/config migrations (document in PR description).</li> <li>Run extended test matrix (integration, contract, API compatibility if applicable).</li> <li>Add <code>BREAKING CHANGE:</code> footer to the PR description if public API impact.</li> <li>Request second reviewer (recommended for high-risk changes).</li> </ol>"},{"location":"develop/maintenance/java/#security-cve-response","title":"Security / CVE Response","text":"<ol> <li>Open branch: <code>security/deps/&lt;cve-id&gt;-&lt;lib&gt;</code>.</li> <li>Update only the affected library versions.</li> <li>Run targeted tests + any security scan tasks.</li> <li>Merge promptly after review.</li> <li>Include mitigation note in release summary.</li> </ol>"},{"location":"develop/maintenance/java/#tooling-commands","title":"Tooling &amp; Commands","text":"Action Command Notes List dependency insights <code>./gradlew dependencies</code> Per module (Optional) Check for new versions <code>./gradlew dependencyUpdates --no-parallel</code> Requires versions plugin Build &amp; test all <code>./gradlew clean build</code> Ensures no stale outputs Run a single module build <code>./gradlew :path:to:module:build</code> Faster feedback Generate dependency report <code>./gradlew htmlDependencyReport</code> If report plugin configured"},{"location":"develop/maintenance/java/#dependency-scopes","title":"Dependency Scopes","text":"Scope Use For Notes <code>implementation</code> Internal library use Not exposed transitively <code>api</code> Libraries whose APIs leak to consumers Use sparingly <code>compileOnly</code> Annotation processors, compile-time only Avoid runtime reliance <code>runtimeOnly</code> Drivers, logging impls Not needed at compile time <code>testImplementation</code> Test frameworks &amp; utilities Keep isolation"},{"location":"develop/maintenance/java/#managing-transitive-dependencies","title":"Managing Transitive Dependencies","text":"<ul> <li>Prefer allowing Gradle to resolve transitives unless a direct version is required to fix a CVE or conflict.</li> <li>Use <code>strictly</code> or <code>reject</code> rules only when necessary.</li> <li>Document any enforced versions in a dedicated section (e.g., below) to avoid accidental removal.</li> </ul>"},{"location":"develop/maintenance/java/#example-enforcement-block","title":"Example Enforcement Block","text":"<pre><code>configurations.all {\n    resolutionStrategy {\n        force(\"com.fasterxml.jackson.core:jackson-databind:2.18.0\")\n    }\n}\n</code></pre> <p>(Shift to version catalog alignment if possible instead of forcing.)</p>"},{"location":"develop/maintenance/java/#when-rarely-to-use-forcing","title":"When (Rarely) to Use Forcing","text":"<p>Use a forced version only as a short\u2011lived mitigation when one of these applies:</p> <ul> <li>A transitive dependency pulls an older vulnerable version (security / CVE hotfix).</li> <li>Upstream libraries have not yet released an aligned version and you must unblock a build.</li> <li>You are bisecting a regression and need to pin a single suspect artifact temporarily.</li> </ul> <p>Forces are global</p> <p>A <code>force</code> statement affects every configuration it touches. It can hide legitimate incompatibilities and introduce subtle runtime or test failures later. Treat it as an exception, not a pattern.</p>"},{"location":"develop/maintenance/java/#prefer-these-alternatives-first","title":"Prefer These Alternatives First","text":"Goal Preferred Mechanism Keep related artifacts aligned Shared version key in <code>libs.versions.toml</code> Enforce a coherent ecosystem version Import a BOM/platform (<code>platform(libs.spring.boot.bom)</code>) Override one problematic transitive Dependency constraint (<code>constraints { implementation(...) }</code>) Remove obsolete / conflicting module Exclusion on the specific dependency (<code>exclude(group = \"...\", module = \"...\")</code>) Document intentional divergence Inline comment + constraint (not a force) <p>Example constraint (scoped, clearer intent than a global force):</p> <pre><code>dependencies {\n    constraints {\n        implementation(\"com.fasterxml.jackson.core:jackson-databind:2.18.0\") {\n            because(\"CVE-2024-XXXX fixed in 2.18.0\")\n        }\n    }\n}\n</code></pre>"},{"location":"develop/maintenance/java/#temporary-force-checklist","title":"Temporary Force Checklist","text":"<ul> <li> Justification (CVE, blocking regression) noted as a code comment.</li> <li> Issue / ticket created to remove the force.</li> <li> Catalog already reflects the target version (avoid hidden divergence).</li> <li> Evaluated BOM / constraint alternative and documented why not used.</li> <li> Full build + tests + key integration tests pass.</li> </ul>"},{"location":"develop/maintenance/java/#migration-off-a-force","title":"Migration Off a Force","text":"<ol> <li>Monitor upstream releases until dependencies naturally resolve to the desired version.</li> <li>Remove the <code>force</code> clause.</li> <li>Run dependency insight to ensure the resolved graph is clean:    <pre><code>./gradlew dependencyInsight --dependency jackson-databind\n</code></pre></li> <li>If multiple versions remain, add a constraint instead of re\u2011adding the force.</li> <li>Commit: <code>chore(deps): remove temporary force on jackson-databind</code>.</li> </ol>"},{"location":"develop/maintenance/java/#auditing-existing-forces","title":"Auditing Existing Forces","text":"<p>Quick commands to discover and validate enforced versions:</p> Purpose Command List all places using strategy <code>grep -R \"resolutionStrategy\" -n .</code> Inspect resolution path <code>./gradlew dependencyInsight --dependency jackson-databind</code> View runtime graph (sample) <code>./gradlew dependencies --configuration runtimeClasspath</code> <p>Automate enforcement hygiene</p> <p>Consider a small CI script that fails if new <code>force(</code> usages are added without an inline <code># justified:</code> marker, encouraging disciplined use.</p>"},{"location":"develop/maintenance/java/#version-alignment-boms","title":"Version Alignment / BOMs","text":"<p>If a platform (BOM) is used (e.g., Spring Boot):</p> <pre><code>dependencies {\n    implementation(platform(libs.spring.boot.bom))\n    implementation(libs.spring.boot.starter.web)\n}\n</code></pre> <p>Keep BOM version updates isolated to ease troubleshooting.</p>"},{"location":"develop/maintenance/java/#testing-strategy-after-upgrades","title":"Testing Strategy After Upgrades","text":"<ol> <li>Unit tests (fast feedback)</li> <li>Integration tests (service wiring, DB migrations)</li> <li>Contract/API tests (if publishing endpoints or libraries)</li> <li>Performance smoke (optional: startup time, memory)</li> </ol>"},{"location":"develop/maintenance/java/#performance-regression-checks-optional","title":"Performance &amp; Regression Checks (Optional)","text":"<p>Track key metrics (startup time, heap, request latency). For significant framework upgrades, capture before/after diffs.</p>"},{"location":"develop/maintenance/java/#common-issues-resolutions","title":"Common Issues &amp; Resolutions","text":"Issue Cause Resolution <code>NoSuchMethodError</code> Mixed library versions Ensure single version in catalog Annotation processor conflicts Duplicate processors Exclude older processor Classpath length errors Deep transitive graph Prune unused libs, prefer lighter alternatives Build cache misses Unstable inputs Pin plugin versions, avoid timestamp-based tasks"},{"location":"develop/maintenance/java/#pr-review-checklist-java","title":"PR Review Checklist (Java)","text":"<ul> <li> Versions updated only in <code>libs.versions.toml</code></li> <li> No stray version strings left in module <code>build.gradle.kts</code></li> <li> All tests pass locally/CI</li> <li> Release notes/migration doc referenced (for majors)</li> <li> No forced resolutions added without justification</li> <li> Security concerns addressed / CVE linked if relevant</li> </ul>"},{"location":"develop/maintenance/java/#future-improvements","title":"Future Improvements","text":"<ul> <li>Adopt automated version report in CI</li> <li>Add SBOM generation (e.g., CycloneDX Gradle plugin)</li> <li>Integrate vulnerability scanning (OWASP, Snyk, etc.)</li> </ul>"},{"location":"develop/maintenance/java/#references","title":"References","text":"<ul> <li>Gradle Version Catalogs: https://docs.gradle.org/current/userguide/platforms.html</li> <li>Dependency Management Best Practices: https://docs.gradle.org</li> </ul>"},{"location":"develop/maintenance/java/#custom-gradle-extensions-buildsrc","title":"Custom Gradle Extensions (buildSrc)","text":"<p>The monorepo defines reusable Gradle convention plugins and helper scripts in <code>buildSrc/</code>. These act like lightweight, locally published plugins automatically available to every build without extra coordinates.</p>"},{"location":"develop/maintenance/java/#what-are-they","title":"What Are They?","text":"<p>Gradle loads any code under <code>buildSrc/</code> as an included build. Kotlin script files under <code>buildSrc/src/main/kotlin/</code> (e.g. <code>sage.java-library.gradle.kts</code>) are effectively precompiled script plugins. Applying <code>id(\"sage.java-library\")</code> in a module delegates to the corresponding file.</p>"},{"location":"develop/maintenance/java/#where-they-live","title":"Where They Live","text":"File Purpose <code>sage.java-common.gradle.kts</code> Base Java configuration (Java 21, encoding) <code>sage.java-library.gradle.kts</code> Library conventions + JUnit (publishing) <code>sage.spring-boot-application.gradle.kts</code> Spring Boot app conventions (tests, image) <code>sage.spring-boot-library.gradle.kts</code> Spring Boot library conventions (no bootJar) <code>sage.lombok.gradle.kts</code> Lombok annotation processing/version override support <code>sage.jacoco-coverage.gradle.kts</code> Jacoco coverage + verification rules"},{"location":"develop/maintenance/java/#why-versions-are-duplicated-here","title":"Why Versions Are Duplicated Here","text":"<p>These scripts cannot (directly) consume the central version catalog (<code>libs.versions.toml</code>) because Gradle resolves <code>buildSrc</code> earlier in the lifecycle. As a result, any dependency or tool versions hard-coded inside these scripts (e.g., JUnit, Jacoco, Lombok) must be kept manually in sync with the catalog to avoid drift.</p>"},{"location":"develop/maintenance/java/#manual-sync-required","title":"Manual Sync Required","text":"<p>When updating versions (e.g., via <code>./gradlew dependencyUpdates --no-parallel</code>):</p> <ol> <li>Update <code>gradle/libs.versions.toml</code> for runtime &amp; library use.</li> <li>Manually inspect <code>buildSrc/src/main/kotlin/*.gradle.kts</code> for hard-coded versions:<ul> <li>JUnit (<code>org.junit.jupiter:junit-jupiter:&lt;ver&gt;</code>)</li> <li>JUnit Platform launcher</li> <li>Lombok default version in <code>LombokExtension</code></li> <li>Jacoco <code>toolVersion</code></li> </ul> </li> <li>Bump those values to match the catalog (if defined there) or to the chosen new version if catalog does not track it yet.</li> <li>Run a clean build to detect mismatches:    <pre><code>./gradlew clean build\n</code></pre></li> <li>If a version in <code>buildSrc</code> intentionally differs (rare), document the rationale in an inline comment.</li> </ol>"},{"location":"develop/maintenance/java/#suggested-future-improvement","title":"Suggested Future Improvement","text":"<ul> <li>Introduce a verification task that parses <code>buildSrc</code> for known version literals and compares them to <code>libs.versions.toml</code>, failing the build when they diverge.</li> <li>Optionally migrate to an included build (composite) plugin project that can itself use a version catalog.</li> </ul>"},{"location":"develop/maintenance/java/#pr-review-checklist-additions","title":"PR Review Checklist Additions","text":"<ul> <li> <code>buildSrc</code> scripts updated for any dependency bumped in catalog</li> <li> JUnit / Jacoco / Lombok versions aligned</li> <li> Inline comment added if intentional divergence</li> </ul>"},{"location":"develop/maintenance/java/#build-tool-jdk-upgrades","title":"Build Tool &amp; JDK Upgrades","text":"<p>This section covers upgrading the Gradle build tool itself and the Java toolchain used by the monorepo. Perform these upgrades separately from routine library dependency batches for clearer review and rollback.</p>"},{"location":"develop/maintenance/java/#upgrading-gradle","title":"Upgrading Gradle","text":"<p>Upgrading Gradle keeps build performance, security, and deprecation coverage current. Wrapper\u2011based upgrades are low risk when validated systematically.</p>"},{"location":"develop/maintenance/java/#when-to-upgrade","title":"When to Upgrade","text":"<ul> <li>New major or minor with performance improvements or important bug fixes</li> <li>Deprecation warnings appearing in current builds that will become errors next release</li> <li>Plugin ecosystem (e.g., Spring Boot plugin) now officially supports a newer Gradle baseline</li> <li>Security advisory in an older Gradle distribution</li> </ul>"},{"location":"develop/maintenance/java/#preflight-checklist","title":"Pre\u2011Flight Checklist","text":"<ul> <li> Review latest Gradle release and its release notes</li> <li> Verify core &amp; third\u2011party plugins declare compatibility</li> <li> Ensure CI images / dev container already have a compatible JDK (Gradle 9 requires Java 21+)</li> <li> No custom init scripts or build logic relying on removed APIs</li> </ul>"},{"location":"develop/maintenance/java/#upgrade-procedure-example-910","title":"Upgrade Procedure (Example: 9.1.0)","text":"<ol> <li>Baseline build:    <pre><code>./gradlew clean build\n</code></pre></li> <li>Regenerate wrapper:    <pre><code>./gradlew wrapper --gradle-version 9.1.0 --distribution-type bin\n</code></pre></li> <li>Inspect changes:<ul> <li><code>gradle/wrapper/gradle-wrapper.properties</code> updated (distributionUrl points to <code>gradle-9.1.0-bin.zip</code>)</li> <li><code>gradlew</code> / <code>gradlew.bat</code> updated (commit them)</li> <li>Do not manually edit wrapper JAR/scripts; regenerate if unexpected</li> </ul> </li> <li> <p>Verify version:</p> <pre><code>./gradlew --version\n</code></pre> <p>Expected style of output:</p> <pre><code>------------------------------------------------------------\nGradle 9.1.0\n------------------------------------------------------------\n\nBuild time:    2025-09-18 13:05:56 UTC\nRevision:      e45a8dbf2470c2e2474ccc25be9f49331406a07e\n\nKotlin:        2.2.0\nGroovy:        4.0.28\nAnt:           Apache Ant(TM) version 1.10.15 compiled on August 25 2024\nLauncher JVM:  21.0.7 (Microsoft 21.0.7+6-LTS)\nDaemon JVM:    /usr/local/sdkman/candidates/java/21.0.7-ms\nOS:            Linux 6.1.148-173.267.amzn2023.x86_64 amd64\n</code></pre> </li> <li> <p>Full rebuild to surface deprecations:    <pre><code>./gradlew clean build\n</code></pre></p> </li> <li>Validate multi\u2011project &amp; Docker image builds:    <pre><code>nx run-many --target=build,build-image --projects=tag:language:java\n</code></pre></li> <li>Remove the Gradle wrapper for Windows:    <pre><code>rm -fr gradlew.bat\n</code></pre></li> <li>Commit:    <pre><code>git add gradle/wrapper/gradle-wrapper.properties gradlew\ngit commit -m \"build: upgrade to Gradle 9.1.0\"\n</code></pre></li> <li>PR description should include:<ul> <li>Release notes link</li> <li>Summary of any new deprecation warnings (or none)</li> <li>Confirmation of successful full + Docker image builds</li> </ul> </li> </ol>"},{"location":"develop/maintenance/java/#postupgrade-validation","title":"Post\u2011Upgrade Validation","text":"<ul> <li> CI pipelines green</li> <li> No new flaky tests introduced</li> <li> (If using scans) No major regression in configuration time</li> <li> Team notified to clear local caches only if necessary</li> </ul>"},{"location":"develop/maintenance/java/#rollback","title":"Rollback","text":"<p>If a blocking issue appears:</p> <pre><code>./gradlew wrapper --gradle-version &lt;previous-version&gt; --distribution-type bin\ngit add gradle/wrapper/gradle-wrapper.properties gradlew gradlew.bat\ngit commit -m \"revert: downgrade Gradle to &lt;previous-version&gt; (regression)\"\n</code></pre> <p>Plugin Compatibility</p> <p>Confirm critical plugins (Spring Boot, Spotless, Jacoco, Testcontainers, publishing) list the new Gradle version in their matrix. Upgrade lagging plugins first.</p> <p>Wrapper Integrity</p> <p>Always regenerate rather than manually editing wrapper files to ensure checksum authenticity.</p>"},{"location":"develop/maintenance/java/#updating-java-placeholder","title":"Updating Java (Placeholder)","text":""},{"location":"develop/maintenance/node/","title":"Node.js / TypeScript Dependency Maintenance","text":"<p>This guide describes the process for updating and validating Node.js and TypeScript dependencies in the monorepo. It focuses on using pnpm workspaces with a centralized <code>package.json</code> and ensuring consistent, reproducible builds across Angular, React, and Node.js applications.</p>"},{"location":"develop/maintenance/node/#overview","title":"Overview","text":"<p>Node.js dependencies are managed primarily via the root <code>package.json</code> file in conjunction with pnpm workspaces. Individual projects reference shared dependencies from the root, ensuring:</p> <ul> <li>Version alignment across all applications and libraries</li> <li>Simplified upgrades and security remediation</li> <li>Reduced dependency drift and bundle duplication</li> <li>Efficient disk usage through symlinked <code>node_modules</code></li> </ul>"},{"location":"develop/maintenance/node/#key-files","title":"Key Files","text":"File / Path Role / Purpose Maintenance Notes <code>package.json</code> (root) Central dependency management for all TypeScript/Node.js projects Primary source of truth for versions <code>pnpm-workspace.yaml</code> Defines workspace packages and exclusions Keep aligned with project structure <code>tsconfig.base.json</code> TypeScript path mapping and compiler configuration Update path mappings when adding/removing libraries <code>nx.json</code> Nx workspace configuration and task execution Configure build, test, and lint tasks <code>apps/*/project.json</code>, <code>libs/*/project.json</code> Individual project configurations Should reference root dependencies via Nx or workspace resolution <code>apps/*/package.json</code>, <code>libs/*/package.json</code> Local package configurations for publishable libraries or deployable artifacts Used for publishable libraries or deployment artifacts (e.g., Lambda functions); must be excluded from pnpm workspace in <code>pnpm-workspace.yaml</code> if using shared <code>node_modules</code> <code>pnpm-lock.yaml</code> Lock file ensuring reproducible installations Automatically updated; commit changes <code>jest.config.ts</code>, <code>jest.preset.js</code> Testing framework configuration Align with testing library versions <code>stylelint.config.mjs</code>, <code>tsconfig.base.json</code> Linting and TypeScript configuration Keep linter versions synchronized <p>Workspace dependency resolution</p> <p>Individual projects within the workspace automatically inherit dependencies from the root <code>package.json</code>. Local <code>package.json</code> files are used in a minority of projects, usually when a library is publishable or when the <code>package.json</code> is an artifact that can be deployed (e.g., as part of an AWS Lambda function). If a project has a local <code>package.json</code> but still uses the shared (root) <code>node_modules</code>, then the project must be excluded from the pnpm workspace in <code>pnpm-workspace.yaml</code>. There is a risk of package definition drift if local <code>package.json</code> files are not regularly reviewed and updated to align with the root dependencies and the libraries actually used by the project.</p>"},{"location":"develop/maintenance/node/#dependency-management-strategy","title":"Dependency Management Strategy","text":"<p>The monorepo uses several strategies to maintain consistent and efficient dependency management:</p>"},{"location":"develop/maintenance/node/#centralized-dependencies","title":"Centralized Dependencies","text":"<p>All shared dependencies are defined in the root <code>package.json</code>, including:</p> <ul> <li>Framework dependencies: Angular, React, Express</li> <li>Testing frameworks: Jest, Playwright, Testing Library</li> <li>Build tools: Nx, TypeScript, Vite, webpack</li> <li>Linting tools: ESLint, Prettier, Stylelint</li> <li>Development utilities: Husky, lint-staged</li> </ul>"},{"location":"develop/maintenance/node/#version-management-conventions","title":"Version Management Conventions","text":""},{"location":"develop/maintenance/node/#general-rules","title":"General Rules","text":"<ol> <li>Single version per package: Maintain one version of each package across the entire monorepo to prevent conflicts</li> <li>Pin exact versions: Use exact version numbers (e.g., <code>\"1.2.3\"</code>) rather than version ranges (e.g., <code>\"^1.2.3\"</code> or <code>\"~1.2.3\"</code>) to ensure reproducible builds and prevent unexpected updates</li> <li>Semantic versioning awareness: Understand the impact of major, minor, and patch updates</li> <li>Framework alignment: Keep related packages (e.g., all Angular packages) at the same version</li> <li>Security priority: Prioritize security updates even for major version jumps</li> <li>Testing coverage: Ensure all version updates are covered by existing test suites</li> </ol>"},{"location":"develop/maintenance/node/#dependency-categories","title":"Dependency Categories","text":"Category Examples Update Strategy Framework Core <code>@angular/core</code>, <code>react</code>, <code>express</code> Coordinated updates with migration guides Nx Ecosystem <code>@nx/angular</code>, <code>@nx/react</code>, <code>nx</code> Keep versions aligned, follow Nx migration guides Testing <code>jest</code>, <code>@playwright/test</code>, <code>@testing-library/*</code> Regular updates, ensure test compatibility Build Tools <code>typescript</code>, <code>vite</code>, <code>webpack</code> Test thoroughly, may require configuration changes Linting/Formatting <code>eslint</code>, <code>prettier</code>, <code>stylelint</code> Regular updates, check for breaking rule changes Development <code>husky</code>, <code>lint-staged</code> Low-risk updates, test development workflows"},{"location":"develop/maintenance/node/#version-alignment-examples","title":"Version Alignment Examples","text":"<p>All dependencies should use exact version numbers without ranges:</p> <pre><code>{\n  \"dependencies\": {\n    \"@angular/animations\": \"20.1.8\",\n    \"@angular/common\": \"20.1.8\",\n    \"@angular/core\": \"20.1.8\",\n    \"@angular/forms\": \"20.1.8\"\n  },\n  \"devDependencies\": {\n    \"@nx/angular\": \"21.4.1\",\n    \"@nx/jest\": \"21.4.1\",\n    \"@nx/workspace\": \"21.4.1\"\n  }\n}\n</code></pre> <p>Avoid version ranges</p> <p>Do not use caret (<code>^1.2.3</code>) or tilde (<code>~1.2.3</code>) ranges as they can lead to inconsistent builds across environments. Always specify exact versions.</p>"},{"location":"develop/maintenance/node/#workflow-routine-upgrade-patch-minor","title":"Workflow: Routine Upgrade (Patch / Minor)","text":"<ol> <li>Check for updates using npm or third-party tools:    <pre><code>pnpm outdated\n</code></pre></li> <li>Update target versions in the root <code>package.json</code>. Always specify exact versions (e.g., <code>\"20.1.8\"</code>) in package.json. If pnpm adds ranges, manually edit to remove them.    <pre><code># Update specific packages\npnpm update echarts ngx-echarts\n# or manually edit package.json with exact versions\n</code></pre></li> <li>Install and update lockfile:    <pre><code>pnpm install\n</code></pre></li> <li>Run quality checks:    <pre><code># Build all projects (includes TypeScript compilation)\nnx run-many -t build\n\n# Run tests\nnx run-many -t test\n\n# Run linting (includes TypeScript/ESLint checks)\nnx run-many -t lint\n</code></pre></li> <li>Test applications locally (sample of key apps):    <pre><code>agora-build-images &amp;&amp; agora-docker-start\nmodel-ad-build-images &amp;&amp; model-ad-docker-start\n</code></pre></li> <li>Commit changes:    <pre><code>git add package.json pnpm-lock.yaml\ngit commit -m \"chore(deps): update dependencies (patch/minor)\"\n</code></pre></li> </ol>"},{"location":"develop/maintenance/node/#batching-updates","title":"Batching Updates","text":"<p>Safe to Batch When:</p> <ul> <li>All changes are patch or minor versions</li> <li>No breaking changes in release notes</li> <li>All packages belong to related ecosystems</li> <li>Build and tests pass without modifications</li> </ul> <p>Good Grouping Examples:</p> <ul> <li>Nx ecosystem: All packages updated by <code>nx migrate latest</code></li> <li>Testing stack: Jest, Testing Library, Playwright</li> <li>Linting tools: ESLint, Prettier, Stylelint and their plugins</li> </ul> <p>Commit Style for Batched Updates:</p> <pre><code>git commit -m \"chore(deps): batch minor updates (nx ecosystem)\"\ngit commit -m \"chore(deps): update testing dependencies\"\n</code></pre>"},{"location":"develop/maintenance/node/#framework-migration-workflows","title":"Framework Migration Workflows","text":"<p>For framework updates that may include breaking changes, automated migrations can handle most of the heavy lifting. This is especially important for Angular and Nx updates.</p> <p>Unified Migration Strategy</p> <p>Always use <code>nx migrate latest</code> as the primary migration command. Nx expects all Nx-related packages (including Angular packages) to be on the same version for compatibility. Attempting to migrate individual packages separately can cause version conflicts and missed migrations.</p>"},{"location":"develop/maintenance/node/#migration-workflow","title":"Migration Workflow","text":"<p>Use When:</p> <ul> <li>Updating any Nx-related packages (<code>@nx/*</code>, <code>@angular/*</code>, <code>@angular-devkit/*</code>)</li> <li>Regular maintenance updates</li> <li>Major framework version jumps</li> </ul> <pre><code># Migrate all Nx and Angular packages together\nnx migrate latest\n\n# Review the changes to package.json then install\npnpm install --no-frozen-lockfile\n\n# This creates migrations.json with pending migrations\n# Review migrations.json to understand what will change\ncat migrations.json\n\n# Run the migrations (this modifies code)\nnx migrate --run-migrations\n\n# Copy and paste the content of migrations.json into the PR description\n\n# Commit the migrations file for posterity\ngit add --force migrations.json\ngit commit -m \"chore(deps): add migrations.json for posterity\"\n\n# Clean up after successful migration\ngit rm --cached migrations.json\ngit commit -m \"chore(deps): remove migrations.json from tracking\"\nrm migrations.json\n</code></pre>"},{"location":"develop/maintenance/node/#migration-troubleshooting","title":"Migration Troubleshooting","text":"Issue Solution Migration fails with conflicts Reset branch, update packages manually, then run specific migrations <code>migrations.json</code> not generated Ensure you're in workspace root, check package versions compatibility Partial migration completion Check git status, commit successful changes, manually handle remaining Build failures after migration Review migration logs, check for custom code that needs manual updates Peer dependency warnings Install missing peers or use <code>pnpm install --ignore-peerDeps</code> temporarily"},{"location":"develop/maintenance/node/#migration-vs-manual-updates","title":"Migration vs Manual Updates","text":"<p>Use migrations for:</p> <ul> <li>Major version jumps (Angular 17 \u2192 18 \u2192 19)</li> <li>Cross-cutting changes (workspace structure, build configs)</li> <li>Breaking API changes</li> </ul> <p>Use manual updates for:</p> <ul> <li>Patch versions (20.1.8 \u2192 20.1.9)</li> <li>Simple dependency version bumps</li> <li>Packages without migration support</li> </ul>"},{"location":"develop/maintenance/node/#workflow-major-upgrade","title":"Workflow: Major Upgrade","text":"<p>Use Migrations First</p> <p>For Angular and Nx major updates, try the Migration Workflows first. Fall back to manual upgrade only if migrations fail or are incomplete.</p> <ol> <li>Create dedicated branch:    <pre><code>git checkout -b chore/deps/node-&lt;package&gt;-&lt;major&gt;-upgrade\n</code></pre></li> <li>Try automated migration for Nx and Angular updates. Follow the steps in Migration Workflows. If the migration succeeds, skip to step 6.</li> <li>Review upstream changes (if migration unavailable/fails):       - Read migration guides and breaking changes       - Check compatibility with other dependencies       - Review Angular/React update guides if applicable</li> <li>Update package.json with new major version</li> <li>Install and check for conflicts:    <pre><code>pnpm install\n# Resolve any peer dependency warnings\n</code></pre></li> <li>Run build and address compilation errors:    <pre><code>nx run-many -t build\n# Fix TypeScript errors, import changes, API modifications\n</code></pre></li> <li>Update configurations as needed:       - Jest configurations for testing framework updates       - ESLint rules for linter updates       - Angular/React configuration files</li> <li>Run comprehensive tests:    <pre><code>nx run-many -t test\nnx run-many -t e2e\n</code></pre></li> <li>Test critical user paths manually</li> <li>Document breaking changes in PR description</li> </ol>"},{"location":"develop/maintenance/node/#security-cve-response","title":"Security / CVE Response","text":"<ol> <li>Identify affected packages:    <pre><code>pnpm audit\n# or use GitHub Security alerts\n</code></pre></li> <li>Create security branch:    <pre><code>git checkout -b security/deps/&lt;cve-id&gt;-&lt;package&gt;\n</code></pre></li> <li>Update to secure version:    <pre><code>pnpm update &lt;vulnerable-package&gt;\n</code></pre></li> <li>Use overrides if necessary. Use exact versions in overrides to ensure security fixes are applied consistently.    <pre><code>{\n  \"pnpm\": {\n    \"overrides\": {\n      \"vulnerable-package\": \"1.2.3\"\n    }\n  }\n}\n</code></pre></li> <li>Verify fix:    <pre><code>pnpm audit\n</code></pre></li> <li>Test, create PR, and merge promptly</li> </ol>"},{"location":"develop/maintenance/node/#tooling-commands","title":"Tooling &amp; Commands","text":"Action Command Notes Check outdated packages <code>pnpm outdated</code> Shows available updates Interactive updates <code>pnpm dlx npm-check-updates -i</code> Select updates interactively Install dependencies <code>pnpm install</code> Updates lockfile Security audit <code>pnpm audit</code> Check for vulnerabilities Framework Migrations Nx &amp; Angular migration <code>nx migrate latest</code> Migrate all Nx/Angular packages Run migrations <code>nx migrate --run-migrations</code> Execute pending migrations Quality Checks Build all projects <code>nx run-many -t build</code> Parallel builds Test all projects <code>nx run-many -t test</code> Run all tests Lint all projects <code>nx run-many -t lint</code> Code quality checks Affected projects only <code>nx affected -t build</code> Build only changed projects"},{"location":"develop/maintenance/node/#managing-transitive-dependencies","title":"Managing Transitive Dependencies","text":""},{"location":"develop/maintenance/node/#pnpm-overrides","title":"PNPM Overrides","text":"<p>Use pnpm overrides to force specific versions of transitive dependencies:</p> <pre><code>{\n  \"pnpm\": {\n    \"overrides\": {\n      \"axios\": \"1.8.2\",\n      \"cross-spawn\": \"7.0.6\"\n    }\n  }\n}\n</code></pre> <p>Pin all overrides</p> <p>Always use exact versions in pnpm overrides to ensure predictable dependency resolution.</p>"},{"location":"develop/maintenance/node/#when-to-use-overrides","title":"When to Use Overrides","text":"<ul> <li>Security fixes: Force secure versions of transitive dependencies</li> <li>Compatibility issues: Resolve version conflicts between packages</li> <li>Bug fixes: Apply fixes not yet propagated through dependency tree</li> </ul> <p>Overrides are global</p> <p>Overrides affect the entire dependency tree. Use sparingly and document reasons.</p>"},{"location":"develop/maintenance/node/#peer-dependencies","title":"Peer Dependencies","text":"<p>Handle peer dependency warnings appropriately:</p> <ul> <li>Install missing peers if they're genuinely needed</li> <li>Use <code>pnpm install --ignore-peerDeps</code> only temporarily during upgrades</li> <li>Document peer dependency choices in commit messages</li> </ul>"},{"location":"develop/maintenance/node/#framework-specific-considerations","title":"Framework-Specific Considerations","text":""},{"location":"develop/maintenance/node/#angular-projects","title":"Angular Projects","text":"<ul> <li>Use unified migrations: Always use <code>nx migrate latest</code> to keep all Angular and Nx packages aligned</li> <li>Incremental major updates: For major Angular version jumps (e.g., 17 \u2192 18 \u2192 19), consider migrating one major version at a time</li> <li>Migration schematics: Refer to Migration Workflows for proper migration approach</li> </ul>"},{"location":"develop/maintenance/node/#react-projects","title":"React Projects","text":"<ul> <li>React version compatibility: Keep <code>react</code> and <code>react-dom</code> aligned</li> <li>Hook dependencies: Update libraries that depend on React hooks together</li> <li>Build tools: Coordinate React updates with Vite/webpack configurations</li> </ul>"},{"location":"develop/maintenance/node/#nodejs-applications","title":"Node.js Applications","text":"<ul> <li>Runtime compatibility: Ensure packages work with the project's Node.js version</li> <li>Express middleware: Test middleware compatibility after Express updates</li> <li>Database drivers: Coordinate database client updates with schema changes</li> </ul>"},{"location":"develop/maintenance/node/#common-issues-resolutions","title":"Common Issues &amp; Resolutions","text":"Issue Cause Resolution <code>Module not found</code> errors Path mapping or dependency issues Check <code>tsconfig.base.json</code> paths, verify package installation <code>Peer dependency warnings</code> Version mismatches Install compatible peer dependencies or use overrides <code>Jest configuration errors</code> Testing framework updates Update Jest config, preset, and transform settings <code>Build failures</code> TypeScript or tooling incompatibilities Check TypeScript version compatibility, update build configs <code>Memory issues during builds</code> Large dependency trees Increase Node.js memory limit, optimize dependencies"},{"location":"develop/maintenance/node/#pr-review-checklist","title":"PR Review Checklist","text":"<ul> <li> Only <code>package.json</code> and <code>pnpm-lock.yaml</code> modified (for simple updates)</li> <li> All versions are exact (no caret <code>^</code> or tilde <code>~</code> ranges)</li> <li> All related packages updated together (e.g., all Angular packages)</li> <li> <code>pnpm audit</code> shows no new vulnerabilities</li> <li> All builds pass: <code>nx run-many -t build</code></li> <li> All tests pass: <code>nx run-many -t test</code></li> <li> No breaking changes without migration documentation</li> <li> Path mappings updated if new libraries added</li> </ul>"},{"location":"develop/maintenance/python/","title":"Python Dependency Maintenance","text":"<p>(Placeholder \u2013 to be populated later.)</p>"},{"location":"develop/maintenance/r/","title":"R Dependency Maintenance","text":"<p>(Placeholder \u2013 to be populated later.)</p>"},{"location":"develop/maintenance/security/","title":"Security &amp; Compliance","text":"<p>(Placeholder \u2013 to be populated later.)</p>"},{"location":"develop/maintenance/troubleshooting/","title":"Troubleshooting &amp; FAQ","text":"<p>(Placeholder \u2013 to be populated later.)</p>"},{"location":"develop/tutorials/angular/add-api-client/","title":"Add API Client","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/angular/add-app/","title":"Create App","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/angular/add-component/","title":"Add Component","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/angular/add-library/","title":"Add Library","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/docker/new-project/","title":"New Project","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/java/add-library/","title":"Add Library","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/java/add-rest-api/","title":"Add REST API","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/r/new-project/","title":"New Project","text":"<p>Content coming soon!</p>"},{"location":"develop/tutorials/terraform/create-backend/","title":"Tutorial: Create a Terraform Backend","text":""},{"location":"develop/tutorials/terraform/create-backend/#overview","title":"Overview","text":"<p>This tutorial guides you through creating a Terraform backend infrastructure project using the monorepo's reusable module pattern. You'll deploy an S3 bucket and DynamoDB table to AWS that will store Terraform state for future infrastructure projects.</p> <p>What is a Terraform Project?</p> <p>In the Sage Monorepo, a Terraform project is an Nx project (located in <code>apps/&lt;product&gt;/infra/terraform/</code> or <code>libs/platform/infra/terraform/</code>) that uses OpenTofu and Terragrunt to manage cloud infrastructure. Each project can contain multiple modules (logical infrastructure components).</p> <p>What you'll build:</p> <ul> <li>S3 bucket with versioning and encryption for state storage</li> <li>DynamoDB table for state locking</li> <li>Secure bucket policies and access controls</li> <li>A Terraform project configured as an Nx project</li> </ul> <p>Time required: 30-45 minutes</p> <p>Prerequisites:</p> <ul> <li>AWS CLI installed and configured</li> <li>AWS SSO profile set up</li> <li>OpenTofu 1.10.6+ (installed in dev container)</li> <li>Terragrunt 0.87.5+ (installed in dev container)</li> <li>Basic understanding of Infrastructure as Code concepts</li> </ul>"},{"location":"develop/tutorials/terraform/create-backend/#step-1-understand-the-architecture","title":"Step 1: Understand the Architecture","text":"<p>Before we start, review the Terraform Infrastructure Architecture to understand:</p> <ul> <li>The layered configuration model (workspace \u2192 project \u2192 module)</li> <li>The bootstrap exception pattern (why this project uses local state)</li> <li>How reusable modules work</li> </ul> <p>Key Concept: The backend creates the infrastructure that stores state for other projects, so it uses a local backend initially. This is intentional and correct.</p>"},{"location":"develop/tutorials/terraform/create-backend/#step-2-create-the-nx-project-structure","title":"Step 2: Create the Nx Project Structure","text":"<p>Create the project directory structure:</p> <pre><code># Navigate to your product's infrastructure directory\ncd apps/&lt;product&gt;/infra/terraform/\n\n# Create the project directory\nmkdir -p terraform-backend/terraform-backend\ncd terraform-backend\n</code></pre> <p>Your structure should look like:</p> <pre><code>apps/&lt;product&gt;/infra/terraform/terraform-backend/\n\u251c\u2500\u2500 project.json           # Nx project configuration (we'll create this)\n\u251c\u2500\u2500 project.hcl            # Project-level Terragrunt configuration\n\u251c\u2500\u2500 config.yaml            # Project settings\n\u251c\u2500\u2500 README.md              # Documentation\n\u251c\u2500\u2500 .env.example           # Environment variable template\n\u2514\u2500\u2500 terraform-backend/     # Module directory\n    \u2514\u2500\u2500 terragrunt.hcl     # Module configuration\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#step-3-create-the-nx-project-configuration","title":"Step 3: Create the Nx Project Configuration","text":"<p>Create <code>project.json</code> to integrate with Nx:</p> <pre><code>{\n  \"name\": \"&lt;product&gt;-infra-terraform-terraform-backend\",\n  \"$schema\": \"../../../../node_modules/nx/schemas/project-schema.json\",\n  \"projectType\": \"application\",\n  \"tags\": [\"language:terraform\", \"product:&lt;product&gt;\", \"type:infra\"],\n  \"targets\": {\n    \"init\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt init --working-dir terraform-backend\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    },\n    \"validate\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt validate --working-dir terraform-backend\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    },\n    \"plan\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt plan --working-dir terraform-backend\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    },\n    \"deploy\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt apply --working-dir terraform-backend\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    },\n    \"destroy\": {\n      \"executor\": \"nx:run-commands\",\n      \"options\": {\n        \"command\": \"terragrunt destroy --working-dir terraform-backend\",\n        \"cwd\": \"{projectRoot}\"\n      }\n    }\n  }\n}\n</code></pre> <p>Replace <code>&lt;product&gt;</code> with your product name (e.g., <code>bixarena</code>, <code>openchallenges</code>).</p>"},{"location":"develop/tutorials/terraform/create-backend/#step-4-create-the-project-configuration","title":"Step 4: Create the Project Configuration","text":"<p>Create <code>project.hcl</code> to define project-level settings:</p> <pre><code>locals {\n  workspace_vars = read_terragrunt_config(find_in_parent_folders(\"workspace.hcl\"))\n\n  # Default configuration structure\n  _default_config = {\n    product     = \"\"\n    application = \"\"\n    environment = \"\"\n    terraform_backend = {\n      bucket_name    = \"\"\n      bucket_region  = \"\"\n      dynamodb_table = \"\"\n    }\n    modules = {\n      terraform_backend = {\n        aws_provider = {\n          region = \"\"\n        }\n      }\n    }\n  }\n\n  # Load config.yaml and merge with defaults\n  _config_yaml = try(\n    yamldecode(file(try(find_in_parent_folders(\"config.yaml\"), \"\"))),\n    {}\n  )\n  _merged_config = merge(local._default_config, local._config_yaml)\n\n  # Project vars with environment variable overrides\n  project_vars = {\n    product     = get_env(\"PRODUCT\", local._merged_config.product)\n    application = get_env(\"APPLICATION\", local._merged_config.application)\n    environment = get_env(\"ENVIRONMENT\", local._merged_config.environment)\n\n    terraform_backend = {\n      bucket_name    = get_env(\"TERRAFORM_BACKEND_BUCKET_NAME\", local._merged_config.terraform_backend.bucket_name)\n      bucket_region  = get_env(\"TERRAFORM_BACKEND_BUCKET_REGION\", local._merged_config.terraform_backend.bucket_region)\n      dynamodb_table = get_env(\"TERRAFORM_BACKEND_DYNAMODB_TABLE\", local._merged_config.terraform_backend.dynamodb_table)\n    }\n\n    modules = {\n      terraform_backend = {\n        aws_provider = {\n          region = get_env(\n            \"MODULES_TERRAFORM_BACKEND_AWS_PROVIDER_REGION\",\n            try(local._merged_config.modules.terraform_backend.aws_provider.region, \"\")\n          )\n        }\n      }\n    }\n  }\n}\n\n# Remote state configuration for OTHER modules (not this bootstrap module)\nremote_state {\n  backend = \"s3\"\n  config = {\n    bucket         = local.project_vars.terraform_backend.bucket_name\n    key            = \"${path_relative_to_include()}/terraform.tfstate\"\n    region         = local.project_vars.terraform_backend.bucket_region\n    encrypt        = true\n    dynamodb_table = local.project_vars.terraform_backend.dynamodb_table\n  }\n  generate = {\n    path      = \"backend.tf\"\n    if_exists = \"overwrite_terragrunt\"\n  }\n}\n\n# Inputs inherited by all modules\ninputs = merge(\n  local.workspace_vars.inputs,\n  {\n    product     = local.project_vars.product\n    application = local.project_vars.application\n    environment = local.project_vars.environment\n  }\n)\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#step-5-create-the-configuration-file","title":"Step 5: Create the Configuration File","text":"<p>Create <code>config.yaml</code> with your project-specific settings:</p> <pre><code># Product identification\nproduct: &lt;product&gt;\napplication: infra\nenvironment: prod\n\n# Terraform backend configuration\n# These values will be populated after the first deployment\nterraform_backend:\n  bucket_name: 'sage-&lt;product&gt;-terraform-backend-use1-prod'\n  bucket_region: 'us-east-1'\n  dynamodb_table: 'sage-&lt;product&gt;-terraform-backend-lock-use1-prod'\n\n# Module-specific configuration\nmodules:\n  terraform_backend:\n    aws_provider:\n      region: 'us-east-1'\n</code></pre> <p>Replace <code>&lt;product&gt;</code> with your product name.</p>"},{"location":"develop/tutorials/terraform/create-backend/#step-6-create-the-module-configuration","title":"Step 6: Create the Module Configuration","text":"<p>Create <code>terraform-backend/terragrunt.hcl</code>:</p> <pre><code>include \"project\" {\n  path   = find_in_parent_folders(\"project.hcl\")\n  expose = true\n}\n\nlocals {\n  module_vars = include.project.locals.project_vars.modules.terraform_backend\n}\n\nterraform {\n  # Reference the shared reusable module\n  source = \"${include.project.inputs.project_paths.terraform}//modules/terraform-s3-backend\"\n}\n\ninputs = {\n  component = \"\"  # Empty for backend (not a sub-component)\n  region    = local.module_vars.aws_provider.region\n\n  # Module configuration\n  enabled                           = true\n  bucket_ownership_enforced_enabled = false\n  force_destroy                     = false  # Set to true only for testing\n  sse_encryption                    = \"aws:kms\"\n\n  # Optional: Override auto-generated names\n  # s3_bucket_name     = \"my-custom-bucket-name\"\n  # dynamodb_table_name = \"my-custom-table-name\"\n}\n\n# IMPORTANT: Bootstrap exception - use local backend\nremote_state {\n  backend = \"local\"\n  config = {\n    path = \"${get_terragrunt_dir()}/terraform.tfstate\"\n  }\n  generate = {\n    path      = \"backend.tf\"\n    if_exists = \"overwrite\"\n  }\n}\n\n# Generate AWS provider configuration\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite\"\n  contents  = &lt;&lt;EOF\nprovider \"aws\" {\n  region = \"${local.module_vars.aws_provider.region}\"\n}\nEOF\n}\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#step-7-create-documentation","title":"Step 7: Create Documentation","text":"<p>Create <code>README.md</code> documenting your project. You can reference the BixArena example at <code>apps/bixarena/infra/terraform/terraform-backend/README.md</code> in the monorepo for a complete template.</p> <p>Key sections to include:</p> <ul> <li>Overview of what the project deploys</li> <li>Prerequisites</li> <li>Available Nx commands</li> <li>State file management notes</li> <li>Troubleshooting tips</li> </ul>"},{"location":"develop/tutorials/terraform/create-backend/#step-8-initialize-the-project","title":"Step 8: Initialize the Project","text":"<p>Authenticate with AWS and initialize the Terraform project:</p> <pre><code># Login to AWS SSO\naws sso login --profile &lt;product&gt;-&lt;env&gt;-Developer\n\n# Initialize Terraform\nnx run &lt;product&gt;-infra-terraform-terraform-backend:init\n</code></pre> <p>This will:</p> <ul> <li>Download the AWS provider</li> <li>Initialize the local backend</li> <li>Prepare the working directory</li> </ul> <p>Terragrunt wraps OpenTofu (the <code>tofu</code> binary), so you'll see OpenTofu output.</p> <p>Expected output:</p> <pre><code>OpenTofu has been successfully initialized!\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#step-9-validate-configuration","title":"Step 9: Validate Configuration","text":"<p>Check your configuration for syntax errors:</p> <pre><code>nx run &lt;product&gt;-infra-terraform-terraform-backend:validate\n</code></pre> <p>Expected output:</p> <pre><code>Success! The configuration is valid.\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#step-10-review-the-plan","title":"Step 10: Review the Plan","text":"<p>Generate an execution plan to see what will be created:</p> <pre><code>nx run &lt;product&gt;-infra-terraform-terraform-backend:plan\n</code></pre> <p>Review the plan carefully. You should see:</p> <ul> <li>1 S3 bucket with versioning and encryption</li> <li>Multiple S3 bucket configuration resources (ACL, policy, etc.)</li> <li>1 DynamoDB table with point-in-time recovery</li> </ul> <p>Example output:</p> <pre><code>Plan: 8 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + dynamodb_table_arn  = (known after apply)\n  + dynamodb_table_name = (known after apply)\n  + s3_bucket_arn      = (known after apply)\n  + s3_bucket_id       = (known after apply)\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#step-11-deploy-the-backend","title":"Step 11: Deploy the Backend","text":"<p>Apply the changes to create resources in AWS:</p> <pre><code>nx run &lt;product&gt;-infra-terraform-terraform-backend:deploy\n</code></pre> <p>Type <code>yes</code> when prompted to confirm.</p> <p>Deployment time: ~30-60 seconds</p>"},{"location":"develop/tutorials/terraform/create-backend/#step-12-capture-outputs","title":"Step 12: Capture Outputs","text":"<p>After successful deployment, OpenTofu displays output values:</p> <pre><code>Outputs:\n\ndynamodb_table_arn = \"arn:aws:dynamodb:us-east-1:123456789012:table/sage-product-terraform-backend-lock-use1-prod\"\ndynamodb_table_name = \"sage-product-terraform-backend-lock-use1-prod\"\ns3_bucket_arn = \"arn:aws:s3:::sage-product-terraform-backend-use1-prod\"\ns3_bucket_id = \"sage-product-terraform-backend-use1-prod\"\ns3_bucket_region = \"us-east-1\"\n</code></pre> <p>Important: Update your <code>config.yaml</code> with these values so other projects can reference this backend.</p>"},{"location":"develop/tutorials/terraform/create-backend/#step-13-verify-in-aws-console","title":"Step 13: Verify in AWS Console","text":"<ol> <li>Navigate to AWS Console \u2192 S3</li> <li>Find your bucket: <code>sage-&lt;product&gt;-terraform-backend-use1-prod</code></li> <li>Verify:<ul> <li>\u2705 Versioning is enabled</li> <li>\u2705 Default encryption is enabled (SSE-KMS)</li> <li>\u2705 Public access is blocked</li> </ul> </li> <li>Navigate to DynamoDB</li> <li>Find your table: <code>sage-&lt;product&gt;-terraform-backend-lock-use1-prod</code></li> <li>Verify:<ul> <li>\u2705 Table exists with <code>LockID</code> hash key</li> <li>\u2705 Point-in-time recovery is enabled</li> </ul> </li> </ol>"},{"location":"develop/tutorials/terraform/create-backend/#step-14-secure-the-state-file","title":"Step 14: Secure the State File","text":"<p>The local state file is located at:</p> <pre><code>apps/&lt;product&gt;/infra/terraform/terraform-backend/terraform-backend/terraform.tfstate\n</code></pre> <p>Critical Security Steps:</p> <ol> <li>\u2705 Verify it's excluded from Git (already in <code>.gitignore</code>)</li> <li>\ud83d\udce6 Back it up to a secure location:<ul> <li>1Password vault</li> <li>AWS Secrets Manager</li> <li>Secure cloud storage</li> </ul> </li> <li>\ud83d\udd12 Restrict access (only infrastructure team)</li> </ol> <p>Why it matters: If this file is lost, you'll need to manually import resources or redeploy.</p>"},{"location":"develop/tutorials/terraform/create-backend/#step-15-test-with-a-dependent-module-optional","title":"Step 15: Test with a Dependent Module (Optional)","text":"<p>Create a test module to verify the backend works:</p> <pre><code># Create a test module\nmkdir -p ../test-module\ncd ../test-module\n\n# Create terragrunt.hcl that uses the remote backend\ncat &gt; terragrunt.hcl &lt;&lt;'EOF'\ninclude \"project\" {\n  path = find_in_parent_folders(\"project.hcl\")\n}\n\nterraform {\n  source = \"tfr:///terraform-aws-modules/s3-bucket/aws?version=3.15.0\"\n}\n\ninputs = {\n  bucket = \"test-remote-state-${get_env(\"USER\", \"demo\")}\"\n  tags = {\n    Test = \"true\"\n  }\n}\nEOF\n\n# Initialize - should use the remote S3 backend\nterragrunt init\n\n# Check the backend configuration\ncat .terragrunt-cache/.../backend.tf\n</code></pre> <p>You should see S3 backend configuration pointing to your new bucket.</p>"},{"location":"develop/tutorials/terraform/create-backend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"develop/tutorials/terraform/create-backend/#error-backend-configuration-has-changed","title":"Error: \"backend configuration has changed\"","text":"<p>Cause: Backend settings were modified.</p> <p>Solution:</p> <pre><code>nx run &lt;product&gt;-infra-terraform-terraform-backend:init\n# Answer 'yes' to reconfigure\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#error-failed-to-get-existing-workspaces","title":"Error: \"Failed to get existing workspaces\"","text":"<p>Cause: AWS credentials expired or invalid.</p> <p>Solution:</p> <pre><code>aws sso login --profile &lt;product&gt;-&lt;env&gt;-Developer\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#error-accessdenied-access-denied","title":"Error: \"AccessDenied: Access Denied\"","text":"<p>Cause: IAM permissions insufficient.</p> <p>Solution: Ensure your AWS profile has permissions for:</p> <ul> <li><code>s3:CreateBucket</code>, <code>s3:PutBucketPolicy</code>, <code>s3:PutEncryptionConfiguration</code></li> <li><code>dynamodb:CreateTable</code>, <code>dynamodb:UpdateTable</code></li> </ul>"},{"location":"develop/tutorials/terraform/create-backend/#resources-already-exist","title":"Resources Already Exist","text":"<p>Cause: Resources were created outside of this Terraform project or in a previous run.</p> <p>Solution: Import existing resources:</p> <pre><code>cd terraform-backend\nterragrunt import aws_s3_bucket.default &lt;bucket-name&gt;\nterragrunt import aws_dynamodb_table.with_server_side_encryption &lt;table-name&gt;\n</code></pre>"},{"location":"develop/tutorials/terraform/create-backend/#next-steps","title":"Next Steps","text":"<p>Now that you have a Terraform backend:</p> <ol> <li>Create additional infrastructure modules that use this remote backend</li> <li>Learn to create reusable modules - See Create a Reusable Module</li> <li>Set up CI/CD for automated deployments</li> <li>Add monitoring with CloudWatch alarms for state access</li> </ol>"},{"location":"develop/tutorials/terraform/create-backend/#best-practices","title":"Best Practices","text":"<ul> <li>\u2705 One backend per product - Don't share backends across products</li> <li>\u2705 Separate environments - Create separate backends for dev/staging/prod</li> <li>\u2705 Version control - Always commit your Terraform project code, never the state files</li> <li>\u2705 Review plans - Never run <code>apply</code> without reviewing the plan first</li> <li>\u2705 Use workspaces carefully - For multi-environment, prefer separate backends</li> </ul>"},{"location":"develop/tutorials/terraform/create-backend/#related-resources","title":"Related Resources","text":"<ul> <li>Terraform Infrastructure Architecture</li> <li>OpenTofu S3 Backend Docs</li> <li>Terraform S3 Backend Docs (compatible)</li> </ul>"},{"location":"develop/tutorials/terraform/create-backend/#example-code-in-the-monorepo","title":"Example Code in the Monorepo","text":"<ul> <li>Reusable Module: <code>libs/platform/infra/terraform/modules/terraform-s3-backend/</code></li> <li>Example Project: <code>apps/bixarena/infra/terraform/terraform-backend/</code></li> </ul>"},{"location":"develop/tutorials/terraform/create-reusable-module/","title":"Tutorial: Create a Reusable Terraform Module","text":""},{"location":"develop/tutorials/terraform/create-reusable-module/#overview","title":"Overview","text":"<p>This tutorial guides you through creating a reusable Terraform module that can be shared across multiple Terraform projects in the monorepo. You'll learn the module pattern, best practices, and how to integrate with the resource-label system.</p> <p>What is a Terraform Module?</p> <p>In this context, a Terraform module is a reusable collection of Terraform/OpenTofu resources (<code>.tf</code> files) that can be used across multiple Terraform projects. Don't confuse this with Terragrunt modules (which are directories containing <code>terragrunt.hcl</code> files that reference Terraform modules).</p> <p>What you'll build:</p> <ul> <li>A reusable VPC Terraform module with configurable parameters</li> <li>Proper input validation and outputs</li> <li>Integration with the resource-label naming convention</li> <li>Documentation for module consumers</li> </ul> <p>Time required: 45-60 minutes</p> <p>Prerequisites:</p> <ul> <li>Understanding of Infrastructure as Code concepts</li> <li>Completed the Create a Terraform Backend tutorial</li> <li>Familiarity with AWS VPC concepts</li> <li>OpenTofu/Terraform module basics</li> </ul>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-1-understand-reusable-module-principles","title":"Step 1: Understand Reusable Module Principles","text":"<p>Reusable modules in the monorepo follow these principles:</p> <ol> <li>Generic and Parameterized - Work for any product/environment</li> <li>Secure by Default - Security best practices built-in</li> <li>Consistent Naming - Use resource-label for all resources</li> <li>Well-Documented - Clear inputs, outputs, and examples</li> <li>Validated - Input validation prevents misconfigurations</li> <li>Testable - Can be deployed independently for testing</li> </ol>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-2-choose-module-scope","title":"Step 2: Choose Module Scope","text":"<p>Decide if your module should be:</p> <p>Shared Module (<code>libs/platform/infra/terraform/modules/</code>):</p> <ul> <li>Generic, reusable across products</li> <li>Examples: VPC, ECS cluster, RDS database, ALB</li> <li>Requires comprehensive documentation</li> </ul> <p>Project Module (<code>apps/&lt;product&gt;/infra/terraform/&lt;project&gt;/modules/</code>):</p> <ul> <li>Product-specific configuration</li> <li>Examples: Custom API service, specific worker configuration</li> <li>Can be simpler, less documentation needed</li> </ul> <p>For this tutorial, we'll create a shared VPC module.</p>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-3-create-module-directory-structure","title":"Step 3: Create Module Directory Structure","text":"<p>Create the module directory:</p> <pre><code>cd libs/platform/infra/terraform/modules/\nmkdir vpc\ncd vpc\n</code></pre> <p>Create the standard Terraform files:</p> <pre><code>touch main.tf\ntouch variables.tf\ntouch outputs.tf\ntouch versions.tf\ntouch context.tf\ntouch README.md\n</code></pre> <p>Your structure:</p> <pre><code>libs/platform/infra/terraform/modules/vpc/\n\u251c\u2500\u2500 main.tf         # Resource definitions\n\u251c\u2500\u2500 variables.tf    # Input variables\n\u251c\u2500\u2500 outputs.tf      # Output values\n\u251c\u2500\u2500 versions.tf     # Provider requirements\n\u251c\u2500\u2500 context.tf      # Resource label integration\n\u2514\u2500\u2500 README.md       # Documentation\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-4-define-provider-requirements","title":"Step 4: Define Provider Requirements","text":"<p>Create <code>versions.tf</code>:</p> <pre><code>terraform {\n  required_version = \"&gt;= 1.8.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"= 5.88.0\"\n    }\n  }\n}\n</code></pre> <p>Note:</p> <ul> <li>Use exact version pinning (<code>= 5.88.0</code>) for reproducibility</li> <li>This works with both OpenTofu and Terraform (syntax compatible)</li> <li>The monorepo uses OpenTofu 1.10.6, which is compatible with Terraform 1.8.0+</li> </ul>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-5-integrate-resource-label","title":"Step 5: Integrate Resource Label","text":"<p>Create <code>context.tf</code> to integrate the naming convention:</p> <pre><code>module \"this\" {\n  source = \"${var.project_paths.terraform}//modules/resource-label\"\n\n  enabled             = var.enabled\n  organization        = var.organization\n  product             = var.product\n  application         = var.application\n  component           = var.component\n  attributes          = var.attributes\n  region              = var.region\n  environment         = var.environment\n  delimiter           = var.delimiter\n  tags                = var.tags\n  label_order         = var.label_order\n  regex_replace_chars = var.regex_replace_chars\n  id_length_limit     = var.id_length_limit\n  label_key_case      = var.label_key_case\n  label_value_case    = var.label_value_case\n  labels_as_tags      = var.labels_as_tags\n  project_paths       = var.project_paths\n\n  context = var.context\n}\n\n# Copy the full context variables from resource-label/variables.tf\n# (All the variable definitions for organization, product, etc.)\n# This pattern allows the module to be used with or without context\n\nvariable \"context\" {\n  type = any\n  default = {\n    enabled             = true\n    organization        = null\n    product             = null\n    application         = null\n    component           = null\n    attributes          = []\n    region              = null\n    environment         = null\n    delimiter           = null\n    tags                = {}\n    regex_replace_chars = null\n    label_order         = []\n    id_length_limit     = null\n    label_key_case      = null\n    label_value_case    = null\n    labels_as_tags      = [\"unset\"]\n  }\n  description = \"Single object for setting entire context at once.\"\n}\n\nvariable \"enabled\" {\n  type        = bool\n  default     = null\n  description = \"Set to false to prevent the module from creating any resources.\"\n}\n\nvariable \"organization\" {\n  type        = string\n  default     = null\n  description = \"Organization name (e.g., 'sage').\"\n}\n\nvariable \"product\" {\n  type        = string\n  default     = null\n  description = \"Product name (e.g., 'bixarena').\"\n}\n\nvariable \"application\" {\n  type        = string\n  default     = null\n  description = \"Application name (e.g., 'api').\"\n}\n\nvariable \"component\" {\n  type        = string\n  default     = null\n  description = \"Component name (e.g., 'vpc', 'alb').\"\n}\n\nvariable \"attributes\" {\n  type        = list(string)\n  default     = []\n  description = \"Additional attributes to add to resource names.\"\n}\n\nvariable \"region\" {\n  type        = string\n  default     = null\n  description = \"AWS region (e.g., 'us-east-1').\"\n}\n\nvariable \"environment\" {\n  type        = string\n  default     = null\n  description = \"Environment (e.g., 'dev', 'staging', 'prod').\"\n}\n\nvariable \"delimiter\" {\n  type        = string\n  default     = null\n  description = \"Delimiter between ID elements.\"\n}\n\nvariable \"tags\" {\n  type        = map(string)\n  default     = {}\n  description = \"Additional tags for resources.\"\n}\n\nvariable \"label_order\" {\n  type        = list(string)\n  default     = null\n  description = \"Order of labels in resource IDs.\"\n}\n\nvariable \"regex_replace_chars\" {\n  type        = string\n  default     = null\n  description = \"Regex to remove characters from IDs.\"\n}\n\nvariable \"id_length_limit\" {\n  type        = number\n  default     = null\n  description = \"Limit ID length (0 for unlimited).\"\n}\n\nvariable \"label_key_case\" {\n  type        = string\n  default     = null\n  description = \"Letter case for tag keys (lower/title/upper).\"\n}\n\nvariable \"label_value_case\" {\n  type        = string\n  default     = null\n  description = \"Letter case for label values (lower/title/upper/none).\"\n}\n\nvariable \"labels_as_tags\" {\n  type        = set(string)\n  default     = [\"default\"]\n  description = \"Labels to include as tags.\"\n}\n\nvariable \"project_paths\" {\n  type        = map(string)\n  default     = null\n  description = \"Mapping of project paths.\"\n}\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-6-define-module-variables","title":"Step 6: Define Module Variables","text":"<p>Create <code>variables.tf</code> with module-specific inputs:</p> <pre><code># VPC Configuration\nvariable \"cidr_block\" {\n  type        = string\n  description = \"CIDR block for the VPC (e.g., '10.0.0.0/16').\"\n\n  validation {\n    condition     = can(cidrhost(var.cidr_block, 0))\n    error_message = \"Must be a valid CIDR block.\"\n  }\n}\n\nvariable \"availability_zones\" {\n  type        = list(string)\n  description = \"List of availability zones for subnets (e.g., ['us-east-1a', 'us-east-1b']).\"\n\n  validation {\n    condition     = length(var.availability_zones) &gt;= 2\n    error_message = \"Must specify at least 2 availability zones for high availability.\"\n  }\n}\n\nvariable \"public_subnet_cidrs\" {\n  type        = list(string)\n  default     = []\n  description = \"CIDR blocks for public subnets (one per AZ).\"\n}\n\nvariable \"private_subnet_cidrs\" {\n  type        = list(string)\n  default     = []\n  description = \"CIDR blocks for private subnets (one per AZ).\"\n}\n\nvariable \"enable_nat_gateway\" {\n  type        = bool\n  default     = true\n  description = \"Enable NAT Gateway for private subnets.\"\n}\n\nvariable \"single_nat_gateway\" {\n  type        = bool\n  default     = false\n  description = \"Use a single NAT Gateway instead of one per AZ (cost optimization).\"\n}\n\nvariable \"enable_dns_hostnames\" {\n  type        = bool\n  default     = true\n  description = \"Enable DNS hostnames in the VPC.\"\n}\n\nvariable \"enable_dns_support\" {\n  type        = bool\n  default     = true\n  description = \"Enable DNS support in the VPC.\"\n}\n\nvariable \"enable_flow_logs\" {\n  type        = bool\n  default     = true\n  description = \"Enable VPC Flow Logs for network traffic analysis.\"\n}\n\nvariable \"flow_logs_retention_days\" {\n  type        = number\n  default     = 7\n  description = \"Number of days to retain flow logs.\"\n\n  validation {\n    condition     = contains([1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653], var.flow_logs_retention_days)\n    error_message = \"Must be a valid CloudWatch Logs retention period.\"\n  }\n}\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-7-implement-main-resources","title":"Step 7: Implement Main Resources","text":"<p>Create <code>main.tf</code>:</p> <pre><code>locals {\n  enabled = module.this.enabled\n\n  # Calculate subnet CIDRs if not provided\n  public_subnet_cidrs = length(var.public_subnet_cidrs) &gt; 0 ? var.public_subnet_cidrs : [\n    for i in range(length(var.availability_zones)) :\n    cidrsubnet(var.cidr_block, 8, i)\n  ]\n\n  private_subnet_cidrs = length(var.private_subnet_cidrs) &gt; 0 ? var.private_subnet_cidrs : [\n    for i in range(length(var.availability_zones)) :\n    cidrsubnet(var.cidr_block, 8, i + 100)\n  ]\n\n  nat_gateway_count = var.enable_nat_gateway ? (var.single_nat_gateway ? 1 : length(var.availability_zones)) : 0\n}\n\n# VPC Resource\nresource \"aws_vpc\" \"this\" {\n  count = local.enabled ? 1 : 0\n\n  cidr_block           = var.cidr_block\n  enable_dns_hostnames = var.enable_dns_hostnames\n  enable_dns_support   = var.enable_dns_support\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = module.this.id\n    }\n  )\n}\n\n# Internet Gateway\nresource \"aws_internet_gateway\" \"this\" {\n  count = local.enabled ? 1 : 0\n\n  vpc_id = one(aws_vpc.this[*].id)\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-igw\"\n    }\n  )\n}\n\n# Public Subnets\nresource \"aws_subnet\" \"public\" {\n  count = local.enabled ? length(var.availability_zones) : 0\n\n  vpc_id                  = one(aws_vpc.this[*].id)\n  cidr_block              = local.public_subnet_cidrs[count.index]\n  availability_zone       = var.availability_zones[count.index]\n  map_public_ip_on_launch = true\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-public-${var.availability_zones[count.index]}\"\n      Type = \"public\"\n    }\n  )\n}\n\n# Private Subnets\nresource \"aws_subnet\" \"private\" {\n  count = local.enabled ? length(var.availability_zones) : 0\n\n  vpc_id            = one(aws_vpc.this[*].id)\n  cidr_block        = local.private_subnet_cidrs[count.index]\n  availability_zone = var.availability_zones[count.index]\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-private-${var.availability_zones[count.index]}\"\n      Type = \"private\"\n    }\n  )\n}\n\n# Elastic IPs for NAT Gateways\nresource \"aws_eip\" \"nat\" {\n  count = local.nat_gateway_count\n\n  domain = \"vpc\"\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-nat-${count.index + 1}\"\n    }\n  )\n\n  depends_on = [aws_internet_gateway.this]\n}\n\n# NAT Gateways\nresource \"aws_nat_gateway\" \"this\" {\n  count = local.nat_gateway_count\n\n  allocation_id = aws_eip.nat[count.index].id\n  subnet_id     = aws_subnet.public[count.index].id\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-nat-${count.index + 1}\"\n    }\n  )\n\n  depends_on = [aws_internet_gateway.this]\n}\n\n# Public Route Table\nresource \"aws_route_table\" \"public\" {\n  count = local.enabled ? 1 : 0\n\n  vpc_id = one(aws_vpc.this[*].id)\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-public\"\n      Type = \"public\"\n    }\n  )\n}\n\n# Public Route to Internet Gateway\nresource \"aws_route\" \"public_internet_gateway\" {\n  count = local.enabled ? 1 : 0\n\n  route_table_id         = one(aws_route_table.public[*].id)\n  destination_cidr_block = \"0.0.0.0/0\"\n  gateway_id             = one(aws_internet_gateway.this[*].id)\n}\n\n# Associate Public Subnets with Public Route Table\nresource \"aws_route_table_association\" \"public\" {\n  count = local.enabled ? length(var.availability_zones) : 0\n\n  subnet_id      = aws_subnet.public[count.index].id\n  route_table_id = one(aws_route_table.public[*].id)\n}\n\n# Private Route Tables (one per NAT Gateway)\nresource \"aws_route_table\" \"private\" {\n  count = local.enabled ? local.nat_gateway_count : 0\n\n  vpc_id = one(aws_vpc.this[*].id)\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-private-${count.index + 1}\"\n      Type = \"private\"\n    }\n  )\n}\n\n# Private Routes to NAT Gateways\nresource \"aws_route\" \"private_nat_gateway\" {\n  count = local.enabled ? local.nat_gateway_count : 0\n\n  route_table_id         = aws_route_table.private[count.index].id\n  destination_cidr_block = \"0.0.0.0/0\"\n  nat_gateway_id         = aws_nat_gateway.this[count.index].id\n}\n\n# Associate Private Subnets with Private Route Tables\nresource \"aws_route_table_association\" \"private\" {\n  count = local.enabled ? length(var.availability_zones) : 0\n\n  subnet_id      = aws_subnet.private[count.index].id\n  route_table_id = aws_route_table.private[var.single_nat_gateway ? 0 : count.index].id\n}\n\n# VPC Flow Logs\nresource \"aws_flow_log\" \"this\" {\n  count = local.enabled &amp;&amp; var.enable_flow_logs ? 1 : 0\n\n  vpc_id          = one(aws_vpc.this[*].id)\n  traffic_type    = \"ALL\"\n  iam_role_arn    = one(aws_iam_role.flow_logs[*].arn)\n  log_destination = one(aws_cloudwatch_log_group.flow_logs[*].arn)\n\n  tags = merge(\n    module.this.tags,\n    {\n      Name = \"${module.this.id}-flow-logs\"\n    }\n  )\n}\n\n# CloudWatch Log Group for Flow Logs\nresource \"aws_cloudwatch_log_group\" \"flow_logs\" {\n  count = local.enabled &amp;&amp; var.enable_flow_logs ? 1 : 0\n\n  name              = \"/aws/vpc/${module.this.id}\"\n  retention_in_days = var.flow_logs_retention_days\n\n  tags = module.this.tags\n}\n\n# IAM Role for Flow Logs\nresource \"aws_iam_role\" \"flow_logs\" {\n  count = local.enabled &amp;&amp; var.enable_flow_logs ? 1 : 0\n\n  name = \"${module.this.id}-flow-logs\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"vpc-flow-logs.amazonaws.com\"\n      }\n    }]\n  })\n\n  tags = module.this.tags\n}\n\n# IAM Policy for Flow Logs\nresource \"aws_iam_role_policy\" \"flow_logs\" {\n  count = local.enabled &amp;&amp; var.enable_flow_logs ? 1 : 0\n\n  name = \"${module.this.id}-flow-logs\"\n  role = one(aws_iam_role.flow_logs[*].id)\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\",\n        \"logs:DescribeLogGroups\",\n        \"logs:DescribeLogStreams\"\n      ]\n      Effect = \"Allow\"\n      Resource = \"*\"\n    }]\n  })\n}\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-8-define-outputs","title":"Step 8: Define Outputs","text":"<p>Create <code>outputs.tf</code>:</p> <pre><code>output \"vpc_id\" {\n  value       = one(aws_vpc.this[*].id)\n  description = \"VPC ID.\"\n}\n\noutput \"vpc_arn\" {\n  value       = one(aws_vpc.this[*].arn)\n  description = \"VPC ARN.\"\n}\n\noutput \"vpc_cidr_block\" {\n  value       = one(aws_vpc.this[*].cidr_block)\n  description = \"VPC CIDR block.\"\n}\n\noutput \"public_subnet_ids\" {\n  value       = aws_subnet.public[*].id\n  description = \"List of public subnet IDs.\"\n}\n\noutput \"private_subnet_ids\" {\n  value       = aws_subnet.private[*].id\n  description = \"List of private subnet IDs.\"\n}\n\noutput \"public_subnet_cidrs\" {\n  value       = aws_subnet.public[*].cidr_block\n  description = \"List of public subnet CIDR blocks.\"\n}\n\noutput \"private_subnet_cidrs\" {\n  value       = aws_subnet.private[*].cidr_block\n  description = \"List of private subnet CIDR blocks.\"\n}\n\noutput \"nat_gateway_ids\" {\n  value       = aws_nat_gateway.this[*].id\n  description = \"List of NAT Gateway IDs.\"\n}\n\noutput \"internet_gateway_id\" {\n  value       = one(aws_internet_gateway.this[*].id)\n  description = \"Internet Gateway ID.\"\n}\n\noutput \"public_route_table_id\" {\n  value       = one(aws_route_table.public[*].id)\n  description = \"Public route table ID.\"\n}\n\noutput \"private_route_table_ids\" {\n  value       = aws_route_table.private[*].id\n  description = \"List of private route table IDs.\"\n}\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-9-document-the-module","title":"Step 9: Document the Module","text":"<p>Create <code>README.md</code>:</p> <pre><code># VPC Terraform Module\n\n## Overview\n\nThis module creates a production-ready AWS VPC with public and private subnets across multiple availability zones, NAT gateways, and optional VPC Flow Logs.\n\n## Features\n\n- Multi-AZ architecture for high availability\n- Public and private subnets\n- NAT Gateways for private subnet internet access\n- Internet Gateway for public subnets\n- VPC Flow Logs for network monitoring\n- Consistent naming via resource-label\n- Fully configurable and validated inputs\n\n## Usage\n\n```hcl\nmodule \"vpc\" {\n  source = \"${path_to_terraform}//modules/vpc\"\n\n  # Context (inherited from project)\n  organization = \"sage\"\n  product      = \"bixarena\"\n  application  = \"api\"\n  component    = \"network\"\n  region       = \"us-east-1\"\n  environment  = \"prod\"\n\n  # VPC Configuration\n  cidr_block         = \"10.0.0.0/16\"\n  availability_zones = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n\n  # Optional: Override calculated subnet CIDRs\n  # public_subnet_cidrs  = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  # private_subnet_cidrs = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n\n  # NAT Gateway Configuration\n  enable_nat_gateway = true\n  single_nat_gateway = false  # Use one NAT per AZ for HA\n\n  # Flow Logs\n  enable_flow_logs           = true\n  flow_logs_retention_days   = 7\n}\n```\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#inputs","title":"Inputs","text":"Name Type Default Description cidr_block string - CIDR block for the VPC availability_zones list(string) - List of AZs (minimum 2) public_subnet_cidrs list(string) calculated Public subnet CIDRs private_subnet_cidrs list(string) calculated Private subnet CIDRs enable_nat_gateway bool true Enable NAT Gateway single_nat_gateway bool false Use single NAT (cost savings) enable_flow_logs bool true Enable VPC Flow Logs"},{"location":"develop/tutorials/terraform/create-reusable-module/#outputs","title":"Outputs","text":"Name Description vpc_id VPC ID public_subnet_ids Public subnet IDs private_subnet_ids Private subnet IDs nat_gateway_ids NAT Gateway IDs"},{"location":"develop/tutorials/terraform/create-reusable-module/#examples","title":"Examples","text":"<p>See examples for usage.</p> <pre><code>## Step 10: Test the Module\n\nCreate a test Terragrunt module to verify:\n\n```bash\ncd apps/bixarena/infra/terraform/\nmkdir -p test-vpc/network\ncd test-vpc\n</code></pre> <p>Create <code>test-vpc/network/terragrunt.hcl</code>:</p> <pre><code>include \"project\" {\n  path = \"../../../terraform-backend/project.hcl\"  # Reuse existing project\n}\n\nterraform {\n  source = \"${include.project.inputs.project_paths.terraform}//modules/vpc\"\n}\n\ninputs = {\n  component          = \"test-network\"\n  cidr_block         = \"10.100.0.0/16\"\n  availability_zones = [\"us-east-1a\", \"us-east-1b\"]\n  single_nat_gateway = true  # Cost optimization for testing\n}\n</code></pre> <p>Test the module:</p> <pre><code>cd network\nterragrunt init\nterragrunt plan\n# terragrunt apply  # Only if you want to create real resources\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#step-11-add-to-version-control","title":"Step 11: Add to Version Control","text":"<pre><code>cd /workspaces/sage-monorepo\ngit add libs/platform/infra/terraform/modules/vpc/\ngit commit -m \"feat(terraform): add reusable VPC module\"\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#best-practices-checklist","title":"Best Practices Checklist","text":"<ul> <li>\u2705 Input validation for all critical parameters</li> <li>\u2705 Sensible defaults for optional parameters</li> <li>\u2705 Integration with resource-label for naming</li> <li>\u2705 Security best practices (private subnets, flow logs)</li> <li>\u2705 Cost optimization options (single NAT gateway)</li> <li>\u2705 Comprehensive outputs for downstream modules</li> <li>\u2705 Clear documentation with examples</li> <li>\u2705 Conditional resource creation (<code>count</code> with <code>enabled</code>)</li> </ul>"},{"location":"develop/tutorials/terraform/create-reusable-module/#common-patterns","title":"Common Patterns","text":""},{"location":"develop/tutorials/terraform/create-reusable-module/#pattern-1-conditional-resources","title":"Pattern 1: Conditional Resources","text":"<pre><code>resource \"aws_nat_gateway\" \"this\" {\n  count = var.enable_nat_gateway ? length(var.availability_zones) : 0\n  # ...\n}\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#pattern-2-calculated-values","title":"Pattern 2: Calculated Values","text":"<pre><code>locals {\n  public_subnet_cidrs = length(var.public_subnet_cidrs) &gt; 0 ? var.public_subnet_cidrs : [\n    for i in range(length(var.availability_zones)) :\n    cidrsubnet(var.cidr_block, 8, i)\n  ]\n}\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#pattern-3-dynamic-tagging","title":"Pattern 3: Dynamic Tagging","text":"<pre><code>tags = merge(\n  module.this.tags,  # Standard tags from resource-label\n  {\n    Name = \"${module.this.id}-custom\"  # Resource-specific name\n  }\n)\n</code></pre>"},{"location":"develop/tutorials/terraform/create-reusable-module/#next-steps","title":"Next Steps","text":"<ul> <li>Create additional shared modules (ALB, ECS, RDS)</li> <li>Add automated testing (Terratest)</li> <li>Document module in team wiki</li> <li>Create example implementations</li> </ul>"},{"location":"develop/tutorials/terraform/create-reusable-module/#related-resources","title":"Related Resources","text":"<ul> <li>Terraform Infrastructure Architecture</li> <li>AWS VPC Documentation</li> </ul>"},{"location":"develop/tutorials/terraform/create-reusable-module/#example-code-in-the-monorepo","title":"Example Code in the Monorepo","text":"<ul> <li>Resource Label Module: <code>libs/platform/infra/terraform/modules/resource-label/</code></li> <li>Terraform Backend Module: <code>libs/platform/infra/terraform/modules/terraform-s3-backend/</code> - Another reusable module example</li> </ul>"},{"location":"products/agora/","title":"Agora","text":"<p>Content coming soon!</p>"},{"location":"products/openchallenges/","title":"OpenChallenges","text":"<p>Content coming soon!</p>"},{"location":"products/services/","title":"Service Catalog","text":"<p>This page provides an overview of all services and applications in the Sage Monorepo.</p> <p>Last updated: 2025-08-23T22:04:00.590Z</p>"},{"location":"products/services/#overview","title":"Overview","text":"<ul> <li>Total Projects: 145</li> <li>Applications: 61</li> <li>Libraries: 84</li> </ul>"},{"location":"products/services/#by-language","title":"By Language","text":"<ul> <li>typescript: 71 projects</li> <li>unknown: 41 projects</li> <li>java: 14 projects</li> <li>python: 12 projects</li> <li>openapi: 6 projects</li> <li>javascript: 1 projects</li> </ul>"},{"location":"products/services/#by-scope","title":"By Scope","text":"<ul> <li>backend: 41 projects</li> <li>shared: 36 projects</li> <li>agora: 20 projects</li> <li>openchallenges: 16 projects</li> <li>model-ad: 13 projects</li> <li>explorers: 10 projects</li> <li>monorepo: 3 projects</li> <li>results-visualization-framework: 1 projects</li> <li>bixarena: 1 projects</li> <li>frontend: 1 projects</li> <li>admin: 1 projects</li> <li>client: 1 projects</li> </ul>"},{"location":"products/services/#admin-projects","title":"Admin Projects","text":""},{"location":"products/services/#applications","title":"Applications","text":""},{"location":"products/services/#openchallenges-infra","title":"openchallenges-infra","text":"<p>This project describe how to deploy the OpenChallenges stack with [Terraform CDK].</p> <ul> <li>Language: unknown</li> <li>Location: <code>apps/openchallenges/infra</code></li> <li>Available Tasks: build, lint, lint-fix, test</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-projects","title":"Agora Projects","text":""},{"location":"products/services/#libraries","title":"Libraries","text":""},{"location":"products/services/#agora-about","title":"agora-about","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/about</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-api-client-angular","title":"agora-api-client-angular","text":"<ul> <li>Type: unknown</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/api-client-angular</code></li> <li>Available Tasks: test, generate</li> </ul>"},{"location":"products/services/#agora-api-description","title":"agora-api-description","text":"<ul> <li>Type: unknown</li> <li>Language: openapi</li> <li>Location: <code>libs/agora/api-description</code></li> <li>Available Tasks: build-individuals, build, lint, clean</li> </ul>"},{"location":"products/services/#agora-charts","title":"agora-charts","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/charts</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-config","title":"agora-config","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/config</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-gene-comparison-tool","title":"agora-gene-comparison-tool","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/gene-comparison-tool</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-genes","title":"agora-genes","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/genes</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-home","title":"agora-home","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/home</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-news","title":"agora-news","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/news</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-nominated-targets","title":"agora-nominated-targets","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/nominated-targets</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-not-found","title":"agora-not-found","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/not-found</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-services","title":"agora-services","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: services</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/services</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-shared","title":"agora-shared","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/shared</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-storybook","title":"agora-storybook","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: docs</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/storybook</code></li> <li>Available Tasks: lint, storybook, build-storybook, static-storybook</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-styles","title":"agora-styles","text":"<ul> <li>Type: styles</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/styles</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-teams","title":"agora-teams","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/teams</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-testing","title":"agora-testing","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/testing</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-themes","title":"agora-themes","text":"<ul> <li>Type: themes</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/themes</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-ui","title":"agora-ui","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/ui</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-util","title":"agora-util","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: util</li> <li>Language: typescript</li> <li>Location: <code>libs/agora/util</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#backend-projects","title":"Backend Projects","text":""},{"location":"products/services/#applications_1","title":"Applications","text":""},{"location":"products/services/#agora-apex","title":"agora-apex","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/agora/apex</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#agora-api-docs","title":"agora-api-docs","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/agora/api-docs</code></li> <li>Available Tasks: create-config, build, serve, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#agora-api-next","title":"agora-api-next","text":"<ul> <li>Language: java</li> <li>Location: <code>tmp/api-next-tmp</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image, generate</li> </ul>"},{"location":"products/services/#agora-data","title":"agora-data","text":"<p>This Python project downloads an Agora data release from Synapse, then seeds and indexes the data in</p> <ul> <li>Language: python</li> <li>Location: <code>apps/agora/data</code></li> <li>Available Tasks: create-config, prepare, serve, serve-detach, scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#agora-mongo","title":"agora-mongo","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/agora/mongo</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#bixarena-apex","title":"bixarena-apex","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/bixarena/apex</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#bixarena-api","title":"bixarena-api","text":"<ul> <li>Language: java</li> <li>Location: <code>apps/bixarena/api</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image, generate</li> </ul>"},{"location":"products/services/#bixarena-app","title":"bixarena-app","text":"<ul> <li>Language: python</li> <li>Location: <code>apps/bixarena/app</code></li> <li>Available Tasks: create-config, prepare, show-packages, serve, serve-detach, serve-demo, scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#bixarena-postgres","title":"bixarena-postgres","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/bixarena/postgres</code></li> <li>Available Tasks: create-config, serve-detach</li> </ul>"},{"location":"products/services/#iatlas-postgres","title":"iatlas-postgres","text":"<p>This project provides a containerized PostgreSQL database for the iAtlas application.</p> <ul> <li>Language: unknown</li> <li>Location: <code>apps/iatlas/postgres</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-apex","title":"model-ad-apex","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/model-ad/apex</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#model-ad-api","title":"model-ad-api","text":"<ul> <li>Language: typescript</li> <li>Location: <code>apps/model-ad/api</code></li> <li>Available Tasks: create-config, build, serve, serve-detach, lint, lint-fix, scan-image, test</li> </ul>"},{"location":"products/services/#model-ad-api-docs","title":"model-ad-api-docs","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/model-ad/api-docs</code></li> <li>Available Tasks: create-config, build, serve, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#model-ad-data","title":"model-ad-data","text":"<p>This Python project downloads a Model-AD data release from Synapse, then seeds and indexes the data in</p> <ul> <li>Language: python</li> <li>Location: <code>apps/model-ad/data</code></li> <li>Available Tasks: create-config, prepare, serve, serve-detach, scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-mongo","title":"model-ad-mongo","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/model-ad/mongo</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#observability-apex","title":"observability-apex","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/observability/apex</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#observability-grafana","title":"observability-grafana","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/observability/grafana</code></li> <li>Available Tasks: serve-detach, scan-image</li> </ul>"},{"location":"products/services/#observability-loki","title":"observability-loki","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/observability/loki</code></li> <li>Available Tasks: serve-detach, scan-image</li> </ul>"},{"location":"products/services/#observability-otel-collector","title":"observability-otel-collector","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/observability/otel-collector</code></li> <li>Available Tasks: serve-detach, scan-image</li> </ul>"},{"location":"products/services/#observability-prometheus","title":"observability-prometheus","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/observability/prometheus</code></li> <li>Available Tasks: serve-detach, scan-image</li> </ul>"},{"location":"products/services/#observability-pyroscope","title":"observability-pyroscope","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/observability/pyroscope</code></li> <li>Available Tasks: serve-detach, scan-image</li> </ul>"},{"location":"products/services/#observability-tempo","title":"observability-tempo","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/observability/tempo</code></li> <li>Available Tasks: serve-detach, scan-image</li> </ul>"},{"location":"products/services/#openchallenges-apex","title":"openchallenges-apex","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/openchallenges/apex</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#openchallenges-api-gateway","title":"openchallenges-api-gateway","text":"<ul> <li>Language: java</li> <li>Location: <code>apps/openchallenges/api-gateway</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image</li> </ul>"},{"location":"products/services/#openchallenges-auth-service","title":"openchallenges-auth-service","text":"<ul> <li>Language: java</li> <li>Location: <code>apps/openchallenges/auth-service</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image, generate, demo</li> </ul>"},{"location":"products/services/#openchallenges-challenge-service","title":"openchallenges-challenge-service","text":"<ul> <li>Language: java</li> <li>Location: <code>apps/openchallenges/challenge-service</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image, generate</li> </ul>"},{"location":"products/services/#openchallenges-image-service","title":"openchallenges-image-service","text":"<p>...</p> <ul> <li>Language: java</li> <li>Location: <code>apps/openchallenges/image-service</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image, generate</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-mcp-server","title":"openchallenges-mcp-server","text":"<p>The OpenChallenges Model Context Protocol (MCP) Server is a Spring Boot application that serves as a bridge between Large Language Models (LLMs) and the OpenChallenges platform. Built with Spring AI a...</p> <ul> <li>Language: java</li> <li>Location: <code>apps/openchallenges/mcp-server</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image, inspect</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-opensearch","title":"openchallenges-opensearch","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/openchallenges/opensearch</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> </ul>"},{"location":"products/services/#openchallenges-organization-service","title":"openchallenges-organization-service","text":"<ul> <li>Language: java</li> <li>Location: <code>apps/openchallenges/organization-service</code></li> <li>Available Tasks: create-config, build-dev, serve, serve-detach, build-image-base, scan-image, generate, demo-auth</li> </ul>"},{"location":"products/services/#openchallenges-postgres","title":"openchallenges-postgres","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/openchallenges/postgres</code></li> <li>Available Tasks: create-config, serve-detach</li> </ul>"},{"location":"products/services/#openchallenges-thumbor","title":"openchallenges-thumbor","text":"<p>This component is based on [Thumbor S3 Docker].</p> <ul> <li>Language: unknown</li> <li>Location: <code>apps/openchallenges/thumbor</code></li> <li>Available Tasks: create-config, serve-detach, scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#sandbox-observability-python","title":"sandbox-observability-python","text":"<p>Develop a sample Python project that implements the \"three pillars of observability\" plus profiling</p> <ul> <li>Language: python</li> <li>Location: <code>apps/sandbox/observability-python</code></li> <li>Available Tasks: create-config, prepare, serve, serve-detach</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#synapse-api-docs","title":"synapse-api-docs","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/synapse/api-docs</code></li> <li>Available Tasks: create-config, build, serve, serve-detach, build-image, scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#bixarena-projects","title":"Bixarena Projects","text":""},{"location":"products/services/#libraries_1","title":"Libraries","text":""},{"location":"products/services/#bixarena-api-description","title":"bixarena-api-description","text":"<ul> <li>Type: unknown</li> <li>Language: openapi</li> <li>Location: <code>libs/bixarena/api-description</code></li> <li>Available Tasks: build-individuals, build, lint, clean</li> </ul>"},{"location":"products/services/#client-projects","title":"Client Projects","text":""},{"location":"products/services/#applications_2","title":"Applications","text":""},{"location":"products/services/#openchallenges-app","title":"openchallenges-app","text":"<ul> <li>Language: typescript</li> <li>Location: <code>apps/openchallenges/app</code></li> <li>Available Tasks: build, serve, extract-i18n, lint, test, serve-static, serve-detach, e2e</li> </ul>"},{"location":"products/services/#explorers-projects","title":"Explorers Projects","text":""},{"location":"products/services/#libraries_2","title":"Libraries","text":""},{"location":"products/services/#explorers-comparison-tools","title":"explorers-comparison-tools","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/comparison-tools</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-constants","title":"explorers-constants","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/constants</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-models","title":"explorers-models","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/models</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-services","title":"explorers-services","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: services</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/services</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-shared","title":"explorers-shared","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/shared</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-styles","title":"explorers-styles","text":"<ul> <li>Type: styles</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/styles</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-testing","title":"explorers-testing","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/testing</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-themes","title":"explorers-themes","text":"<ul> <li>Type: themes</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/themes</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-ui","title":"explorers-ui","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/ui</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-util","title":"explorers-util","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/explorers/util</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#frontend-projects","title":"Frontend Projects","text":""},{"location":"products/services/#applications_3","title":"Applications","text":""},{"location":"products/services/#openchallenges-opensearch-dashboards","title":"openchallenges-opensearch-dashboards","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/openchallenges/opensearch-dashboards</code></li> <li>Available Tasks: serve-detach, scan-image</li> </ul>"},{"location":"products/services/#model-ad-projects","title":"Model-ad Projects","text":""},{"location":"products/services/#libraries_3","title":"Libraries","text":""},{"location":"products/services/#model-ad-api-client-angular","title":"model-ad-api-client-angular","text":"<ul> <li>Type: unknown</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/api-client-angular</code></li> <li>Available Tasks: test, generate</li> </ul>"},{"location":"products/services/#model-ad-api-description","title":"model-ad-api-description","text":"<ul> <li>Type: unknown</li> <li>Language: openapi</li> <li>Location: <code>libs/model-ad/api-description</code></li> <li>Available Tasks: build-individuals, build, lint, clean</li> </ul>"},{"location":"products/services/#model-ad-config","title":"model-ad-config","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/config</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-disease-correlation-comparison-tool","title":"model-ad-disease-correlation-comparison-tool","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/disease-correlation-comparison-tool</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-gene-expression-comparison-tool","title":"model-ad-gene-expression-comparison-tool","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/gene-expression-comparison-tool</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-home","title":"model-ad-home","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/home</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-model-details","title":"model-ad-model-details","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/model-details</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-model-overview-comparison-tool","title":"model-ad-model-overview-comparison-tool","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/model-overview-comparison-tool</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-services","title":"model-ad-services","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: services</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/services</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-styles","title":"model-ad-styles","text":"<ul> <li>Type: styles</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/styles</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-testing","title":"model-ad-testing","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/testing</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-themes","title":"model-ad-themes","text":"<ul> <li>Type: themes</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/themes</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-util","title":"model-ad-util","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: util</li> <li>Language: typescript</li> <li>Location: <code>libs/model-ad/util</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#monorepo-projects","title":"Monorepo Projects","text":""},{"location":"products/services/#applications_4","title":"Applications","text":""},{"location":"products/services/#monorepo-dev-qa-agent","title":"monorepo-dev-qa-agent","text":"<ul> <li>Language: typescript</li> <li>Location: <code>apps/monorepo/dev-qa-agent</code></li> <li>Available Tasks: build, serve, lint, test</li> </ul>"},{"location":"products/services/#monorepo-docs-maintainer-agent","title":"monorepo-docs-maintainer-agent","text":"<ul> <li>Language: typescript</li> <li>Location: <code>apps/monorepo/docs-maintainer-agent</code></li> <li>Available Tasks: build, serve, run, run-dry, run-propose, lint, test</li> </ul>"},{"location":"products/services/#monorepo-tools","title":"monorepo-tools","text":"<p>A powerful script to analyze OpenAPI dependencies\ud83d\udd17 Used Files with Dependency Chains:</p> <ul> <li>Language: typescript</li> <li>Location: <code>apps/monorepo/tools</code></li> <li>Available Tasks: analyze-openapi</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-projects","title":"Openchallenges Projects","text":""},{"location":"products/services/#libraries_4","title":"Libraries","text":""},{"location":"products/services/#openchallenges-about","title":"openchallenges-about","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/about</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-api-client-java","title":"openchallenges-api-client-java","text":"<p>OpenChallenges API</p> <ul> <li>Type: library</li> <li>Language: java</li> <li>Location: <code>libs/openchallenges/api-client-java</code></li> <li>Available Tasks: generate</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-app-config-data","title":"openchallenges-app-config-data","text":"<ul> <li>Type: library</li> <li>Language: java</li> <li>Location: <code>libs/openchallenges/app-config-data</code></li> <li>Available Tasks:</li> </ul>"},{"location":"products/services/#openchallenges-assets","title":"openchallenges-assets","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: assets</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/assets</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-challenge","title":"openchallenges-challenge","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/challenge</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-challenge-search","title":"openchallenges-challenge-search","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/challenge-search</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-config","title":"openchallenges-config","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/config</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-home","title":"openchallenges-home","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/home</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-not-found","title":"openchallenges-not-found","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/not-found</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-org-profile","title":"openchallenges-org-profile","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/org-profile</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-org-search","title":"openchallenges-org-search","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/org-search</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-styles","title":"openchallenges-styles","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: styles</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/styles</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-team","title":"openchallenges-team","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/team</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-themes","title":"openchallenges-themes","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: themes</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/themes</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-ui","title":"openchallenges-ui","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/ui</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-util","title":"openchallenges-util","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: util</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/util</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#results-visualization-framework-projects","title":"Results-visualization-framework Projects","text":""},{"location":"products/services/#libraries_5","title":"Libraries","text":""},{"location":"products/services/#results-visualization-framework-ui","title":"results-visualization-framework-ui","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: feature</li> <li>Language: typescript</li> <li>Location: <code>libs/results-visualization-framework/ui</code></li> <li>Available Tasks: test, lint, storybook, build-storybook, static-storybook</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-projects","title":"Shared Projects","text":""},{"location":"products/services/#applications_5","title":"Applications","text":""},{"location":"products/services/#agora-api","title":"agora-api","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/agora/api</code></li> <li>Available Tasks: create-config, build, serve, serve-detach, lint, lint-fix, scan-image, test</li> </ul>"},{"location":"products/services/#agora-app","title":"agora-app","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/agora/app</code></li> <li>Available Tasks: create-config, build, serve, test, extract-i18n, lint, lint-fix, serve-static, serve-detach, scan-image, e2e</li> </ul>"},{"location":"products/services/#agora-locust","title":"agora-locust","text":"<p>A Locust-based load testing project to benchmark the performance and scalability of Agora's REST API</p> <ul> <li>Language: python</li> <li>Location: <code>apps/agora/locust</code></li> <li>Available Tasks: prepare, serve</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#bixarena-tools","title":"bixarena-tools","text":"<p>Project description here.</p> <ul> <li>Language: python</li> <li>Location: <code>apps/bixarena/tools</code></li> <li>Available Tasks: lock, sync, add, update, remove, build, lint, format, test, install, run-hello</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#iatlas-api","title":"iatlas-api","text":"<p>A GraphQL API that serves data from the iAtlas Data Database. This is built in Python and developed</p> <ul> <li>Language: python</li> <li>Location: <code>apps/iatlas/api</code></li> <li>Available Tasks: create-config, prepare, serve-detach, test-depends-on-db</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#iatlas-data","title":"iatlas-data","text":"<p>This project provides a containerized solution for downloading iAtlas data and its schema from</p> <ul> <li>Language: python</li> <li>Location: <code>apps/iatlas/data</code></li> <li>Available Tasks: create-config, prepare, serve, serve-detach</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#model-ad-app","title":"model-ad-app","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/model-ad/app</code></li> <li>Available Tasks: create-config, build, serve, test, extract-i18n, lint, lint-fix, serve-static, serve-detach, scan-image, e2e</li> </ul>"},{"location":"products/services/#openchallenges-api-docs","title":"openchallenges-api-docs","text":"<ul> <li>Language: unknown</li> <li>Location: <code>apps/openchallenges/api-docs</code></li> <li>Available Tasks: build, serve</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-tools","title":"openchallenges-tools","text":"<p>Project description here.</p> <ul> <li>Language: python</li> <li>Location: <code>apps/openchallenges/tools</code></li> <li>Available Tasks: create-config, lock, sync, add, update, remove, build, lint, format, test, install, run-get-challenge, index-kaggle-challenges</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#sage-otel-collector","title":"sage-otel-collector","text":"<p>As discussed in this Github Issue: https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/30798#issuecomment-2009233014</p> <ul> <li>Language: unknown</li> <li>Location: <code>apps/sage/otel-collector</code></li> <li>Available Tasks: serve-detach, scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#sage-shiny-base","title":"sage-shiny-base","text":"<p>Base Docker image for Shiny applications</p> <ul> <li>Language: unknown</li> <li>Location: <code>apps/sage/shiny-base</code></li> <li>Available Tasks: scan-image</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#sandbox-lambda","title":"sandbox-lambda","text":"<ul> <li>Environments</li> </ul> <ul> <li>Language: unknown</li> <li>Location: <code>tmp/lambda</code></li> <li>Available Tasks: cdk, deploy, deploy-all, destroy, diff, ls, synth, watch, generate-event, invoke, start-api, start-lambda, lint, test</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#sandbox-lambda-e2e","title":"sandbox-lambda-e2e","text":"<ul> <li>Language: unknown</li> <li>Location: <code>tmp/lambda-e2e</code></li> <li>Available Tasks: e2e, generate-event, lint</li> </ul>"},{"location":"products/services/#sandbox-lambda-nodejs","title":"sandbox-lambda-nodejs","text":"<p>nx build sandbox-lambda-nodejs</p> <ul> <li>Language: javascript</li> <li>Location: <code>apps/sandbox/lambda-nodejs</code></li> <li>Available Tasks: build, serve, serve-detach, invoke</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#libraries_6","title":"Libraries","text":""},{"location":"products/services/#bixarena-api-client-python","title":"bixarena-api-client-python","text":"<p>Advance bioinformatics by evaluating and ranking AI agents.</p> <ul> <li>Type: unknown</li> <li>Language: python</li> <li>Location: <code>libs/bixarena/api-client-python</code></li> <li>Available Tasks: lock, sync, add, update, remove, build, lint-excluded, test, install, run-python-version, generate</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#explorers-storybook","title":"explorers-storybook","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: unknown</li> <li>Language: unknown</li> <li>Location: <code>libs/explorers/storybook</code></li> <li>Available Tasks: lint, storybook, build-storybook, static-storybook</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-api-client-angular","title":"openchallenges-api-client-angular","text":"<ul> <li>Type: unknown</li> <li>Language: typescript</li> <li>Location: <code>libs/openchallenges/api-client-angular</code></li> <li>Available Tasks: test, generate</li> </ul>"},{"location":"products/services/#openchallenges-api-client-python","title":"openchallenges-api-client-python","text":"<p>Project description here.</p> <ul> <li>Type: unknown</li> <li>Language: unknown</li> <li>Location: <code>tmp/api-client-python-bak</code></li> <li>Available Tasks: lock, sync, add, update, remove, build, lint, format, test, install</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-api-client-python_1","title":"openchallenges-api-client-python","text":"<p>Discover, explore, and contribute to open biomedical challenges.</p> <ul> <li>Type: unknown</li> <li>Language: python</li> <li>Location: <code>libs/openchallenges/api-client-python</code></li> <li>Available Tasks: lock, sync, add, update, remove, build, lint-excluded, test, install, run-python-version, generate</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-api-client-python_2","title":"openchallenges-api-client-python","text":"<p>No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)</p> <ul> <li>Type: library</li> <li>Language: python</li> <li>Location: <code>tmp/api-client-python-bak/tmp/api-client-python-bak</code></li> <li>Available Tasks: prepare, generate, release</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#openchallenges-api-description","title":"openchallenges-api-description","text":"<ul> <li>Type: unknown</li> <li>Language: openapi</li> <li>Location: <code>libs/openchallenges/api-description</code></li> <li>Available Tasks: build-individuals, build, lint, clean</li> </ul>"},{"location":"products/services/#sage-monorepo","title":"sage-monorepo","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: unknown</li> <li>Language: unknown</li> <li>Location: <code>tmp/sage-monorepo-plugin/nx-sage-monorepo</code></li> <li>Available Tasks: build, lint, test</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#sage-monorepo-nx-plugin","title":"sage-monorepo-nx-plugin","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: unknown</li> <li>Language: unknown</li> <li>Location: <code>libs/sage-monorepo/nx-plugin</code></li> <li>Available Tasks: build, lint, test</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#sandbox-angular-lib","title":"sandbox-angular-lib","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: unknown</li> <li>Language: unknown</li> <li>Location: <code>libs/sandbox/angular-lib</code></li> <li>Available Tasks: test, lint</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-java-util","title":"shared-java-util","text":"<ul> <li>Type: feature</li> <li>Language: java</li> <li>Location: <code>libs/shared/java/util</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-typescript-assets","title":"shared-typescript-assets","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: assets</li> <li>Language: typescript</li> <li>Location: <code>libs/shared/typescript/assets</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-typescript-charts","title":"shared-typescript-charts","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: util</li> <li>Language: typescript</li> <li>Location: <code>libs/shared/typescript/charts</code></li> <li>Available Tasks: build, lint, lint-fix, test</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-typescript-charts-angular","title":"shared-typescript-charts-angular","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: util</li> <li>Language: typescript</li> <li>Location: <code>libs/shared/typescript/charts-angular</code></li> <li>Available Tasks: test, lint, lint-fix, storybook, build-storybook, static-storybook</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-typescript-google-tag-manager","title":"shared-typescript-google-tag-manager","text":"<p>A reusable Google Tag Manager integration component for Angular applications in the sage-monorepo.</p> <ul> <li>Type: util</li> <li>Language: typescript</li> <li>Location: <code>libs/shared/typescript/google-tag-manager</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-typescript-styles","title":"shared-typescript-styles","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: styles</li> <li>Language: typescript</li> <li>Location: <code>libs/shared/typescript/styles</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-typescript-themes","title":"shared-typescript-themes","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: themes</li> <li>Language: typescript</li> <li>Location: <code>libs/shared/typescript/themes</code></li> <li>Available Tasks:</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#shared-typescript-util","title":"shared-typescript-util","text":"<p>This library was generated with Nx.</p> <ul> <li>Type: util</li> <li>Language: typescript</li> <li>Location: <code>libs/shared/typescript/util</code></li> <li>Available Tasks: test, lint, lint-fix</li> <li>Documentation: README</li> </ul>"},{"location":"products/services/#synapse-api-client-angular","title":"synapse-api-client-angular","text":"<ul> <li>Type: unknown</li> <li>Language: typescript</li> <li>Location: <code>libs/synapse/api-client-angular</code></li> <li>Available Tasks: test, generate</li> </ul>"},{"location":"products/services/#synapse-api-description","title":"synapse-api-description","text":"<ul> <li>Type: unknown</li> <li>Language: openapi</li> <li>Location: <code>libs/synapse/api-description</code></li> <li>Available Tasks: get, build, lint</li> </ul> <p>This catalog was automatically generated from the Nx workspace configuration. Last updated: 2025-08-23T22:04:00.614Z</p>"},{"location":"resources/bug-report/","title":"Bug Reports","text":"<p>Issues should be used to report problems with this project, request a new feature, or to discuss potential changes before a pull request (PR) is created. When you create a new issue, a template will be loaded that will guide you through collecting and providing the information we need to investigate.</p> <p>If you find an issue that addresses the problem you're having, please add your own reproduction information to the existing issue rather than creating a new one. Adding a reaction can also help be indicating to our maintainers that a particular problem is affecting more than just the reporter.</p>"},{"location":"resources/faq/","title":"Frequently Asked Questions","text":""},{"location":"resources/faq/#getting-started","title":"Getting Started","text":""},{"location":"resources/faq/#what-is-the-sage-monorepo","title":"What is the Sage Monorepo?","text":"<p>The Sage Monorepo is a comprehensive development platform for building biomedical research applications. It provides shared components, modern tooling, and standardized development practices across multiple programming languages and frameworks.</p>"},{"location":"resources/faq/#how-do-i-get-started","title":"How do I get started?","text":"<p>Check out our Quick Start Guide to set up your local development environment. For a deeper understanding, explore our Architecture Overview.</p>"},{"location":"resources/faq/#what-technologies-are-supported","title":"What technologies are supported?","text":"<p>We support:</p> <ul> <li>Frontend: TypeScript, Angular, React</li> <li>Backend: Java (Spring Boot), Python (Flask), R (Shiny)</li> <li>Databases: PostgreSQL, MongoDB</li> <li>Infrastructure: Docker, Kubernetes, AWS</li> </ul>"},{"location":"resources/faq/#development","title":"Development","text":""},{"location":"resources/faq/#how-do-i-create-a-new-project","title":"How do I create a new project?","text":"<p>Follow our technology-specific tutorials:</p> <ul> <li>Angular App</li> <li>Java REST API</li> <li>Python REST API</li> <li>Docker Project</li> </ul>"},{"location":"resources/faq/#how-do-i-share-code-between-projects","title":"How do I share code between projects?","text":"<p>Use our library creation guides:</p> <ul> <li>Angular Library</li> <li>Java Library</li> </ul>"},{"location":"resources/faq/#can-i-develop-remotely","title":"Can I develop remotely?","text":"<p>Yes! We support remote development through:</p> <ul> <li>Dev Containers</li> <li>Remote Host Development</li> </ul>"},{"location":"resources/faq/#platform-services","title":"Platform &amp; Services","text":""},{"location":"resources/faq/#what-services-are-available","title":"What services are available?","text":"<p>See our Service Catalog for a complete list of all 145 projects, or explore our main products:</p> <ul> <li>Agora</li> <li>OpenChallenges</li> </ul>"},{"location":"resources/faq/#how-do-i-access-the-apis","title":"How do I access the APIs?","text":"<p>All our APIs are documented in the API section. Each service provides OpenAPI specifications and client libraries.</p>"},{"location":"resources/faq/#where-can-i-find-api-documentation","title":"Where can I find API documentation?","text":"<p>API documentation is auto-generated and available for:</p> <ul> <li>Agora API</li> <li>BixArena API</li> <li>Model-AD API</li> <li>OpenChallenges API</li> <li>Synapse API</li> </ul>"},{"location":"resources/faq/#contributing","title":"Contributing","text":""},{"location":"resources/faq/#how-can-i-contribute","title":"How can I contribute?","text":"<p>We welcome contributions! Check out our Contributing Guidelines to get started. You can:</p> <ul> <li>Report bugs using our Bug Report Template</li> <li>Request features using our Feature Request Guidelines</li> <li>Submit pull requests with improvements</li> </ul>"},{"location":"resources/faq/#how-do-i-report-issues","title":"How do I report issues?","text":"<p>Use our GitHub Issues page. Please use the appropriate template:</p> <ul> <li>Bug reports</li> <li>Feature requests</li> <li>Documentation improvements</li> </ul>"},{"location":"resources/faq/#can-i-collaborate-with-multiple-authors-on-commits","title":"Can I collaborate with multiple authors on commits?","text":"<p>Yes! See our guide on Creating Multi-author Commits.</p>"},{"location":"resources/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"resources/faq/#my-local-setup-isnt-working","title":"My local setup isn't working","text":"<p>Common solutions:</p> <ol> <li>Ensure you have the required dependencies installed</li> <li>Check that your environment variables are set correctly</li> <li>Verify your Docker setup if using containers</li> <li>Review the local development guide</li> </ol>"},{"location":"resources/faq/#build-or-test-failures","title":"Build or test failures","text":"<ol> <li>Clear your cache: <code>nx reset</code></li> <li>Reinstall dependencies: <code>pnpm install</code></li> <li>Check for any environment-specific requirements</li> <li>Review recent changes that might affect your project</li> </ol>"},{"location":"resources/faq/#api-connection-issues","title":"API connection issues","text":"<ol> <li>Verify your API endpoints and credentials</li> <li>Check network connectivity</li> <li>Review the specific API documentation</li> <li>Ensure you're using the correct API version</li> </ol>"},{"location":"resources/faq/#updates-news","title":"Updates &amp; News","text":""},{"location":"resources/faq/#how-do-i-stay-updated","title":"How do I stay updated?","text":"<ul> <li>Follow our Updates page for the latest announcements</li> <li>Subscribe to our Blog for technical insights</li> <li>Watch our GitHub repository for releases</li> </ul>"},{"location":"resources/faq/#where-can-i-find-release-notes","title":"Where can I find release notes?","text":"<p>Check our Updates page for version-specific release notes and feature announcements.</p>"},{"location":"resources/faq/#still-need-help","title":"Still Need Help?","text":"<p>If you can't find the answer to your question:</p> <ol> <li>Search our GitHub Issues</li> <li>Create a new issue with the appropriate template</li> <li>Contact our development team through GitHub</li> </ol> <p>This FAQ is continuously updated. If you think a question should be added, please let us know!</p>"},{"location":"resources/feature-requests/","title":"Feature Requests","text":"<p>PRs to our repositories are always welcome and can be a quick way to get your fix or improvement slated for the next release. In general, PRs should:</p> <ul> <li>Only fix/add the functionality in question OR address wide-spread   whitespace/style issues, not both.</li> <li>Add unit or integration tests for fixed or changed functionality (if a test   suite already exists).</li> <li>Address a single concern in the least number of changed lines as possible.</li> <li>Include documentation in the repo or on our [docs site].</li> <li>Be accompanied by a complete Pull Request template (loaded automatically when   a PR is created).</li> </ul> <p>For changes that address core functionality or would require breaking changes (e.g. a major release), it's best to open an Issue to discuss your proposal first. This is not required but can save time creating and reviewing changes.</p> <p>In general, we follow the Forking Workflow:</p> <ol> <li>Fork the repository to your own Github account</li> <li>Clone the project to your machine</li> <li> <p>Create a branch locally with a succinct but descriptive name</p> <p>git checkout -b \uff1cnew-branch\uff1e main</p> </li> <li> <p>Commit changes to the branch</p> </li> <li>Following any formatting and testing guidelines specific to this repo</li> <li>Push changes to your fork</li> <li>Open a PR in our repository and follow the PR template so that we can    efficiently review the changes.</li> </ol> <p>We recommend that you add this repository as an upstream remote to your local git repository so that you can fetch the latest updates.</p> <pre><code>$ git remote add upstream https://github.com/Sage-Bionetworks/sage-monorepo.git\n$ git remote -v\n&gt; ...\n&gt; upstream  https://github.com/Sage-Bionetworks/sage-monorepo.git (fetch)\n&gt; upstream  https://github.com/Sage-Bionetworks/sage-monorepo.git (push)\n</code></pre> <p>On your local machine make sure you have the latest version of the <code>main</code> branch from this upstream repository:</p> <pre><code>git checkout main\ngit fetch upstream\ngit rebase upstream/main\n</code></pre>"},{"location":"resources/feature-requests/#setup-development-environment","title":"Setup Development Environment","text":"<p>This project relies on Node tools and project-specific commands defined in package.json to streamline the development and testing. The command below will install the required development tools.</p> <p>Source <code>dev-env.sh</code>.</p> <pre><code>. dev-env.sh\n</code></pre> <p>Prepare the development environment.</p> <pre><code>openchallenges-prepare\n</code></pre>"},{"location":"resources/feature-requests/#linting","title":"Linting","text":"<p>Lint all the projects.</p> <pre><code>yarn lint\n</code></pre>"},{"location":"resources/feature-requests/#testing","title":"Testing","text":"<p>Build and test all the projects.</p> <pre><code>yarn build\nyarn test\n</code></pre>"},{"location":"resources/feature-requests/#start-the-openchallenges","title":"Start the OpenChallenges","text":"<p>Start the web app and its dependencies (API, API database).</p> <pre><code>yarn start\n</code></pre>"},{"location":"resources/feature-requests/#release-procedure","title":"Release Procedure","text":"<p>Maintainers are required to follow the procedure below when creating a new release.</p> <p>TBA</p>"},{"location":"resources/feature-requests/#getting-help","title":"Getting Help","text":"<p>Join us on the XXX and post your question to the channel that best matches the topic of your request.</p>"},{"location":"resources/oc-schemas/","title":"OpenChallenges Schemas","text":"<p>Content coming soon!</p>"},{"location":"rfcs/","title":"Request for Comments (RFCs)","text":"<p>This directory contains proposals for significant changes to the system that require review and feedback before implementation.</p>"},{"location":"rfcs/#rfc-lifecycle","title":"RFC Lifecycle","text":"<p>RFCs follow a structured review process to ensure proposals are well-vetted before implementation:</p> <ol> <li>Open for Review: Author creates RFC, publishes to docs site (PR #1), and opens feedback PR (PR #2)</li> <li>Approved: RFC accepted, detailed architecture plan created in <code>docs/architecture/</code></li> <li>Rejected: RFC declined with documented rationale</li> </ol> <p>Once approved, the RFC remains in <code>docs/rfcs/</code> as an immutable historical record. Implementation progress is tracked in the corresponding architecture plan in <code>docs/architecture/</code>.</p>"},{"location":"rfcs/#key-principles","title":"Key Principles","text":"<ul> <li>Two-PR Workflow: First PR publishes RFC to docs site, second PR collects feedback</li> <li>RFCs Stay Put: Approved RFCs remain in <code>docs/rfcs/</code> permanently; architecture plans are created as NEW files in <code>docs/architecture/</code> that reference the source RFC</li> <li>Centralized Feedback: All discussion happens in PR #2 for easy reference</li> <li>Lightweight Proposals: RFCs focus on \"why\" and \"what\", detailed \"how\" comes in architecture plan after approval</li> </ul>"},{"location":"rfcs/#rfcs","title":"RFCs","text":"RFC Title Status Author Date 0001 BixArena Leaderboard Snapshot Automation Open for Review Thomas Schaffter 2026-01-09"},{"location":"rfcs/#template","title":"Template","text":"<p>See template.md for the RFC template.</p>"},{"location":"rfcs/#submitting-an-rfc","title":"Submitting an RFC","text":"<p>See the Documentation Submission Workflow guide for detailed step-by-step instructions on the two-PR process.</p>"},{"location":"rfcs/#guidelines","title":"Guidelines","text":"<ul> <li>Use clear, descriptive titles</li> <li>Include motivation, proposed solution, alternatives considered</li> <li>Focus on \"why\" and \"what\", not just \"how\"</li> <li>Keep implementation details at appropriate level (not too granular)</li> <li>Include timeline and success criteria</li> </ul>"},{"location":"rfcs/#related-documentation","title":"Related Documentation","text":"<ul> <li>See Architecture Plans for approved designs</li> <li>See ADRs for specific architectural decisions</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/","title":"BixArena Leaderboard Snapshot Automation","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#metadata","title":"Metadata","text":"<ul> <li>Status: Proposed</li> <li>Author: Thomas Schaffter</li> <li>Created: 2026-01-09</li> <li>Last Updated: 2026-01-12</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#summary","title":"Summary","text":"<p>Automate the daily generation of BixArena leaderboard snapshots using AWS Lambda and EventBridge scheduling, eliminating the manual process and improving system security by removing the need for external database access via SSH tunneling.</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#motivation","title":"Motivation","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#problem-statement","title":"Problem Statement","text":"<p>The BixArena leaderboard snapshot generation currently requires manual intervention by engineering team members. This manual process involves:</p> <ol> <li>Setting up SSH port forwarding via AWS SSM to a bastion host</li> <li>Running a Python script from <code>bixarena-tools</code> that connects directly to the production database</li> <li>Generating the leaderboard snapshot from battle evaluation data</li> <li>Publishing the snapshot to make it visible to users</li> </ol> <p>This process must be repeated each time a leaderboard update is needed, consuming engineering resources and limiting how frequently leaderboards can be updated.</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#current-state","title":"Current State","text":"<ul> <li>Process: Manual execution of Python script from <code>bixarena-tools</code></li> <li>Frequency: Ad-hoc, dependent on engineer availability (typically Rong)</li> <li>Database Access: Direct connection from development environment via AWS SSM port forwarding through bastion host</li> <li>Security: Database accessible from outside the private network (via tunneling)</li> <li>Reliability: Dependent on manual execution, prone to being forgotten or delayed</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#desired-state","title":"Desired State","text":"<ul> <li>Process: Fully automated, scheduled execution</li> <li>Frequency: Predictable daily updates at consistent times</li> <li>Database Access: Lambda functions running within VPC private subnets</li> <li>Security: Database connections confined to private network, no external access required</li> <li>Reliability: Guaranteed daily execution with monitoring and alerting</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#business-impact","title":"Business Impact","text":"<p>Credibility Risk: Infrequent or irregular leaderboard updates can negatively impact BixArena's credibility with users who expect to see current rankings reflecting recent battles and evaluations.</p> <p>User Engagement: Stale leaderboards reduce user engagement and confidence in the platform's activity level.</p> <p>Engineering Efficiency: Manual process consumes engineering time that could be better spent on feature development.</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#proposal","title":"Proposal","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#overview","title":"Overview","text":"<p>Implement automated daily leaderboard snapshot generation using a hybrid approach:</p> <ol> <li>Primary Execution Path: AWS Lambda function triggered by EventBridge (CloudWatch Events) on a daily schedule</li> <li>Secondary Execution Path: Protected admin API endpoint in AI service for on-demand generation</li> <li>Shared Logic: Reusable leaderboard library extracted from <code>bixarena-tools</code></li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TD\n    A[EventBridge Rule&lt;br/&gt;Daily 2 AM UTC] --&gt;|Triggers| B[Lambda Function&lt;br/&gt;leaderboard-snapshot-gen]\n    C[AI Service&lt;br/&gt;Admin Endpoint] --&gt;|Async Invoke| B\n    B --&gt;|Reads| D[RDS PostgreSQL&lt;br/&gt;Battle Evaluations]\n    B --&gt;|Writes| E[RDS PostgreSQL&lt;br/&gt;Leaderboard Snapshots]\n    B --&gt;|Notifies| F[SNS Topic]\n    F --&gt;|Triggers| G[Slack Notification]\n\n    H[Admin User] --&gt;|Authenticated Request| C\n\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style D fill:#bbf,stroke:#333,stroke-width:2px\n    style E fill:#bbf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#key-components","title":"Key Components","text":"<ol> <li> <p>Shared Library (<code>libs/bixarena/leaderboard/python/</code>)</p> <ul> <li>Extract snapshot generation logic from <code>bixarena-tools</code></li> <li>Reusable by both Lambda function and CLI tool</li> <li>Contains Bradley-Terry ranking algorithm, database queries, snapshot generation</li> </ul> </li> <li> <p>Lambda Function (Container Image)</p> <ul> <li>Scheduled daily execution via EventBridge</li> <li>Runs in VPC private subnet with access to RDS</li> <li>Retrieves credentials from AWS Secrets Manager</li> <li>Publishes notifications to SNS on success/failure</li> </ul> </li> <li> <p>Admin API Endpoint</p> <ul> <li>Protected endpoint in AI service (<code>/admin/generate-snapshot</code>)</li> <li>Requires JWT authentication with admin role</li> <li>Async Lambda invocation (returns immediately with correlation ID)</li> <li>Enables on-demand snapshot generation when needed</li> </ul> </li> <li> <p>Monitoring &amp; Notifications</p> <ul> <li>CloudWatch logs with structured logging</li> <li>SNS notifications to Slack on success/failure</li> <li>CloudWatch alarms for Lambda errors</li> <li>Correlation IDs for tracing requests</li> </ul> </li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Lambda vs ECS Scheduled Task: Lambda chosen for simplicity, cost-effectiveness, and built-in retry logic</li> <li>Container Image vs Zip: Container image chosen to support NumPy/SciPy dependencies without size constraints</li> <li>RDS Proxy: Optional connection pooling for Lambda (to be evaluated)</li> <li>Auto-publish: Snapshots automatically set to <code>visible=true</code> after generation</li> <li>Async API Invocation: Admin endpoint returns immediately (HTTP 202) to avoid timeout issues</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#security-improvements","title":"Security Improvements","text":"<ol> <li>Network Isolation: Lambda functions run in VPC private subnets, no external database access</li> <li>Least Privilege IAM: Scoped permissions for Lambda execution and ECS task roles</li> <li>Secrets Management: Database credentials stored in AWS Secrets Manager</li> <li>Authentication: Admin API requires JWT with admin role validation</li> <li>Input Validation: Whitelist-based validation of all Lambda inputs</li> <li>Audit Logging: Track trigger source (scheduled/manual), user email, correlation IDs</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#alternative-1-ecs-scheduled-tasks","title":"Alternative 1: ECS Scheduled Tasks","text":"<p>Description: Use ECS scheduled tasks instead of Lambda functions.</p> <p>Pros:</p> <ul> <li>Familiar deployment model (already using ECS for AI service)</li> <li>No execution time limits (Lambda has 15-minute max)</li> <li>Can reuse existing Docker images and infrastructure</li> </ul> <p>Cons:</p> <ul> <li>Higher cost (ECS task runs for full duration, Lambda charges per 100ms)</li> <li>More complex infrastructure (task definitions, scheduling, networking)</li> <li>Requires VPC and load balancer configuration</li> <li>Overkill for a simple scheduled job</li> </ul> <p>Decision: Rejected. Lambda is more appropriate for scheduled batch jobs with predictable execution times under 15 minutes.</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#alternative-2-step-functions-for-orchestration","title":"Alternative 2: Step Functions for Orchestration","text":"<p>Description: Use AWS Step Functions to orchestrate a multi-step workflow (fetch data \u2192 compute rankings \u2192 save snapshot \u2192 notify).</p> <p>Pros:</p> <ul> <li>Visual workflow representation</li> <li>Built-in error handling and retries</li> <li>State persistence across steps</li> <li>Can handle complex multi-step workflows</li> </ul> <p>Cons:</p> <ul> <li>Added complexity for a simple single-step process</li> <li>Additional cost for Step Functions executions</li> <li>Current snapshot generation is a single atomic operation</li> <li>Unnecessary for initial implementation</li> </ul> <p>Decision: Rejected for v1. Consider if future requirements introduce multi-step workflows (e.g., data validation \u2192 generation \u2192 approval \u2192 publishing).</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#alternative-3-keep-manual-process","title":"Alternative 3: Keep Manual Process","text":"<p>Description: Continue with manual CLI execution via SSH tunneling.</p> <p>Pros:</p> <ul> <li>No infrastructure changes required</li> <li>Engineers maintain direct control</li> <li>Simple debugging and troubleshooting</li> </ul> <p>Cons:</p> <ul> <li>Does not address core problems (reliability, frequency, security)</li> <li>Continues to consume engineering resources</li> <li>Scales poorly as more leaderboards are added</li> <li>Security risk from external database access</li> </ul> <p>Decision: Rejected. Automation is necessary to improve reliability, security, and engineering efficiency.</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#alternative-4-aws-batch","title":"Alternative 4: AWS Batch","text":"<p>Description: Use AWS Batch for scheduled job execution.</p> <p>Pros:</p> <ul> <li>Designed for batch compute workloads</li> <li>Can handle long-running jobs (hours)</li> <li>Built-in retry and failure handling</li> </ul> <p>Cons:</p> <ul> <li>More complex than Lambda for simple jobs</li> <li>Requires compute environment configuration</li> <li>Higher cost for our use case</li> <li>Slower cold starts compared to Lambda</li> </ul> <p>Decision: Rejected. AWS Batch is better suited for large-scale batch processing. Lambda is sufficient for our needs.</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#performance-implications","title":"Performance Implications","text":"<p>Current Performance:</p> <ul> <li>Manual execution time: ~2-5 minutes (depending on data volume)</li> <li>Database query time: ~30 seconds for battle evaluations</li> <li>Bradley-Terry computation: ~1-2 minutes (1000 bootstrap iterations)</li> </ul> <p>Expected Performance with Lambda:</p> <ul> <li>Execution time: 2-5 minutes (same as manual)</li> <li>Cold start: ~2-3 seconds (container image)</li> <li>Memory: 2GB (sufficient for NumPy/SciPy)</li> <li>Timeout: 15 minutes (3x buffer)</li> </ul> <p>Database Impact:</p> <ul> <li>Read queries: Same as manual process (fetch battle evaluations)</li> <li>Write queries: Same as manual process (insert snapshot + entries)</li> <li>Connection pooling: RDS Proxy can be added if connection limits become an issue</li> </ul> <p>Network Performance:</p> <ul> <li>Lambda in same VPC as RDS: Low latency, no data transfer costs</li> <li>No external network hops (unlike current SSH tunneling approach)</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#security-considerations","title":"Security Considerations","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#current-security-concerns","title":"Current Security Concerns","text":"<ol> <li>External Database Access: SSH tunneling exposes database to connections from development environments</li> <li>Credential Management: Engineers need database credentials to run manual script</li> <li>Audit Trail: Limited visibility into who generated snapshots and when</li> <li>Network Exposure: Bastion host and SSM session manager create additional attack surface</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#proposed-security-improvements","title":"Proposed Security Improvements","text":"<ol> <li> <p>Network Isolation:</p> <ul> <li>Lambda runs in VPC private subnets</li> <li>No ingress from external networks</li> <li>Database never exposed outside VPC</li> </ul> </li> <li> <p>IAM-Based Access Control:</p> <ul> <li>Lambda execution role with least-privilege permissions</li> <li>Scoped access to Secrets Manager, RDS, SNS</li> <li>No long-lived credentials in environment variables</li> </ul> </li> <li> <p>Secrets Management:</p> <ul> <li>Database credentials stored in AWS Secrets Manager</li> <li>Automatic rotation supported</li> <li>Encrypted at rest and in transit</li> </ul> </li> <li> <p>Authentication &amp; Authorization:</p> <ul> <li>Admin API requires valid JWT token</li> <li>Admin role validation (Cognito groups)</li> <li>User email tracked for audit logging</li> </ul> </li> <li> <p>Audit Logging:</p> <ul> <li>CloudWatch logs capture all executions</li> <li>Structured logging with correlation IDs</li> <li>Track trigger source (scheduled vs manual) and user identity</li> </ul> </li> <li> <p>Input Validation:</p> <ul> <li>Whitelist allowed leaderboard IDs</li> <li>Validate bootstrap iteration ranges</li> <li>Sanitize all inputs before database queries</li> </ul> </li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#timeline","title":"Timeline","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#phase-1-foundation","title":"Phase 1: Foundation","text":"<ul> <li>Extract <code>bixarena-leaderboard</code> library from <code>bixarena-tools</code></li> <li>Write unit tests for snapshot generation logic</li> <li>Update CLI tool to use library (Optional, no longer needed when calling the API server)</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#phase-2-lambda-function","title":"Phase 2: Lambda Function","text":"<ul> <li>Implement Lambda handler with Docker container</li> <li>Create CDK infrastructure (Lambda, EventBridge, SNS)</li> <li>Add Slack notifications</li> <li>Deploy to dev environment</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#phase-3-admin-api","title":"Phase 3: Admin API","text":"<ul> <li>Add protected endpoint to AI service</li> <li>Implement JWT authentication with admin role check</li> <li>Grant IAM permissions for Lambda invocation</li> <li>Test end-to-end flow</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#phase-4-testing-deployment","title":"Phase 4: Testing &amp; Deployment","text":"<ul> <li>Integration testing in dev environment</li> <li>Verify daily scheduled executions</li> <li>Test manual triggering via API</li> <li>Deploy to staging and production</li> </ul> <p>Total Estimated Timeline: 2-3 weeks</p>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#success-criteria","title":"Success Criteria","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#immediate-success-criteria-week-1-2","title":"Immediate Success Criteria (Week 1-2)","text":"<ul> <li> First successful automated snapshot generated in dev environment</li> <li> Slack notification received for success and failure scenarios</li> <li> Snapshot correctly marked as visible (auto-publish works)</li> <li> Lambda execution time &lt; 5 minutes (within performance targets)</li> <li> No database connection errors or timeouts</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#short-term-success-criteria-30-days","title":"Short-Term Success Criteria (30 Days)","text":"<ul> <li> Operational Reliability: 95% success rate over 30 consecutive days (\u22642 failures/month)</li> <li> Scheduled Execution: Daily snapshots generated at consistent time (2 AM UTC \u00b15 minutes)</li> <li> Zero Manual Interventions: No engineer involvement required for routine updates</li> <li> Alert Response Time: All failures detected and notified to Slack within 5 minutes</li> <li> Security Compliance: Zero external database connections (SSH tunneling eliminated)</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#long-term-success-criteria-90-days","title":"Long-Term Success Criteria (90 Days)","text":"<ul> <li> Update Frequency: Leaderboard updated daily without interruption for 90 days</li> <li> User Engagement: Increase in user visits to leaderboard page (baseline TBD)</li> <li> Engineering Time Saved: 4-8 hours/month of engineering time freed (estimated)</li> <li> System Scalability: Successfully extended to support multiple leaderboard types (open-source, multimodal)</li> <li> Monitoring Coverage: CloudWatch dashboard with key metrics (success rate, duration, entry counts)</li> </ul>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#measurable-kpis","title":"Measurable KPIs","text":"<ol> <li> <p>Success Rate: <code>(successful_executions / total_executions) \u00d7 100</code></p> <ul> <li>Target: \u226595%</li> </ul> </li> <li> <p>Execution Consistency: Standard deviation of execution times</p> <ul> <li>Target: Within \u00b110% of mean execution time</li> </ul> </li> <li> <p>Notification Delivery: Time from execution completion to Slack notification</p> <ul> <li>Target: &lt;1 minute</li> </ul> </li> <li> <p>Security Incidents: External database access attempts</p> <ul> <li>Target: 0 (after cutover from manual process)</li> </ul> </li> <li> <p>Engineering Efficiency: Hours saved per month</p> <ul> <li>Baseline: ~6-8 hours/month (manual execution + troubleshooting)</li> <li>Target: Reduced to ~1 hour/month (monitoring only)</li> </ul> </li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#open-questions","title":"Open Questions","text":""},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#scheduling-frequency","title":"Scheduling &amp; Frequency","text":"<ol> <li>Daily Schedule: Is 2 AM UTC the optimal time? Should we consider user timezone distributions?</li> <li>Multiple Executions: Should we support multiple daily snapshots (e.g., morning and evening updates)?</li> <li>Holiday Handling: Should execution be paused during holidays or maintenance windows?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#failure-handling","title":"Failure Handling","text":"<ol> <li>Retry Policy: How many automatic retries before alerting engineers? (Current: 2 retries)</li> <li>Failure Escalation: Should failures page on-call engineer or just notify Slack?</li> <li>Partial Failures: How to handle cases where snapshot generates but notification fails?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#notification-preferences","title":"Notification Preferences","text":"<ol> <li>Slack Channel: Which channel should receive notifications? (#bixarena-alerts, #bixarena-leaderboard, or both?)</li> <li>Notification Content: Should success notifications include full details or just a summary?</li> <li>Quiet Hours: Should notifications be suppressed during certain hours to avoid noise?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#leaderboard-scope","title":"Leaderboard Scope","text":"<ol> <li>Multiple Leaderboards: When will we add support for other leaderboards (open-source, multimodal)?</li> <li>Leaderboard Priority: If multiple leaderboards exist, should they run sequentially or in parallel?</li> <li>Bootstrap Iterations: Is 1000 iterations appropriate for production, or should it be configurable per leaderboard?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#database-optimization","title":"Database Optimization","text":"<ol> <li>RDS Proxy: Should we implement RDS Proxy immediately or wait until we see connection pooling needs?</li> <li>Read Replicas: Should snapshot generation read from a replica to reduce load on primary database?</li> <li>Query Optimization: Are current database queries optimized for production data volumes?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ol> <li>CloudWatch Dashboard: What metrics are most valuable to track? (Duration, entry counts, success rate?)</li> <li>Custom Metrics: Should we publish custom metrics to CloudWatch for leaderboard-specific KPIs?</li> <li>Log Retention: Is 30 days sufficient for CloudWatch logs, or should we archive to S3?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#security-compliance","title":"Security &amp; Compliance","text":"<ol> <li>Audit Requirements: Do we need to track snapshot generation in a compliance log beyond CloudWatch?</li> <li>Access Control: Should multiple admin roles exist (e.g., admin-read-only vs admin-write)?</li> <li>Rate Limiting: Is 100 API requests/day sufficient rate limit for manual triggering?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Snapshot Retention: Should old snapshots be automatically archived or deleted after a certain period?</li> <li>Snapshot Comparison: Should the system detect and alert on significant ranking changes between snapshots?</li> <li>User Notifications: Should users be notified when new snapshots are published (e.g., email, in-app)?</li> </ol>"},{"location":"rfcs/0001-bixarena-leaderboard-snapshot-automation/#references","title":"References","text":"<ul> <li>AWS Lambda Limits</li> <li>EventBridge Scheduling</li> </ul>"},{"location":"rfcs/template/","title":"[RFC Title]","text":""},{"location":"rfcs/template/#metadata","title":"Metadata","text":"<ul> <li>Status: Open for Review</li> <li>Author: [Your Name]</li> <li>Created: YYYY-MM-DD</li> <li>Last Updated: YYYY-MM-DD</li> </ul>"},{"location":"rfcs/template/#summary","title":"Summary","text":"<p>[One paragraph summary of the proposal]</p>"},{"location":"rfcs/template/#motivation","title":"Motivation","text":""},{"location":"rfcs/template/#problem-statement","title":"Problem Statement","text":"<p>[What problem are we trying to solve?]</p>"},{"location":"rfcs/template/#current-state","title":"Current State","text":"<p>[How do we handle this today?]</p>"},{"location":"rfcs/template/#desired-state","title":"Desired State","text":"<p>[What would we like to achieve?]</p>"},{"location":"rfcs/template/#proposal","title":"Proposal","text":""},{"location":"rfcs/template/#overview","title":"Overview","text":"<p>[High-level description of the proposed solution]</p>"},{"location":"rfcs/template/#architecture","title":"Architecture","text":"<p>[Architecture diagrams, component descriptions]</p>"},{"location":"rfcs/template/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Decision 1: [Rationale]</li> <li>Decision 2: [Rationale]</li> </ul>"},{"location":"rfcs/template/#implementation-phases","title":"Implementation Phases","text":"<p>[Break down the implementation into phases/milestones]</p>"},{"location":"rfcs/template/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"rfcs/template/#alternative-1-name","title":"Alternative 1: [Name]","text":"<p>Pros:</p> <ul> <li>[Benefit 1]</li> <li>[Benefit 2]</li> </ul> <p>Cons:</p> <ul> <li>[Drawback 1]</li> <li>[Drawback 2]</li> </ul> <p>Decision: [Why rejected/accepted]</p>"},{"location":"rfcs/template/#security-considerations","title":"Security Considerations","text":"<p>[Security implications and mitigations]</p>"},{"location":"rfcs/template/#performance-implications","title":"Performance Implications","text":"<p>[Performance impact and benchmarks if applicable]</p>"},{"location":"rfcs/template/#timeline","title":"Timeline","text":"<p>[Estimated timeline for implementation]</p>"},{"location":"rfcs/template/#success-criteria","title":"Success Criteria","text":"<p>[How will we know this is successful?]</p> <ul> <li> Criterion 1</li> <li> Criterion 2</li> </ul>"},{"location":"rfcs/template/#open-questions","title":"Open Questions","text":"<p>[Questions that need to be answered during review]</p> <ol> <li>Question 1?</li> <li>Question 2?</li> </ol>"},{"location":"rfcs/template/#references","title":"References","text":"<ul> <li>[Link to related docs]</li> <li>[Link to related issues]</li> </ul>"},{"location":"updates/","title":"Updates","text":"<p>Welcome to the Sage Monorepo updates! This section contains release notes, feature announcements, and important updates about our platform and tools.</p>"},{"location":"updates/#recent-updates","title":"Recent Updates","text":"<p>Browse our latest updates to stay informed about new features, improvements, and platform changes:</p> <ul> <li>September 2025 - Synapse automation, Model-AD UX, OpenChallenges gateway</li> <li>June 2022 - Backend Microservices &amp; UI Modernization</li> <li>May 2022 - Microservices Architecture &amp; Authentication Infrastructure</li> <li>April 2022 - Development Environment Standardization &amp; UI Enhancements</li> <li>March 2022 - Coverage Reporting &amp; Development Environment Evolution</li> <li>February 2022 - Platform Foundation &amp; Initial Launch</li> </ul>"},{"location":"updates/#stay-updated","title":"Stay Updated","text":"<ul> <li>Watch our repository to get notifications</li> <li>Report issues if you encounter problems</li> <li>Request features to help shape our roadmap</li> </ul> <p>Updates are published monthly or as significant features become available. Each update includes detailed information about new features, improvements, bug fixes, and breaking changes.</p>"},{"location":"updates/april-2022/","title":"April 2022","text":"<p>Published on May 1, 2022</p> <p>Welcome to our April 2022 update! This month brought continued progress in the Challenge Registry platform with 15 pull requests merged to the main branch. Our development efforts were led by @tschaffter and @rrchai, focusing on foundational development tools, platform architecture, and user experience enhancements.</p>"},{"location":"updates/april-2022/#summary","title":"Summary","text":"<ul> <li>Total Pull Requests: 15 merged PRs</li> <li>Key Focus: Development environment standardization and testing infrastructure</li> <li>Major Projects: Challenge Registry platform development, HTML linting integration, UI component improvements</li> </ul>"},{"location":"updates/april-2022/#technical-architecture-overview","title":"Technical architecture overview","text":""},{"location":"updates/april-2022/#development-environment-standardization","title":"Development environment standardization","text":"<p>April marked significant progress in establishing consistent development environments across the monorepo. We upgraded our development container infrastructure and implemented standardized tooling to ensure all contributors have access to the same development experience.</p>"},{"location":"updates/april-2022/#quality-assurance-framework-expansion","title":"Quality assurance framework expansion","text":"<p>We expanded our testing and linting capabilities by introducing HTML linting through webhint, complementing our existing JavaScript and TypeScript quality checks. This addition ensures our web applications maintain high standards for accessibility and semantic correctness.</p>"},{"location":"updates/april-2022/#platform-infrastructure-evolution","title":"Platform infrastructure evolution","text":"<p>The Challenge Registry platform continued to grow with new page components and routing infrastructure. We established the foundation for challenge and organization search functionality, setting the stage for comprehensive challenge discovery features.</p>"},{"location":"updates/april-2022/#user-interface-enhancement-initiatives","title":"User interface enhancement initiatives","text":"<p>Focused improvements to the About page styling and component testing coverage demonstrate our commitment to both user experience and code quality. These enhancements ensure our platform remains both visually appealing and technically robust.</p>"},{"location":"updates/april-2022/#pull-requests-merged","title":"Pull requests merged","text":""},{"location":"updates/april-2022/#challenge-registry-platform-development","title":"Challenge Registry platform development","text":"<ul> <li>#135: Add org search page</li> <li>#134: Add challenge search page</li> <li>#133: Add Challenge page</li> <li>#139: Update styling on About page</li> </ul>"},{"location":"updates/april-2022/#development-environment-and-tooling","title":"Development environment and tooling","text":"<ul> <li>#161: Add 4g memory requirement for devcontainer</li> <li>#160: Add vscode extension to manage GH PRs</li> <li>#148: Use dev container image <code>sagebionetworks/python-3-node:0.2.0</code></li> <li>#128: Update dev container to v0.1.0</li> </ul>"},{"location":"updates/april-2022/#testing-and-quality-assurance","title":"Testing and quality assurance","text":"<ul> <li>#153: Increase TS coverage for ui library</li> <li>#152: Add --runInBand to jest testing on large components</li> <li>#138: Add HTML linter</li> </ul>"},{"location":"updates/april-2022/#documentation-and-governance","title":"Documentation and governance","text":"<ul> <li>#140: Add documentation <code>Using Nx</code></li> <li>#142: Update codeowners</li> <li>#132: Re-use CoC from Sage-Bionetworks/developer-handbook</li> <li>#131: Use Sage Code of Conduct</li> </ul>"},{"location":"updates/april-2022/#community-impact","title":"Community impact","text":"<p>April's contributions demonstrate our commitment to building a robust development ecosystem that supports both current and future platform growth. The focus on development environment standardization ensures that new contributors can quickly become productive, while our expanded testing infrastructure maintains the high quality standards essential for research platform reliability.</p> <p>The establishment of challenge and organization search page foundations represents a significant step toward making biomedical challenges more discoverable and accessible to the research community.</p>"},{"location":"updates/april-2022/#thank-you","title":"Thank you","text":"<p>Last but certainly not least, a big Thank You to the contributors of the Sage monorepo.</p> <ul> <li>@tschaffter</li> <li>@rrchai</li> </ul> <p>We're building something amazing together, and every contribution moves us closer to our vision of advancing biomedical research through innovative technology platforms. Join us in shaping the future of collaborative research.</p>"},{"location":"updates/february-2022/","title":"February 2022","text":"<p>Published on February 28, 2022</p> <p>February 2022 represents the foundational month for the sage-monorepo project, marking the transition to a comprehensive Nx monorepo architecture. This period saw the establishment of core infrastructure, development tooling, and the initial application and library ecosystem that would support future development across multiple technology stacks.</p> <p>During this month, 28 pull requests were successfully merged, establishing the essential infrastructure and initial applications for the monorepo. The primary contributor was @tschaffter, who led the architectural design and implementation of the workspace foundation.</p>"},{"location":"updates/february-2022/#summary","title":"Summary","text":"<ul> <li>Total Pull Requests: 28 merged PRs</li> <li>Key Focus: Foundational monorepo architecture establishment and core infrastructure development</li> <li>Major Projects: Nx workspace setup, containerization strategy, development tooling, and initial application ecosystem</li> </ul>"},{"location":"updates/february-2022/#technical-architecture-overview","title":"Technical architecture overview","text":"<p>February 2022 was a pivotal month focused on establishing robust architectural foundations and development infrastructure. The team implemented a sophisticated Nx monorepo structure designed to support multiple applications and shared libraries across different technology stacks while maintaining code quality and development efficiency.</p>"},{"location":"updates/february-2022/#monorepo-workspace-establishment","title":"Monorepo workspace establishment","text":"<p>The development team established a comprehensive Nx workspace architecture designed to support multiple applications and shared libraries across TypeScript, Angular, React, and Web Components technologies. This architectural foundation enables efficient code sharing, dependency management, and coordinated development across different technology stacks. The workspace structure provides clear separation of concerns between applications, libraries, and shared resources while maintaining consistent development patterns and build processes.</p>"},{"location":"updates/february-2022/#containerization-and-deployment-infrastructure","title":"Containerization and deployment infrastructure","text":"<p>A comprehensive containerization strategy was implemented using Docker to ensure consistent deployment across different environments. Each application received its own Docker configuration with optimized build processes, enabling independent deployment and scaling capabilities. The team established Docker Compose orchestration for local development, simplifying the setup process for new contributors and ensuring environment consistency across the development team.</p>"},{"location":"updates/february-2022/#development-tooling-and-quality-assurance","title":"Development tooling and quality assurance","text":"<p>Significant investment was made in development tooling and quality assurance infrastructure to maintain high code quality standards. This included implementing pre-commit hooks with lint-staged configurations, continuous integration pipelines with GitHub Actions, and comprehensive linting and testing frameworks. The team also established cross-platform development support using tools like shx to accommodate different development environments and ensure consistent behavior across operating systems.</p>"},{"location":"updates/february-2022/#pull-requests-merged","title":"Pull requests merged","text":""},{"location":"updates/february-2022/#workspace-foundation-and-initial-setup","title":"Workspace foundation and initial setup","text":"<ul> <li>#1: Scafold workspace</li> </ul>"},{"location":"updates/february-2022/#platform-infrastructure","title":"Platform infrastructure","text":"<ul> <li>#6: Add CI workflow</li> <li>#8: Fix CI workflow</li> <li>#10: Replace npm commands by yarn in CI workflow</li> <li>#12: Add targets <code>build</code> and <code>test</code></li> <li>#21: Add pre-commit hook</li> <li>#38: Use bash -c in package.json to call sourced functions</li> <li>#39: Set yarn min version required</li> <li>#46: Run cp command with shx</li> </ul>"},{"location":"updates/february-2022/#libraries-and-shared-components","title":"Libraries and shared components","text":"<ul> <li>#2: Add lib api-docs</li> <li>#7: Configure target <code>lint</code> and <code>lint-fix</code> for existing apps and libs</li> <li>#28: Show how to use a Web Components UI library in Angular and React</li> <li>#29: Import UI components from forked sage-angular library</li> <li>#30: Import sage-angular lib</li> <li>#31: Add Web Components (WC) component example</li> </ul>"},{"location":"updates/february-2022/#applications","title":"Applications","text":"<ul> <li>#4: Add API app</li> <li>#5: Add db-cli</li> <li>#16: Add project <code>api-db</code></li> <li>#20: Create project <code>web-app</code></li> <li>#25: Dockerize the projects <code>api</code> and <code>api-db</code></li> <li>#33: Integrate <code>api-angular</code> into <code>web-app</code></li> <li>#35: Dockerize <code>web-app</code></li> </ul>"},{"location":"updates/february-2022/#documentation-and-developer-experience","title":"Documentation and developer experience","text":"<ul> <li>#13: Move initial README generated by Nx to docs/nx.md</li> <li>#40: Document how to start the web app with Docker</li> <li>#43: Add docs/legacy.md</li> <li>#44: Add Code Of Conduct and Contributing Guide</li> <li>#48: Add documentation</li> <li>#49: Fix doc on how to start with Docker</li> </ul>"},{"location":"updates/february-2022/#community-impact","title":"Community Impact","text":"<p>February 2022 represents a pivotal moment in Sage Bionetworks' software development approach, transitioning from isolated application development to a unified, collaborative ecosystem. The establishment of this monorepo architecture demonstrates the organization's commitment to open science principles, code reusability, and efficient development practices.</p> <p>This foundational work enables multiple research teams to collaborate more effectively, share common components, and maintain consistent quality standards across all biomedical software projects. The emphasis on comprehensive documentation and developer experience ensures that the platform remains accessible to both current team members and future contributors.</p>"},{"location":"updates/february-2022/#thank-you","title":"Thank You","text":"<p>Last but certainly not least, a big Thank You to the contributors of the Sage monorepo.</p> <ul> <li>@tschaffter</li> </ul> <p>This inaugural month sets the stage for revolutionizing biomedical research application development. The infrastructure established in February 2022 would support rapid development of multiple scientific applications throughout the year. Join us in building the future of research software!</p>"},{"location":"updates/june-2022/","title":"June 2022","text":"<p>Published on July 1, 2022</p> <p>Welcome to our June 2022 update! This month marked a transformative period for the Challenge Registry platform with 43 pull requests merged to the main branch. Our development efforts were led by @tschaffter, @rrchai, and @vpchung, focusing on major architectural restructuring, Java ecosystem expansion, and comprehensive UI modernization.</p>"},{"location":"updates/june-2022/#summary","title":"Summary","text":"<ul> <li>Total Pull Requests: 43 merged PRs</li> <li>Key Focus: Microservices architecture transformation and multi-application platform evolution</li> <li>Major Projects: Challenge Registry platform restructuring, Java shared libraries, UI v3 implementation</li> </ul>"},{"location":"updates/june-2022/#technical-architecture-overview","title":"Technical Architecture Overview","text":""},{"location":"updates/june-2022/#multi-application-platform-transformation","title":"Multi-application platform transformation","text":"<p>June represented a pivotal month in our platform evolution with the complete restructuring of the monorepo to support multi-application and microservice architecture. We implemented comprehensive project renaming conventions, established database service isolation with dedicated MariaDB, MongoDB, and PostgreSQL instances, and created standardized development workflows that support both standalone and integrated service deployment.</p>"},{"location":"updates/june-2022/#java-ecosystem-foundation","title":"Java ecosystem foundation","text":"<p>The introduction of shared Java libraries marked a significant milestone in our backend development strategy. We established Maven-based dependency management, implemented comprehensive linting and testing frameworks, and created reusable components that streamline microservice development across the platform ecosystem.</p>"},{"location":"updates/june-2022/#user-interface-modernization-initiative","title":"User interface modernization initiative","text":"<p>Our frontend development reached new heights with the implementation of UI v3 design standards across critical user interfaces. We modernized the login experience, transformed navigation components with enhanced theming capabilities, and established consistent design patterns that improve both accessibility and user engagement throughout the platform.</p>"},{"location":"updates/june-2022/#development-environment-standardization","title":"Development environment standardization","text":"<p>Significant improvements to our development infrastructure included comprehensive devcontainer integration, automated dependency caching in CI workflows, and enhanced project management tools. These changes ensure consistent development experiences across different environments while optimizing build performance and developer productivity.</p>"},{"location":"updates/june-2022/#pull-requests-merged","title":"Pull Requests Merged","text":""},{"location":"updates/june-2022/#platform-infrastructure","title":"Platform infrastructure","text":"<ul> <li>#390: Restore challenge-registry-ui tests</li> <li>#389: Fix error 130</li> <li>#387: Fix lombok extension</li> <li>#381: Add rest client icon</li> <li>#379: Fix target <code>build-image</code> of microservices that depend on a local lib</li> <li>#376: Lint challenge-user-service and challenge-core-service</li> <li>#373: Setup maven cache in CI workflow</li> <li>#366: Explore the creation of a shared Java library</li> <li>#360: Fix coverage in CI workflow</li> <li>#284: Upgrade monorepo to support multi-app and microservice architecture</li> <li>#282: Remove extension <code>orta.vscode-jest</code></li> <li>#275: Remove duplicated vs code settings</li> <li>#272: Source dev-env.sh if found in current directory</li> <li>#270: Use <code>docker run</code> to import and export KC data</li> <li>#265: Use challenge-devcontainer:5ad791b9</li> <li>#264: Fix devcontainer definition</li> </ul>"},{"location":"updates/june-2022/#challenge-registry-platform","title":"Challenge registry platform","text":"<ul> <li>#359: Update Footer Component</li> <li>#354: Update login page to UI v3</li> <li>#353: Update navbar to UI v3</li> <li>#278: Fix challenge registry blank page</li> <li>#287: Rename project <code>db-cli</code> to <code>challenge-db-cli</code></li> <li>#290: Remove login projects</li> <li>#304: Add alias <code>challenge-docker-stop</code> to stop running containers</li> <li>#305: Add challenge-registry-prepare alias</li> </ul>"},{"location":"updates/june-2022/#development-environment-enhancements","title":"Development environment enhancements","text":"<ul> <li>#374: Update draw.io library and instructions</li> <li>#281: Add badge to open repo in devcontainer</li> <li>#271: Add unzip and vim to devcontainer</li> <li>#269: Remove Google identity provider data</li> <li>#254: Push devcontainer to new Docker Hub repository</li> </ul>"},{"location":"updates/june-2022/#documentation-and-project-management","title":"Documentation and project management","text":"<ul> <li>#295: Update readme</li> <li>#294: Add install alias</li> <li>#303: Add list of contributors to the README</li> </ul>"},{"location":"updates/june-2022/#community-impact","title":"Community Impact","text":"<p>June 2022 marked a turning point in our platform's architectural maturity and development velocity. The successful implementation of multi-application support demonstrates our commitment to building scalable, maintainable solutions that serve the broader scientific community. Our focus on standardizing development practices and improving user experience reflects our dedication to creating tools that researchers can rely on for critical computational challenges.</p> <p>The establishment of comprehensive Java ecosystem support opens new possibilities for backend service development, while our UI modernization efforts ensure that users have access to intuitive, accessible interfaces. These foundational improvements position the platform for sustained growth and enhanced community engagement.</p>"},{"location":"updates/june-2022/#thank-you","title":"Thank You","text":"<p>Last but certainly not least, a big Thank You to the contributors of the Sage monorepo.</p> <ul> <li>@tschaffter</li> <li>@rrchai</li> <li>@vpchung</li> </ul> <p>We're building the future of computational challenges, and every contribution brings us closer to empowering researchers worldwide. Join us in creating tools that advance scientific discovery.</p>"},{"location":"updates/march-2022/","title":"March 2022","text":"<p>Published on March 31, 2022</p> <p>Welcome to our March 2022 update! This month brought impressive development momentum with 34 pull requests merged, primarily contributed by @tschaffter and @vpchung. March was a pivotal month focused on implementing comprehensive test coverage, establishing development workflows, and building out core platform features for the Challenge Registry.</p>"},{"location":"updates/march-2022/#summary","title":"Summary","text":"<ul> <li>Total Pull Requests: 34 merged PRs</li> <li>Key Focus: Test coverage implementation, CI/CD optimization, web application development</li> <li>Major Projects: Challenge Registry frontend development, development environment standardization, coverage reporting system</li> </ul>"},{"location":"updates/march-2022/#technical-architecture-overview","title":"Technical Architecture Overview","text":""},{"location":"updates/march-2022/#coverage-reporting-system","title":"Coverage reporting system","text":"<p>March introduced a comprehensive test coverage reporting system that aggregates coverage data across all TypeScript and Python projects in the monorepo. The implementation combines Jest for TypeScript testing with Python's coverage tools, automatically merging reports during CI/CD execution. This unified approach provides clear visibility into code quality metrics while supporting the diverse technology stack of the Sage ecosystem.</p>"},{"location":"updates/march-2022/#development-environment-standardization","title":"Development environment standardization","text":"<p>The development workflow received significant improvements with the introduction of standardized development container configurations and streamlined dependency management. These changes eliminate environment-specific issues and provide consistent development experiences across different platforms. The new approach separates Docker-based deployment from local development requirements, making it easier for contributors to get started.</p>"},{"location":"updates/march-2022/#challenge-registry-platform-evolution","title":"Challenge Registry platform evolution","text":"<p>The Challenge Registry web application saw substantial progress with the implementation of core user interface components and routing infrastructure. The development focused on creating reusable component libraries and establishing clear architectural patterns for future feature development. The platform now supports user profiles, organization pages, and authentication workflows.</p>"},{"location":"updates/march-2022/#pull-requests-merged","title":"Pull Requests Merged","text":""},{"location":"updates/march-2022/#platform-infrastructure","title":"Platform infrastructure","text":"<ul> <li>#122: Fix API coverage file paths</li> <li>#121: Fix api coverage</li> <li>#119: Generate coverage report in lcov format for the API</li> <li>#117: Add coverage badge and a few TS unit tests</li> <li>#115: Generate coverage score</li> <li>#99: Use dev container sagebionetworks/python-3-node:0.0.1</li> <li>#97: Fix web-app docker issue related to CA</li> <li>#89: Use Node.js 16 (LTS) in CI workflow</li> <li>#88: Cache node_modules in CI workflow</li> <li>#87: Make use of Node.js cache in CI workflow</li> <li>#85: Add pyenv to development requirements</li> <li>#81: Cleanup nx cache</li> <li>#67: Mirror VS Code settings to dev container config</li> <li>#66: Set docker service name of web-app</li> <li>#64: Enable nx scan option</li> <li>#60: Add CODEOWNERS</li> </ul>"},{"location":"updates/march-2022/#challenge-registry-application","title":"Challenge Registry application","text":"<ul> <li>#112: Document how to add a theme to a component</li> <li>#105: Document how styles, assets and themes work</li> <li>#104: Add Challenges page</li> <li>#102: Add org page</li> <li>#101: Patch user profile</li> <li>#96: Add user page</li> <li>#94: Add Log in page</li> <li>#92: Generate Sign up page</li> <li>#90: Rename search-viewer to challenge-search</li> <li>#82: Add about page</li> <li>#79: Add sections to navbar</li> <li>#78: Add not found page</li> <li>#68: Import homepage</li> </ul>"},{"location":"updates/march-2022/#documentation-and-governance","title":"Documentation and governance","text":"<ul> <li>#95: update readme</li> </ul>"},{"location":"updates/march-2022/#community-impact","title":"Community Impact","text":"<p>March 2022 marked a significant milestone in the Challenge Registry project's evolution toward a modern, maintainable platform. The focus on test coverage and development standardization demonstrates the team's commitment to sustainable development practices. These foundational improvements will enable faster, more reliable feature development while maintaining the high quality standards expected in the biomedical research community.</p> <p>The comprehensive approach to development environment setup and documentation ensures that new contributors can quickly become productive, supporting the open-source nature of the Sage Bionetworks mission.</p>"},{"location":"updates/march-2022/#thank-you","title":"Thank You","text":"<p>Last but certainly not least, a big Thank You to the contributors of the Sage monorepo.</p> <ul> <li>@tschaffter</li> <li>@vpchung</li> </ul> <p>Your dedication to building robust, well-tested software infrastructure continues to enable groundbreaking research in the biomedical community. Together, we're creating tools that empower scientists and accelerate discovery.</p>"},{"location":"updates/may-2022/","title":"May 2022","text":"<p>Published on June 1, 2022</p> <p>Welcome to our May 2022 update! This month brought significant expansion in the Challenge Registry platform architecture with 21 pull requests merged to the main branch. Our development efforts were led by @tschaffter and @rrchai, focusing on microservices architecture, authentication systems, and comprehensive platform tooling.</p>"},{"location":"updates/may-2022/#summary","title":"Summary","text":"<ul> <li>Total Pull Requests: 21 merged PRs</li> <li>Key Focus: Microservices architecture expansion and authentication infrastructure</li> <li>Major Projects: Keycloak integration, Challenge Service Registry, npm scope standardization</li> </ul>"},{"location":"updates/may-2022/#technical-architecture-overview","title":"Technical Architecture Overview","text":""},{"location":"updates/may-2022/#microservices-infrastructure-evolution","title":"Microservices infrastructure evolution","text":"<p>May marked a pivotal month in our platform architecture with the introduction of comprehensive microservices infrastructure. We established the Challenge Service Registry, API Gateway, and User Service foundations, creating a robust distributed system architecture that supports scalable challenge management.</p>"},{"location":"updates/may-2022/#authentication-and-authorization-framework","title":"Authentication and authorization framework","text":"<p>The integration of Keycloak as our identity and access management solution represents a major architectural milestone. We implemented OAuth 2.0 authentication flows, established protected resource access patterns, and created the foundation for comprehensive user management across the platform ecosystem.</p>"},{"location":"updates/may-2022/#development-environment-modernization","title":"Development environment modernization","text":"<p>Significant improvements to our development infrastructure included devcontainer integration with CI/CD workflows, comprehensive Docker composition updates, and enhanced documentation for remote development scenarios. These changes ensure consistent development experiences across different environments and deployment targets.</p>"},{"location":"updates/may-2022/#package-management-standardization","title":"Package management standardization","text":"<p>The transition to the <code>@sagebionetworks</code> npm scope aligns our JavaScript ecosystem with organizational standards and provides better package namespace management. This standardization supports future package publishing and cross-project dependency management.</p>"},{"location":"updates/may-2022/#pull-requests-merged","title":"Pull Requests Merged","text":""},{"location":"updates/may-2022/#microservices-architecture","title":"Microservices architecture","text":"<ul> <li>#243: Create Challenge Service Registry</li> <li>#247: Integrate keycloak to microserve infra</li> <li>#245: Rename keycloak projects</li> <li>#234: Create project <code>challenge-platform-app</code></li> </ul>"},{"location":"updates/may-2022/#authentication-and-security","title":"Authentication and security","text":"<ul> <li>#206: Login with KC and access protected movie resources</li> <li>#192: Add movie REST API to test Keycloak integration</li> <li>#136: Explore Keycloak as workspace IAM</li> </ul>"},{"location":"updates/may-2022/#containerization-and-infrastructure","title":"Containerization and infrastructure","text":"<ul> <li>#253: Add devcontainer definition</li> <li>#205: Update workspace docker compose</li> <li>#204: Create project <code>apps/keycloak-db</code></li> <li>#203: Rename project task <code>docker</code> to <code>build-image</code></li> <li>#182: Use devcontainer in CI/CD workflow</li> </ul>"},{"location":"updates/may-2022/#challenge-registry-platform-features","title":"Challenge Registry platform features","text":"<ul> <li>#207: Add challenge sponsor</li> </ul>"},{"location":"updates/may-2022/#development-tools-and-configuration","title":"Development tools and configuration","text":"<ul> <li>#256: Use npm scope @sagebionetworks</li> <li>#228: Rename workspace npm scope</li> <li>#240: Add VS Code extensions for Java</li> <li>#162: Add GUI plugin in vscode for jest</li> <li>#163: Restore pipenv cache in CI/CD workflow</li> </ul>"},{"location":"updates/may-2022/#documentation-and-guides","title":"Documentation and guides","text":"<ul> <li>#201: Describe how to start api before contributing to the web-app</li> <li>#200: Update remote host setup doc</li> <li>#181: Document how to setup development for devcontainers running on EC2</li> </ul>"},{"location":"updates/may-2022/#community-impact","title":"Community Impact","text":"<p>May's contributions represent a fundamental shift toward a mature, enterprise-ready platform architecture. The introduction of microservices patterns and comprehensive authentication systems positions the Challenge Registry as a scalable foundation for biomedical research collaboration.</p> <p>The establishment of Keycloak-based authentication creates secure, standardized access patterns that will support future multi-tenant capabilities and fine-grained authorization controls. This infrastructure investment ensures the platform can grow to support larger research communities while maintaining security and performance standards.</p>"},{"location":"updates/may-2022/#thank-you","title":"Thank You","text":"<p>Last but certainly not least, a big Thank You to the contributors of the Sage monorepo.</p> <ul> <li>@tschaffter</li> <li>@rrchai</li> </ul> <p>We're building something amazing together, and every contribution moves us closer to our vision of advancing biomedical research through innovative technology platforms. Join us in shaping the future of collaborative research.</p>"},{"location":"updates/september-2025/","title":"September 2025","text":"<p>Published on October 1, 2025</p> <p>September advanced platform coherence, developer reliability, and scientific user experience across the Sage monorepo with 96 pull requests merged. Highlights included centralized authentication and gateway evolution, resilient Synapse workflow hardening, Model-AD data+UX refinements, systematic OpenAPI normalization, and stronger local/compose infrastructure. Primary contributors included @tschaffter, @hallieswan, @rrchai, @sagely1, and automation support from @github-actions[bot].</p>"},{"location":"updates/september-2025/#summary","title":"Summary","text":"<ul> <li>Total Pull Requests: 96 merged PRs</li> <li>Key Focus: Gateway &amp; auth consolidation, Synapse workflow resilience, Model-AD data/model version alignment &amp; UX, OpenAPI governance, infrastructure composability</li> <li>Major Projects: OpenChallenges authorization redesign, Model-AD mv84\u2192mv85 transition, Synapse automated update pipeline, cross-project OpenAPI/tag normalization, Caddy &amp; Docker compose unification</li> </ul>"},{"location":"updates/september-2025/#technical-architecture-overview","title":"Technical Architecture Overview","text":""},{"location":"updates/september-2025/#platform-evolution","title":"Platform evolution","text":"<p>Authentication moved toward centralized mediation via the OpenChallenges API gateway while BixArena added its own gateway foundation. Caddy image consolidation and consistent reverse-proxy patterns reduced divergence and simplified operational updates.</p>"},{"location":"updates/september-2025/#developer-experience","title":"Developer experience","text":"<p>Deterministic environments improved through Gradle 9 adoption, ES target unification, dependency hygiene, and pruning unused config (implicit dependencies, deprecated libraries). A refreshed, security\u2011scanned dev container ships a fully pinned multi\u2011language toolchain so every contributor builds and tests with the same versions\u2014eliminating \"works on my machine\" drift and shortening onboarding/setup time. Together these changes decrease friction and accelerate iterative feature delivery.</p>"},{"location":"updates/september-2025/#api-modernization-initiative","title":"API modernization initiative","text":"<p>Kebab-case path/file conventions, tag singularization, vendor extension standardization (<code>x-audience</code>\u2192<code>x-internal</code>), and removal of deprecated annotations drove a cleaner, more discoverable OpenAPI surface across Agora, Model-AD, Synapse, and Explorers\u2014laying groundwork for consistent client generation.</p>"},{"location":"updates/september-2025/#user-interface-enhancements","title":"User interface enhancements","text":"<p>Model-AD received iterative accessibility and interaction upgrades (keyboard navigation, highlight, scrolling behavior, adaptive layouts, improved placeholders, legend visibility) plus new data-driven conditional panels. Cross\u2011project search and visualization components were tuned for clarity and responsiveness.</p>"},{"location":"updates/september-2025/#data-schema-lifecycle","title":"Data &amp; schema lifecycle","text":"<p>Successive data bumps (mv84, mv85), new comparative disease correlation endpoint, and alignment of omics card logic modernized scientific insights delivery while tightening frontend\u2013backend semantic parity.</p>"},{"location":"updates/september-2025/#workflow-automation-resilience","title":"Workflow &amp; automation resilience","text":"<p>The Synapse update workflow underwent multi-stage stabilization culminating in reliable automated artifact regeneration, while broader CI triggers and concurrency controls prevented redundant runs. It now executes a once\u2011per\u2011day scheduled check of the official upstream Synapse OpenAPI description; if a content diff is detected it regenerates the monorepo's OpenAPI\u2011derived client artifacts (including the Angular client) and raises a PR, otherwise it terminates as a no\u2011op. The generated Angular client, for example, is leveraged by Agora and Model-AD to interact with Synapse Wikis, keeping downstream integrations current without manual intervention.</p>"},{"location":"updates/september-2025/#pull-requests-merged","title":"Pull Requests Merged","text":""},{"location":"updates/september-2025/#authentication-gateway-centralization","title":"Authentication &amp; gateway centralization","text":"<ul> <li>#3438: feat(openchallenges): centralize authentication (JWT tokens &amp; API keys) in the API gateway (SMR-444, SMR-445, SMR-446)</li> <li>#3505: feat(bixarena): add BixArena API gateway (SMR-504)</li> <li>#3485: fix(openchallenges): hotfix org login, replace <code>org.acronym</code> by <code>org.shortName</code> and adopt <code>x-sage-internal</code> (SMR-480, SMR-489)</li> </ul>"},{"location":"updates/september-2025/#workflow-ci-resilience","title":"Workflow &amp; CI resilience","text":"<ul> <li>#3534: chore: update trigger for Lint PR workflow and CI workflow (SMR-507)</li> <li>#3532: chore(synapse): disable Nx Cloud in the Synapse API workflow</li> <li>#3528: fix(synapse): set <code>COREPACK_ENABLE_DOWNLOAD_PROMPT</code> in Synapse API workflow (SMR-507)</li> <li>#3527: fix(synapse): update the Synapse API workflow to create PR that triggers downstream workflows (SMR-507)</li> <li>#3523: fix(synapse): use alternative method to run the dev container to update Synapse API (SMR-435)</li> <li>#3521: feat(synapse): simplify the workflow that update the Synapse API</li> <li>#3520: fix(synapse): specify base branch in workflow that updates the API</li> <li>#3519: refactor(synapse): create commit inside the dev container in the update workflow</li> <li>#3518: refactor(synapse): rewrite the Synapse update workflow</li> <li>#3517: fix(synapse): pass env var <code>GITHUB_TOKEN</code> to the dev container in the workflow that updates Synapse API</li> <li>#3515: fix(synapse): simplify PR description in the workflow that updates the API description and clients</li> <li>#3514: fix(synapse): fix issue because of missing GitHub env var inside the dev container</li> <li>#3513: fix(synapse): fix nested single quotes in GitHub workflow that updates Synapse API (SMR-435)</li> <li>#3512: fix(synapse): fix GitHub workflow that updates Synapse API - Take 4 (SMR-435)</li> <li>#3511: fix(synapse): fix GitHub workflow that updates Synapse API - Take 3 (SMR-435)</li> <li>#3510: fix(synapse): fix GitHub workflow that updates Synapse API - Take 2 (SMR-435)</li> <li>#3509: fix(synapse): fix GitHub workflow that updates Synapse API (SMR-435)</li> <li>#3467: fix(explorers): skip nx cloud for e2e tests (SMR-463)</li> </ul>"},{"location":"updates/september-2025/#openapi-governance-consistency","title":"OpenAPI governance &amp; consistency","text":"<ul> <li>#3469: chore: replace OpenAPI extension <code>x-audience</code> by <code>x-internal</code> (SMR-465)</li> <li>#3457: chore(amp-als): remove x-java annotations from AMP-ALS OpenAPI description (SMR-385)</li> <li>#3455: chore(model-ad): update API paths and files to use kebab case (MG-405)</li> <li>#3454: chore(amp-als): generate Angular API client files in kebab case for AMP-ALS (SMR-416)</li> <li>#3443: chore(agora): remove @ from Agora openapi file and directory names (AG-1844, SMR-336)</li> <li>#3441: chore(synapse): generate Angular API client files in kebab case (SMR-415)</li> <li>#3440: chore(agora): update Agora OpenAPI tags to use spaces and singular form (SMR-338)</li> <li>#3439: chore(model-ad): update Model-AD OpenAPI tags to use spaces and singular form (SMR-337)</li> <li>#3436: chore(explorers): generate Angular API client files in kebab case (MG-383, AG-1851)</li> </ul>"},{"location":"updates/september-2025/#uiux-accessibility-refinement","title":"UI/UX &amp; accessibility refinement","text":"<ul> <li>#3484: fix(model-ad): ensure scroll to section after same-document navigation (MG-413)</li> <li>#3483: fix(model-ad): copying section links shouldn't change the current URL (MG-412)</li> <li>#3481: fix(model-ad): clean up section link fragments (MG-414)</li> <li>#3479: fix(model-ad): conditionally display omics tab, fallback to default tab when invalid or disabled tab in URL (MG-282, MG-415)</li> <li>#3471: fix(model-ad): update 'no results' message in search input (MG-409)</li> <li>#3465: fix(model-ad): wrap boxplots grid to enable complete image downloads (MG-407)</li> <li>#3463: feat(model-ad): align boxplot style with design (MG-298)</li> <li>#3462: feat(model-ad): allow search results to be selected with mouse or keyboard (MG-361)</li> <li>#3460: chore(explorers): update boxplot chart to use axis tooltips built into echarts 5.6.0+ (MG-270)</li> <li>#3456: fix(model-ad): unstick the toc (MG-403)</li> <li>#3453: feat(model-ad): matched control(s) value(s) in hero link to JAX (MG-397)</li> <li>#3450: feat(model-ad): update search tile wording (MG-366)</li> <li>#3449: fix(model-ad): update hero image (MG-303)</li> <li>#3451: fix(model-ad): style terms of service content (MG-296)</li> <li>#3448: feat(model-ad): scroll to panel content when tab specified in url, delay scroll when changing between tabs to give content time to load (MG-390)</li> <li>#3447: fix(model-ad): move allen institute card to resources (MG-396)</li> <li>#3446: feat(model-ad): highlight search query in search results, modernize search input to use signals, fix display of long search results (MG-360)</li> <li>#3445: fix(model-ad): reset model details variables when route is updated to ensure default tab is loaded for each model (MG-394)</li> <li>#3444: feat(model-ad): share link tooltip and copy behavior improvements (MG-399)</li> <li>#3442: fix(model-ad): allow model details boxplots container to stretch wider than the section (MG-330)</li> <li>#3435: feat(model-ad): align style of \"no data\" boxplot placeholder with design, fix bug where legend is not displayed when last boxplot doesn't have data (MG-317)</li> <li>#3433: fix(model-ad): fix bug preventing display of all boxplots (MG-387, MG-392)</li> </ul>"},{"location":"updates/september-2025/#data-schema-lifecycle-evolution","title":"Data &amp; schema lifecycle evolution","text":"<ul> <li>#3464: chore(model-ad): bump model-ad-data to mv85 and align e2e tests with available data (MG-398)</li> <li>#3434: chore(model-ad): bump model-ad-data to mv84 and align models with updated data structure (MG-362)</li> <li>#3470: feat(model-ad): conditionally display omics cards (MG-278)</li> <li>#3452: feat(model-ad): create api description and route for disease_correlation (MG-237)</li> <li>#3472: fix(model-ad): remove frontend search query sanitization in favor of backend query escaping (MG-408)</li> <li>#3488: feat(explorers): update site version in model-ad footer, create shared service to manage data version and site version (MG-373)</li> </ul>"},{"location":"updates/september-2025/#developer-environment-determinism","title":"Developer environment determinism","text":"<ul> <li>#3531: chore(deps): remove <code>packageManager</code> from <code>package.json</code> (SMR-507)</li> <li>#3530: chore: activate the dev container <code>sha-db91d6849b048b6ef69472cdeb80caad262b3e68</code> (SMR-497)</li> <li>#3526: chore(bixarena): switch bixarena-app task names to serve different versions</li> <li>#3504: refactor: remove \"-angular\" suffix from TypeScript aliases</li> <li>#3502: chore(deps): upgrade to Gradle 9 (SMR-494)</li> <li>#3495: chore(deps): update dependencies with nx migrate latest and add SOP for updating packages (SMR-469)</li> <li>#3494: chore(deps): adopt pnpm minimumReleaseAge (SMR-492)</li> <li>#3501: chore(deps): update testing and storybook dependencies (SMR-469)</li> <li>#3507: chore(deps): update dependencies with security vulnerabilities (SMR-469)</li> <li>#3476: chore(deps): update all es targets to <code>es2024</code> (SMR-284)</li> <li>#3458: chore: remove empty <code>implicitDependencies</code> in <code>project.json</code> files (SMR-362)</li> <li>#3437: chore(sage-monorepo): use chat.tools.terminal.autoApprove instead (SMR-388)</li> </ul>"},{"location":"updates/september-2025/#infrastructure-local-composability","title":"Infrastructure &amp; local composability","text":"<ul> <li>#3474: feat: centralize all Caddy images and update to Caddy 2.10.2 - Take 2 (SMR-433)</li> <li>#3459: feat: add Docker Compose <code>extra_hosts</code> to OC, Agora and Model-AD apexes (SMR-384)</li> <li>#3466: chore(openchallenges): migrate from sse to streamable http transport</li> <li>#3468: chore(sage-monorepo): remove the deprecated shiny-base Docker project</li> <li>#3486: feat(openchallenges): make the image service zero-config (SMR-478)</li> <li>#3487: feat(openchallenges): set allowlist used by Thumbor's HTTP loader (SMR-478)</li> <li>#3480: feat(openchallenges): switch default Thumbor loader to <code>thumbor.loaders.http_loader</code> (SMR-477)</li> <li>#3492: refactor(openchallenges): update app properties of the MCP server (SMR-479)</li> <li>#3489: chore(openchallenges): remove unused Terraform infra project</li> <li>#3508: refactor(openchallenges): remove the <code>openchallenges-app-config-data</code> Java library (SMR-462)</li> </ul>"},{"location":"updates/september-2025/#security-scope-taxonomy-authorization-redesign","title":"Security scope taxonomy &amp; authorization redesign","text":"<p>No direct scope taxonomy\u2013only PRs merged this month (authorization semantics were bundled with gateway consolidation above).</p>"},{"location":"updates/september-2025/#automation-generated-artifacts","title":"Automation &amp; generated artifacts","text":"<ul> <li>#3535: chore(synapse): update Synapse API description and generated artifacts</li> </ul>"},{"location":"updates/september-2025/#documentation-framework-foundation","title":"Documentation framework foundation","text":"<ul> <li>#3473: chore: batch patch/minor Java dependencies update (SMR-468)</li> <li>#3493: docs(sage-monorepo): redesign the main README (SMR-496)</li> <li>#3431: docs(sage-monorepo): update nav structure of the docs site and add first monthly updates - Take 2 (SMR-418, SMR-419)</li> </ul>"},{"location":"updates/september-2025/#community-impact","title":"Community Impact","text":"<p>Consistent OpenAPI governance and automated regeneration reduce integration friction for downstream tools and clients. Centralized authentication plus emerging gateway patterns strengthen security posture while simplifying service composition. Model-AD\u2019s refined interaction patterns and data alignment accelerate hypothesis exploration. Resilient Synapse workflows and deterministic dev environments shrink feedback cycles, and unified Caddy + Docker compose strategies make local multi-application orchestration more intuitive for contributors.</p>"},{"location":"updates/september-2025/#thank-you","title":"Thank You","text":"<p>Last but certainly not least, a big Thank You to the contributors of the Sage monorepo.</p> <ul> <li>@tschaffter</li> <li>@hallieswan</li> <li>@rrchai</li> <li>@sagely1</li> <li>@github-actions[bot]</li> </ul> <p>We continue to build an open, interoperable research platform that empowers communities to accelerate discovery. Join us and help shape the next wave of scientific tooling.</p>"}]}